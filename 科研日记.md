### 2025.05.20

- 搞清楚了VLM的`detection_forward()`和`prepare_inputs_labels_for_multimodal_detection()`是如何在原本`forward()`和`prepare_inputs_for_generation()`的基础上魔改的。原本的图片是现场把`PIL.Image`给处理成`Tensor`再预处理，打成Patch替换`<|IMAGE|>`；LLMDet是把特征图的某个像素点的特征向量直接替换进去。
- 更清楚了LLMDet这个图——Feature Map为$\displaystyle\sum_{i=1}^{N_{\text{feats}}}{H_iW_i}$，通过Query Selection的筛选后，既送到VLM的CA，又送到VLM的图像Token，这已经不是VLM默认的视觉Encoder了，相当于完全换了一个。

明天接着看`GroundingDINOHead.loss()`中的`all_stage_assign_result_list[0][0].gt_inds`为什么会出现稍微大一些的整数，以及如何拿到对应类别的文本字符串。

### 2025.05.21

搞完了一阵书稿。

- 搞明白了昨天的问题：
	- `gt_inds[i]`（$N_{\text{infer\_bbox}}$）表示第`i`个推理框匹配到了Ground Truth中的第`gt_inds[i]`个框
	- `labels[i]`（$N_{\text{infer\_bbox}}$）表示推理框匹配到了Ground Truth中的第`labels[i]`个标签
	- `batch_data_samples[j]`表示Batch中的第`j`张图片的Ground Truth集合，其中的`.text`是由若干类别字符串组成的、由逗号分割的长字符串，第`i`个类别的字符串由`.tokens_positive[i]`给定的起始与终止下标给定，可以据此截取子串。

```python
>>> a = all_stage_assign_result[-1][0].labels[0].item()
<<< 2

>>> batch_data_samples[0].tokens_positive
<<< {0: [[...]], 1: [[...]], 2: [[...]], 3: [[...]], 4: [[...]], 5: [[...]], 6: [[...]], 7: [[...]]}

>>> batch_data_samples[0].tokens_positive[a]
<<< [[266, 274]]

>>> batch_data_samples[0].text[batch_data_samples[0].tokens_positive[a][0][0]:batch_data_samples[0].tokens_positive[a][0][1]]
<<< 'rag doll'
```

TODO: `batch_size > 1`时，GroundingDINO.loss()中的`lmm_imput_dict['image_queries']`的shape是什么？是batch_size × N_{GTbbox}吗？注意后面相加项的变形