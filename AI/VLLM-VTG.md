
|                                                        | Pipeline                                                                                                                                                                                                                          | 标签                                   | 架构   |
| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ---- |
| [Grounding-Prompter](https://arxiv.org/abs/2312.17117) | 视频通过ASR与BLIP，生成字幕与画面文本描述。<br>文本不变。<br>与文本一起，直接送入LLM，以自然语言输出时间戳。                                                                                                                                                                   | 输入仅文本<br>由自然语言输出                     | VLM  |
| [VTG-GPT](https://arxiv.org/abs/2403.02076)            | 视频通过VLLM，生成画面文本描述。<br>文本经过LLM，进行纠错与细化。<br>视频与文本经过相似度筛选，按阈值直接得到时间戳，后附NMS。                                                                                                                                                          | 输入仅文本<br>由文本-文本相似度矩阵输出               | 提议   |
| [GPTSee](http://arxiv.org/abs/2403.01437)              | 视频通过VLLM，生成画面文本描述，经CLIP得到视频特征。<br>文本经过LLM，生成若干语义等价的文本，以此扩充数据集。<br>画面文本与任一等价文本做相似度，得到$L_v \times 1$序列，与视频特征拼接，送入视觉编码器，与原始文本一同送入编码器，生成跨模态特征，注入可学习查询与基于相似度的位置嵌入，送入解码器，过FFN。                                                          | 输入仅文本<br>将**等价查询文本信息**注入解码器<br>输出连续值 | DETR |
| [GroundVQA](https://arxiv.org/abs/2312.06505)          | 视频通过InternVideo，提取视频特征，经线性层对齐到文本特征。<br>文本经Tokenizer，提取文本特征。<br>视频特征与文本特征送入Flan-T5编码器，然后执行下游任务。                                                                                                                                    | 输入视频与文本<br>用Decoder输出做下游任务           | VLM  |
| [LMR](https://arxiv.org/abs/2405.12540)                | 视频通过MiniGPT-v2生成画面文本描述；通过CLIP/InternVideo提取视频特征$\mathbf{F}_v$。<br>画面文本与查询文本经过CLIP/LLaMA生成文本特征$\mathbf{F}'_t$/$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F_t'}$分别与$\mathbf{F}_t$做共享参数的交叉注意力融合，拼在一起进入编码器做自注意力，以$F_t$作为查询进入解码器，过FFN。 | 输入视频与文本<br>将**等价视觉描述信息**注入编码器        | VLM  |
| [TEA](https://arxiv.org/abs/2406.17880)                | 视频通过LLaVA-v1.5/BLIP-2生成画面文本描述，通过3D-CNN提取视频特征。<br>画面文本与查询文本通过GloVe获取文本特征。<br>视频特征与画面文本拼接后过MLP，与查询文本做交叉注意力；画面文本单独与查询文本做交叉注意力；分别送入预测器。预测器接受两个输入，按照[Span Predictor](https://arxiv.org/pdf/2004.13931)的流程输出边界。这两个边界加权平均即为输出。           | 输入视频与**细化**文本<br>将**等价视觉描述信息**注入编码器  | DETR |
| [TFVTG](https://arxiv.org/pdf/2408.16219)              | 视频通过CLIP提取视频特征。<br>查询文本由LLM拆成若干有时间约束关系的子事件，通过CLIP提取文本特征。<br>分别对子事件进行DETR推理，得到帧-置信度曲线，设计动态&静态得分函数选出TopK个提案，得到若干子事件区间，排列组合得到若干总事件区间，按时间约束关系进行过滤，根据动态&静态得分排名出最佳答案。                                                                   | 无训练<br>新Loss函数<br>手工设计子查询文本与合并机制     | DETR |
