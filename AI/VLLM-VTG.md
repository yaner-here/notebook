## 提议

|                                               | 日期      | 架构  | Pipeline                                                                                                                                                                                                                                                                | 标签                                              | 算力                    | 训练时间    | Charades-STA指标(`R@0.3`,`R@0.5`,`R@0.7`,`mIoU`) |
| --------------------------------------------- | ------- | --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | --------------------- | ------- | ---------------------------------------------- |
| [VTG-GPT](https://arxiv.org/abs/2403.02076)   | 2024.03 | 提议  | 视频通过VLLM，生成画面文本描述。<br>文本经过LLM，进行纠错与细化。<br>视频与文本经过相似度筛选，按阈值直接得到时间戳，后附NMS。                                                                                                                                                                                                | 输入仅文本<br>由文本-文本相似度矩阵输出                          | **8 Nvidia RTX 3090** | **无训练** |                                                |
| [ReCorrect](https://arxiv.org/abs/2412.00811) | 2024.12 | 提议  | 视频通过GPT-4o/Gemini-1.0生成大量粗粒度数据集，使用CLIP计算视频帧与文本的相似度，小于阈值则视为噪声并删除。再移动片段左右边界，使得片段内各帧分数总和减去片段外各帧分数总和取得最大值。<br>借用SimBase作为VTG的Backbone，维护一个存储统一样本在不同Epoch内的推理结果的Memory Bank，最终选择该样本内若干推理结果中，与其它推理结果IoU之和最高的推理结果，作为最终伪Ground Truth进行反向传播。<br>然后既可以直接拿SimBase推理，也可以继续后续的强监督训练。 | 弱监督<br>预训练<br>数据增强，发布数据集Vid-Morp<br>Memory Bank |                       |         |                                                |
| [Moment-GPT](http://arxiv.org/abs/2501.07972) | 2025.01 | 提议  | 视频通过MiniGPT-v2得到帧画面文本描述$\mathbf{F}_v$。<br>查询文本通过LLaMA-3进行纠错与重写，得到若干个等价的查询文本，通过LLaMA-3提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$做相似图，得到若干区间提议，使用Video-ChatGPT得到区间画面文本描述$\mathbf{F}'v$，计算与$\mathbf{F}_t$的相似度，对区间提议打分，过NMS。<br>                              | 相似度<br>按相似度阈值生成提议的策略与超参数                        | **无GPU**              | **无训练** |                                                |
## DETR

|                                                 | 日期      | 架构   | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 标签                                                                                                          | 算力                     | 训练时间 | Charades-STA指标(`R@0.3`,`R@0.5`,`R@0.7`,`mIoU`) |
| ----------------------------------------------- | ------- | ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ---------------------- | ---- | ---------------------------------------------- |
| [GPTSee](http://arxiv.org/abs/2403.01437)       | 2024.05 | DETR | 视频通过VLLM，生成画面文本描述，经CLIP得到视频特征。<br>文本经过LLM，生成若干语义等价的文本，以此扩充数据集。<br>画面文本与任一等价文本做相似度，得到$L_v \times 1$序列，与视频特征拼接，送入视觉编码器，与原始文本一同送入编码器，生成跨模态特征，注入可学习查询与基于相似度的位置嵌入，送入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 输入仅文本<br>将**等价查询文本信息**注入解码器<br>输出连续值                                                                        | **8 Nvidia RTX 3090**  |      |                                                |
| [TEA](https://arxiv.org/abs/2406.17880)         | 2024.06 | DETR | 视频通过LLaVA-v1.5/BLIP-2生成画面文本描述，通过3D-CNN提取视频特征。<br>画面文本与查询文本通过GloVe获取文本特征。<br>视频特征与画面文本拼接后过MLP，与查询文本做交叉注意力；画面文本单独与查询文本做交叉注意力；分别送入预测器。预测器接受两个输入，按照[Span Predictor](https://arxiv.org/pdf/2004.13931)的流程输出边界。这两个边界加权平均即为输出。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 输入视频与**细化**文本<br>将**等价视觉描述信息**注入编码器                                                                         |                        |      |                                                |
| [EI-VLG](http://arxiv.org/abs/2408.02336)       | 2024.08 | DETR | 视频通过LLaVA生成画面文本描述，过一层可训练的BERT得到画面文本特征，与文本查询进行对比学习。<br>画面文本特征与查询文本特征经过带残差的交叉注意力，与视频特征进行带残差的拼接，送到预测头。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 使用对比学习，对查询文本与画面文本进行对齐，然后再融合                                                                                 | **8 Nvidia A5000**     |      |                                                |
| [TFVTG](https://arxiv.org/pdf/2408.16219)       | 2024.08 | DETR | 视频通过CLIP提取视频特征。<br>查询文本由LLM拆成若干有时间约束关系的子事件，通过CLIP提取文本特征。<br>分别对子事件进行DETR推理，得到帧-置信度曲线，设计动态&静态得分函数选出TopK个提案，得到若干子事件区间，排列组合得到若干总事件区间，按时间约束关系进行过滤，根据动态&静态得分排名出最佳答案。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 无训练<br>新Loss函数<br>手工设计子查询文本与合并机制                                                                            |                        |      |                                                |
| [VideoLights](https://arxiv.org/abs/2412.01558) | 2024.12 | DETR | 视频通过SlowFast+CLIP+BLIP拼接提取片段特征，通过FFCNN而非线性层对齐到文本空间，得到$\mathbf{F}_v$。<br>视频切成10s片段，通过BLIP生成画面文本描述，作为查询制造合成查询文本，以扩充数据集做预训练。查询文本通过CLIP+BLIP，拼接后传入FFCNN而非线性层对齐到视觉空间，得到$\mathbf{F}_t$。<br>进行细化。$\mathbf{F}_t$与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}'_v$；对$\mathbf{F}_t$做平均池化，与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}''_v$，与$\mathbf{F}_v$做矩阵乘法得到相似度矩阵$\mathrm{V}_S$；计算$\mathbf{F}_v$与$\mathbf{F}_t$的相似度矩阵$\mathbf{V}_Q$。最后将$(\mathbf{F}'_v, \mathbf{V}_Q, \mathbf{V}_S, \mathbf{F}''_v)$拼接过MLP得到新的$\mathbf{F}_v$。新的$\mathbf{F}_t$同理。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过双向注意力+交叉注意力+残差，得到文本增强的视觉Token，输入到DETR。<br>显著性预测头通过MLP预测每个片段的高光得分，有交叉熵Loss与余弦相似度Loss尝试向Ground Truth靠近；分类头预测DETR Decoder输出的固定数量的Query是否为检索片段，本质是二分类问题，有交叉熵Loss；回归头预测时间区间，有L1 Loss和IoU Loss。VTG任务的分类头输出，理应与显著性预测头一致，将两者相似度作为Loss。显著性预测头面对正样本时使用L2 Loss，面对负样本时使用L1 Loss，随Epoch增长而加倍，迫使模型学习难样本<br>                                                                                                                                                                                                                                                                                                                                                         | 弱监督+强监督二阶段训练<br>数据增强，仅靠视频与VLM生成的文本描述来造Ground  Truth<br>**特征融合机制**<br>新Loss，跨任务对齐+动态权重<br>叠Backbone接Concat刷分 | **Nvidia RTX 3050 Ti** |      |                                                |
| [OmniSTVG](https://arxiv.org/abs/2503.10500)    | 2025.03 | DETR | 视频使用2D Backbone和3D Backbone，与查询文本模态融合。<br>按Channel拆成三份，分为空间、中间辅助、时间，中间辅助空间和时间做增强。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 新任务，与VTG无关<br>缝Backbone<br>视觉Token增强                                                                        |                        |      |                                                |
| [UPM](https://github.com/EdenGabriel/UPM)       | 2025.08 | DETR | 视频根据自适应滑动窗口长度，分割成$N_v$个片段。使用SlowFast/CLIP/VGG/I3D提取特征$X^v\in\mathbb{R}^{N_v\times D}$，音频使用PANN提取特征$X^a\in\mathbb{R}^{N_v\times D}$，查询文本使用CLIP提取特征$X^q\in\mathbb{R}^{N_q\times D}$。<br>MCoE算法：对于模态$\mathbb{R}^{N\times D}$中的每个特征$f_i\in\mathbb{R}^{1\times D}$，我们将其视为$K$个高斯分布叠加的结果，使用最大期望法优化高斯分布的参数$\{z_{ik}\}_{k=1}^{K}$与权重$\{\mu_{k},\Sigma_k\}_{k=1}^{K}$，称$K$个均值$\{\mu_{k}\}_{k=1}^{K}\in\mathbb{R}^{K\times D}$为模态的统一模式，即模态压缩后的特征。计算特征增强前后的二分图相似矩阵$A=\lambda\frac{X}{\|\|X\|\|_2}\frac{Y}{\|\|Y\|\|_2}^T$，用它更新$Y\leftarrow (1-\omega)(I - \omega^2 A \|\|A\|\|_1^T)^{-1} (\omega A Y + X)$后作为残差更新$Y$。三个模态$X^v,X^a,X^q$均经过MCoE算法增强，得到$Y^v,Y^a,Y^q\in\mathbb{R}^{K\times D}$。为了保证音频与视频的对应关系，计算视频与文本的相似度，得到每个视频特征的得分，以此为权重得到增强音频特征$Y^a$，作为残差叠加到视频得到增强视频特征$Y^{va}$。<br>随后，三个模态的特征送入DETR即可。<br>将$Y$送入Clip4caption的解码器，生成画面描述文本，衡量它与查询文本的相似度，指标使用BLEU/METEOR/ROUGE-L/CIDEr。                                                                                                                                                                                                                                                                                                                                      | 加入音频模态<br>**视觉Token压缩**<br>**视觉Token增强**                                                                    | **1 Nvidia RTX 4090**  |      | `????, 59.2, 38.1, 50.6`                       |
| [CG-DETR](https://arxiv.org/pdf/2311.08835)     | 2023.11 | DETR | 视频数据集提供了早已处理过的视频特征（SlowFast/CLIP/VGG/I3D）。<br>查询文本过CLIP得到查询文本特征（包括开头Token`<ST>`、结尾Token`<ED>`）。<br>ACA注意力机制：查询文本特征与视频特征做交叉注意力，在文本特征后加入$L_d$个Dummy Token$D$，防止一帧无关画面以100%的注意力总和来关注查询文本Token，输出时排除Dummy Token即可，期望相关单帧画面的注意力之和为1，以此作为交叉熵Loss。为了防止$L_d$个Dummy Token趋同，特地使用正交Loss迫使其保持正交。最终得到文本增强的视频特征。<br>设置一个可学习的时间Token$M$。由于我们已知一个Batch中第$b$个样本的Ground Truth，因此可以把视觉特征$V^b$分成正负样本$V^{b+},V^{b-}$，统称为$V^{b*}$。将$M$其放在$V^{b*}$之前，做自注意力，可得$\hat{M}^{b+}, \hat{V}^{b+}, \hat{M}^{b-}, \hat{V}^{b-}$。设置一个可学习的句子Token$S$。步骤同上，把文本特征分成正负样本$Q^b,D^b$，同理可得$\hat{S}^{b+}, \hat{Q}^{b}, \hat{S}^{b-}, \hat{D}^{b}$。使用InfoNCE Loss，拉近$\hat{S}^{b+}$和$\hat{M}^{b+}$，拉近$\hat{S}^{b-}$和$\hat{M}^{b-}$。计算$\hat{V}^{b+}$与$[\hat{Q}; \hat{D}]$的相似度，以此为基准纠正ACA的注意力结果，使用KL散度作为Loss。<br>计算视频的所有帧特征的平均值$V_{\mathrm{ctx}}$、每一帧的平均离差。类似于Q-Former，设置$L_p$个可学习Token$\{P_i\}_{i=1}^{L_p}$，计算每个Token与每个帧特征的平均离差的相似度之和，过SoftMax归一化，乘以ACA注意力给每个帧的打分作为权重。于是得到了$L_p$个得分，取其中Top-K个可学习Token，将其相加，作为$V_{\mathrm{ctx}}$的残差，过一个MLP（与ACA中的Query使用的MLP共享参数），得到最终的显著性Token$T$。<br>ACA增强的视频特征 + 显著性Token一起送入DETR。<br>高光检测：取DETR Encoder输出，直接用显著性Token与视频特征做相似度，相似度得分记为高光检测分数。使用Ranking Loss、对比Loss、交叉熵Loss。<br>VTG：时间戳L1 Loss、IoU Loss、前景/背景分类头交叉熵Loss。 | 新注意力机制                                                                                                      | **8 Nvidia A100**      |      | `70.4, 58.4, 36.3, 50.1`                       |
|                                                 |         |      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                             |                        |      |                                                |

## LLM/VLM

|                                                                                 | 日期      | 架构        | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 标签                                                                                                                                                       | 算力                       | 训练时间        | Charades-STA指标(`R@0.3`,`R@0.5`,`R@0.7`,`mIoU`) |
| ------------------------------------------------------------------------------- | ------- | --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ | ----------- | ---------------------------------------------- |
| [Chrono](http://arxiv.org/abs/2406.18113)                                       | 2024.06 | LLM       | 证明了视觉Token无需使用时间位置编码，而是直接把时间戳纯文本拼到LLM输入即可。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 时间位置编码                                                                                                                                                   | **4 Nvidia A100 (80GB)** | **20h**     |                                                |
| [DeVi](https://arxiv.org/abs/2409.04388)                                        | 2024.09 | LLM       | 视频通过GPT-4生成若干时间窗口尺度的片段描述，一起输入到GPT-4o，对每个描述进行改写，生成事件集合与全局摘要；通过CLIP得到视觉特征。<br>把事件集合、全局摘要、查询文本、选项文本送入GPT-4o/Gemini-2.0，得到答案。将答案对应区间内的视频特征与查询文本做相似度匹配，若相似度过低则继续多轮对话，制度相似度达标或触发对话轮次上限为止。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 无训练<br>提出密集VQA+VTG任务<br>引入查询文本-片段特征相似度作为重复推理的依据，会导致推理时间变长。                                                                                               | **无GPU**                 | **无训练**     |                                                |
| [DEViL](https://arxiv.org/abs/2512.06673)                                       | 2025.12 | LLM + OVD | 架构完全源于VideoLLaMA3-7B。<br>LLM推理时如果涉及空间坐标，会输出`<BOX>`RST Token，提取其隐藏状态，过线性层对齐到Grounding DINO文本嵌入空间，做单帧空间预测。为了实现多帧空间跟踪，将Grounding DINO分类头的$K$个Query作为输入，相邻两帧做$K\times K$相似度 + 二分图最大匹配，建立一个管道。按帧顺序将管道中的$T$个Query过RNN，从而维护一个管道的Memory。前文的相邻两帧$K\times K$中的前一帧指的就是Memory。最后取管道内置信度平均值最大的管道。<br>一阶段：全局训练，训练模型输出RST Token<br>二阶段：仅微调LLM，增强时序理解<br>三阶段：全局训练                                                                                                                                                                                                                                                                                                                                                                                                                    | 三阶段训练<br>特殊Token提取隐藏状态，过专用OVD做下游任务                                                                                                                       | **4 Nvidia A800**        | **6d**      |                                                |
| [DSTH](http://arxiv.org/pdf/2509.15178v1)                                       | 2025.09 | VLM       | 视频每一帧经过Grounding DINO得到$K$个物体位置，经过SAM-2跟踪物体得到$K$个管道。查询文本过GPT-4o，分成空间查询$Q_s$与时间查询$Q_t$。<br>创建一个可学习的、形状与$T_s$完全一致的位置编码$V_s$，两者相加后得到视觉Token，与$Q_s$混合后输入到LLaVA-Next-Video-7B/Qwen2-VL-7B/ShareGPT4Video-8B/LLaVA-OneVision-7B，优化模型输出`yes`与`no`的概率，从而训练位置编码。$Q_t$与$V_t$同理。<br>假设LLM的Transformer有$N$层、注意力头数有$H$个、输入总Token数量为$L$，其中视觉Token数量为$L_v$，LLM词表自带特殊Token数量为$L_{\mathrm{role}}$则LLM输出的文本到视觉的注意力图为$\mathbb{R}^{N\times H\times L\times L}\rightarrow \mathbb{R}^{L_{\mathrm{role}}\times L_v}$，据此可选择相关性最高的特殊Token作为定位Token，于是$\mathbb{R}^{L_{\mathrm{role}}\times L_v} \rightarrow \mathbb{R}^{1\times L_v}$，得到正序注意力图$A_g^S$。同理，视频倒放，得到倒序注意力图$\overline{A}_g^S$。使用Grounding DINO输出的框位置作为掩码，最大化两者在框内做与运算的值。<br>给定一个Grounding DINO物体位置，以此为掩码计算正序注意力图之和，取最大的物体位置作为最终定位。 | 无需训练，即插即用<br>查询文本分为双流，空间查询与时间查询<br>用OVD直接提取物体位置，用相似度衡量置信度做Top-1筛选<br>数据增强，倒序播放                                                                           | **8 Nvidia A800**        |             |                                                |
| [E.T.Chat](http://arxiv.org/abs/2409.18111)                                     | 2024.09 | LLM       | 视频按1FPS采样，通过ViT-G，**借鉴了LLaMA-VID的压缩架构**，**过Q-Former做压缩**，使用交叉注意力用文本特征来增强压缩前的特征，直接与文本相加，过平均池化得到视觉特征，过线性层与文本对齐，输入到Phi-3-Mini-3.8B，用LoRA微调。<br>输出两个个特殊的时间Token`<vid>`，提取时间Token与视觉Token的隐藏状态，过MLP后做相似度检测，取最匹配的作为时间戳，得到起始+终止的时间戳。Loss函数不使用0/1匹配，而是使用指数衰减，允许临近帧为`0.5`/`0.25`/......的值。<br>修改LLM的Mask，让视频Token之间互相可见，等价于做自注意力                                                                                                                                                                                                                                                                                                                                                                                                                                              | 输出时间Token，提取隐藏状态做片段时间戳的回归<br>数据增强，发布E.T.Bench数据集<br>新分类Loss，平滑匹配                                                                                         | **8 Nvidia V100**        | **20h**     |                                                |
| [GeLM](http://arxiv.org/abs/2408.14469)                                         | 2024.08 | LLM       | 视频通过InternVideo-MM-L-14提取视觉特征，使用Tokenzier得到查询文本Token，作为Vicuna-1.3的输入，输出包含定位Token对，得到查询向量。<br>视觉特征和查询向量进行融合：（1）将视觉特征过一个显著性头，预测每一帧的显著性得分，做交叉熵Loss；（2）做预先相似度矩阵，得到相似度得分，做NCE Loss。利用显著性得分，大于`0.7`的取并集得到粗粒度答案；利用相似度矩阵，过平滑卷积+Softmax，同样让大于`0.7`的取并集得到细粒度答案。二者均能作为答案，且能互补。<br>创建数据标注Pipeline，基于Ego4D数据集i，用Spacy解析旁白文本，构建主谓宾语法树，筛选多次出现的概念，交给GPT-4o生成问答文本对。                                                                                                                                                                                                                                                                                                                                                                                                                   | 数据增强，发布MULTIHOP-EGOQA数据集，专注多跳视频推理和定位<br>引入定位Token，解剖LLM，提取其隐藏状态做定位任务                                                                                     | **4 Nvidia H800 (80GB)** |             |                                                |
| [GranAlign](https://arxiv.org/abs/2601.00584)                                   | 2026.01 | VLM + 提议  | 给定查询文本$Q$，使用LLaMA-3-8B重写为$m=3$个简化查询（删除生僻词+修饰词，仅保留主谓）与$m=3$个详细查询，记为$Q_s$与$Q_d$。<br>视频过Qwen-2.5-VL-7B生成每帧的通用描述$C_{\mathrm{agn}}$，与$Q_s$做相似度，得到粗粒度相似度；视频每帧过ViT-B，与$Q$做相似度，筛选Top-K%作为候选关键帧，命令Qwen-2.5-VL-7B着重生成查询文本出现的主谓的的详细描述$C_{\mathrm{awr}}$，与$Q_d$做相似度，得到细粒度相似度。两种相似度取平均值得到最终相似度$S_f$。基于$S_f$，设定阈值筛选候选帧，过NMS得到片段时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                | 无需训练，即插即用<br>粗/细粒度截阈值 + 二阶段提议 + NMS                                                                                                                      | **4 Nvidia A6000**       | **无训练**     |                                                |
| [Grounded-VideoLLM](https://arxiv.org/abs/2410.03290)                           | 2024.10 | VLM       | 视频分成$K=300$个片段，每个片段取中间帧过ViT+平均池化+MLP得到空间流，取整体过InternVideo2-1B+平均池化+MLP作为时间流，合并成双流。$K$个双流与查询文本拼接，作为Phi3.5-Vision或LLaVA-1.5-7B的输入。<br>构建$K=300$个离散的时间Token，数据集中的Q与A均用时间Token表示。<br>一阶段：用图像-文本对训练MLP，实现视觉对齐<br>二阶段：训练密集视频描述+时序定位任务，实现学习时间Token<br>三阶段：多任务指令微调                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 三阶段训练<br>离散时间Token<br>输入双流，即高分辨率单帧+低分辨率视频<br>数据增强，发布数据集Grounded VideoQA                                                                                  | **? Nvidia A100**        |             |                                                |
| [Grounding-Prompter](https://arxiv.org/abs/2312.17117)                          | 2023.12 | VLM       | 视频通过ASR与BLIP，生成字幕与画面文本描述。<br>文本不变。<br>与文本一起，直接送入LLM，以自然语言输出时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 输入仅文本<br>由自然语言输出                                                                                                                                         |                          |             |                                                |
| [GroundingGPT](https://arxiv.org/abs/2401.06071)                                | 2024.03 | LLM       | 图像通过ViT，视频等距采样若干帧后通过BLIP-2 Q-Former，音频截取2秒通过FFT + 时间位置编码 + ImageBind，过线性层对齐到Vicuna-1.5的文本嵌入空间，直接输出文本。<br>一阶段：仅训练线性层，训练多模态<br>二阶段：仅训练线性层 + LLM，训练输出时间+空间坐标<br>三阶段：仅训练线性层 + LLM，优化模型回复                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 多阶段<br>与[VTimeLLM](https://arxiv.org/abs/2311.18445)完全一致，多加了一个音频模态                                                                                       | **8 Nvidia A100**        |             |                                                |
| [GroundVQA](https://arxiv.org/abs/2312.06505)                                   | 2024.04 | VLM       | 视频通过InternVideo，提取视频特征，经线性层对齐到文本特征。<br>文本经Tokenizer，提取文本特征。<br>视频特征与文本特征送入Flan-T5编码器，然后执行下游任务。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 输入视频与文本<br>用Decoder输出做下游任务                                                                                                                               |                          |             |                                                |
| [HawkEye](https://arxiv.org/abs/2403.10228)                                     | 2024.03 | LLM       | 完全基于VideoChat2架构，在原先第三步的微调阶段使用了自己发布的InternVid-G数据集。<br>LLM不再直接输出文本时间戳，而是输出前面/中间/后面/全程等闭集类别，通过二分递归定位锁定片段                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 数据增强，构建正负样本对，发布InternVid-G数据集<br>LLM不输出文本时间戳，而是返回二分搜索的`check()`值。                                                                                        | **8 Nvidia V100**        | **7d**      |                                                |
| [LITA](https://arxiv.org/abs/2403.19046)                                        | 2024.03 | LLM       | 视频均匀采样100帧，通过ViT + SlowFast + 线性层得到视觉Token，与文本Token合并注入到Vicuna。<br>**引入新的相对时间Token`<1>~<100>`**，与帧率解耦。<br>多任务联合微调：密集视频描述、事件定位、视频问答、自然语言视觉问答、推理时序定位                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 数据增强，考验推理能力，发布数据集ActivityNet-RTL<br>多任务微调                                                                                                                | **8 Nvidia A100**        | **9h~13h**  |                                                |
| [LLaVA-MR](http://arxiv.org/abs/2411.14505)                                     | 2024.11 | LLM       | 视频经过BLIP-2视觉编码器得到帧特征，在时序上做差分 + 高斯平滑，筛选出Top-K个变化最大的关键帧，则非关键帧可降低Token数量，过Q-Former压缩，但是只保留沿时序方差最大的Top-K个Query作为视觉Token，或者直接选取相邻的K个Token做平均池化，与查询文本Token一起输入到BLIP-2中。一切数字作为文本Token，但是时间与视觉Token有自己的特殊分隔Token。BLIP-2的输入形如：`<time_begin>2<time_end>(<frame_begin>F_i<frome_end>)+文本查询`。<br>针对不同帧率和时长的数据集，若采样帧率`<1`，则对时间戳取整；若采样帧率`>1`，则使用基于帧总数的相对位置编码——因为LLM不擅长输出文本小数。<br>                                                                                                                                                                                                                                                                                                                                                                                                  | **视觉Token压缩**<br>帧率自适应时间位置编码                                                                                                                             | **4 Nvidia A100 (80GB)** | **70h**     |                                                |
| [LLaVA-ST](https://arxiv.org/abs/2501.08282)                                    | 2025.01 | LLM       | 视频均匀采样$N$帧，过SigLIP提取得到$H\times W$个Patch。为LLM扩充$N\times H \times W$个可学习的特殊Token。取特殊Token及其LLM分类头对应的权重矩阵对应的行向量的平均值，作为时间位置编码，直接加到视觉Patch作为视觉Token。对视觉Token进行初步压缩，即在等距划分若干区域内应用平均池化作为区域特征，使用交叉注意力得到视觉Token强化的区域特征。然后分空间压缩与时间压缩——空间压缩重复初步压缩的步骤；时间压缩将时间等距分为若干区域，在区域内使用平均池化，使用交叉注意力得到增强的区域特征。最后将两种压缩得到的视觉Token，与查询文本Token合并，输入到LLaVA-OneVision 7B，输出的时间戳与二维位置坐标也使用词表扩充的$N\times H \times W$个可学习的特殊Token。<br>一阶段：做画面描述任务，仅训练时空压缩模块<br>二阶段：做时序定位 + 空间定位任务<br>三阶段：做综合时序任务                                                                                                                                                                                                                                                                                                      | 三阶段训练<br>时间位置编码<br>**视觉Token压缩**<br>输出时间戳/看，空间坐标的特殊Token<br>数据增强，发布ST-Align数据集                                                                           | **48 Nvidia A100**       | **72h**     |                                                |
| [LMR](https://arxiv.org/abs/2405.12540)                                         | 2024.05 | VLM       | 视频通过MiniGPT-v2生成画面文本描述；通过CLIP/InternVideo提取视频特征$\mathbf{F}_v$。<br>画面文本与查询文本经过CLIP/LLaMA生成文本特征$\mathbf{F}'_t$/$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F_t'}$分别与$\mathbf{F}_t$做共享参数的交叉注意力融合，拼在一起进入编码器做自注意力，以$F_t$作为查询进入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 输入视频与文本<br>将**等价视觉描述信息**注入编码器                                                                                                                            | **1 Nvidia RTX 3090**    | **2h~5h**   |                                                |
| [MeCo](http://arxiv.org/abs/2503.09027)                                         | 2025.03 | LLM       | 架构完全源于E.T.Chat。<br>视频过ViT-G + Q-Former + 帧压缩器，把每帧的若干视觉Patch压缩成一个视觉Token，与查询文本Token一起输入到Phi-3-Mini-3.8B/Qwen2-7B，输出形如`[(<tst>)\|(描述文本<ent>)]`。其中Transition Token`<tst>`表示背景过渡，Event Token`<ent>`表示与查询文本相关的事件。<br>训练集中的每个样本已知片段数量及其事件描述。每个`<tst>`/`<ent>`特殊Token代表着一个片段，让每一帧的视觉`Token`与这些特殊Token的隐藏状态做相似度匹配，取Top-1给每一帧分配到一个片段。片段数量由LLM输出分类头的交叉熵Loss训练，语义由相似度匹配的交叉熵Loss实现。<br>数据增强，使用MiniCPM-V2.6/GPT-4o-mini生成片段的语言描述。                                                                                                                                                                                                                                                                                                                                                 | **视觉Token压缩**<br>LLM不输出任何时间戳文本或特殊Token，而是通过特殊Token的隐藏状态接匹配头实现<br>数据增强，但未发布数据集<br>                                                                        | **4 Nvidia A100 (80GB)** |             |                                                |
| [MLLM-TA](https://ieeexplore.ieee.org/document/10777595/)                       | 2024.12 | LLM       | 视频均匀采样，过ViT-L，得到视觉特征$\mathrm{F}_v$。查询文本过CLIP文本编码器，得到文本特征$\mathrm{F}_v$。两者过交叉注意力，得到文本增强的视觉特征，过时序卷积层得到$w_{\mathrm{vis}}$，预测每帧是否在所求区间内，此处有交叉熵损失。将增强的视觉Token与文本Token输入到Vicuna-7B/Vicuna-13B，输出的文本包含很多事件的片段预测值，Split后经过CLIP文本编码器得到单个向量，于是每个帧都能对应到某个事件，某个事件都能对应到单个向量，得到$w_{\mathrm{text}}$，它应该与$w_{\mathrm{vis}}$贴近，此处有L1 Loss。                                                                                                                                                                                                                                                                                                                                                                                                                                            | **增强视觉Token**<br>**LLM输出末端有下游相似度对齐**<br>                                                                                                                 |                          |             |                                                |
| [Momentor](https://arxiv.org/abs/2402.11435)                                    | 2024.06 | VLM       | 构建$N$个可学习的时间锚点，**线性插值得到连续的时间编码空间**，且有锚点滑动窗口反向机制，让时间编码空间更连续。<br>视频通过ViT + 线性层 + 时间位置编码，与文本Token合并，输入到LLaMA-2。<br>一阶段：做描述任务，仅训练线性层，把视觉Token对齐到文本嵌入空间<br>二阶段：训练VTG任务<br>三阶段：在自己构建的Moment-10M数据集上训练，使用Grounding DINO + PyScene Detect + Vicuna建语义连贯的时间片段多任务数据集，保留通用能力                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 多阶段<br>时间编码<br>数据增强，发布Moment-10M数据集                                                                                                                      |                          |             |                                                |
| [Mr.BLIP](http://arxiv.org/abs/2406.18113)                                      | 2024.06 | VLM       | 视频帧特征与时间戳文本Token交替插入，输出时间戳纯文本，对VLM进行微调。<br>架构与模型无关，在BLIP-2、Qwen2.5-VL、GPT-4o均有效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 暴力做法                                                                                                                                                     |                          |             |                                                |
| [NumPro](https://arxiv.org/abs/2411.10332)                                      | 2024.11 | VLM       | 直接在视频的每一帧右下角覆盖红色40号字体的数字，交给VLM直接进行下游任务。既可以免训练，也可以只训练视觉Token线性层与LoRA微调大模型。实验证明Qwen2-VL-7B、Qwen2-VL-72B、LLaVA-Video-7B、LLaVA-OneVision-7B、LongVA-7B-DPO均有效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 一种暴力的时间位置编码<br>数据增强，发布NumPro-enhanced数据集                                                                                                                 | **8 Nvidia H800**        |             |                                                |
| [Seq2Time](http://arxiv.org/abs/2411.16932)                                     | 2024.11 | VLM       | 架构完全源于TimeChat。<br>将时间戳映射到`[0, 1]`之间的相对坐标，取高四位小数，格式化成四个特殊的数码Token。<br>数据增强，用若干图片-文本对来模拟一个视频，用LongVA做图像描述。<br>多下游任务联合训练，包括时间定位、事件描述、时序推理                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 使用特殊时间戳Token<br>数据增强，但未发布数据集                                                                                                                             | **8 Nvidia A100 (80GB)** |             |                                                |
| [SeViLA](http://arxiv.org/abs/2305.06988)                                       | 2023.05 | VLM       | 视频通过ViT得到帧画面特征，过Q-Former和线性层，与文本查询一起注入到定位器FlanT5，将生成的第一个Token为"Yes"的概率作为帧得分，设置阈值超参数表示一段区间内能容许最大数量的低分帧，得到VTG任务的答案。<br>把帧得分作为权重注入到帧画面特征，过Q-Former和线性层，得到多帧特征，与文本查询一起注入到回答器FlanT5，得到QA任务的答案。然后输入单帧特征，看看QA任务答案是否正确，如果正确则标记为关键帧，生成伪标签，反向训练定位器FlanT5。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 弱监督，只需视频与查询文本，无需标注时间戳<br>QA+VTG双向辅佐训练<br>绘制"Yes"的Token生成概率曲线，按阈值生成区间提案                                                                                   | **4 Nvidia A6000**       | **12h**     |                                                |
| [SlowFocus](https://openreview.net/forum?id=FOkKndty5B)                         | 2024.09 | LLM       | 对整个视频使用ViT-L进行稀疏采样，获取稀疏帧，检索相关片段。对片段进行密集采样，将稀疏采样与密集采样加上时序编码，过自设计的注意力，合并文本查询，输入到LLM。<br>时序编码由一个可学习矩阵构成，将时间分成1000个步数，作为时间Token，直接加到视觉Token中。<br>自设计注意力机制，用稀疏采样通过交叉注意力增强密集采样，随后稀疏采样、密集采样、文本查询合并，作为Vicuna-1.5 7B的输入。<br>一阶段：用图像-文本对训练线性层对齐+时间编码器，实现视觉对齐<br>二阶段：训练密集视频描述+时序定位任务<br>三阶段：适应自设计注意力机制，输入稀疏采样和密集采样。                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 三阶段训练<br>新注意力机制<br>分层采样，稀疏+密集<br>时间编码<br>数据增强，发布FineAction-CGR数据集                                                                                        |                          |             |                                                |
| [SMORE](https://arxiv.org/html/2601.09350v1)                                    | 2026.01 | LLM       | 给定查询文本，使用LLM拆解为对象集合与动作集合，利用InstructBLIP做VQA任务，判断某一帧是否包含指定的集合与动作。然后使用BLIP2生成画面描述，如果包含则生成细节画面描述，不包含则生成通用画面描述。帧视觉特征与查询文本做相似度，用[CLIPScore](https://arxiv.org/pdf/2104.08718)计算真视觉特征、查询文本、画面描述的相似度，两个相似度加权得到相关分数，以此为权重调整画面描述。<br>按时间顺序维护一个锚点帧$f_a$，如果当前帧$f_i$与锚点帧德相似度`>0.9`，则用锚点帧替换它，反之使用SVD，解方程$M_i = \left[\begin{matrix}f_a\\ f_1\end{matrix}\right] \approx U \Sigma V^T$解得$\Sigma \rightarrow f_a$。<br>                                                                                                                                                                                                                                                                                                                                                              | 双流，空间与时间<br>**视觉Token压缩**                                                                                                                                | **8 Nvidia A6000**       | **12h**     |                                                |
| [SpaceVLLM](https://arxiv.org/abs/2503.13983)                                   | 2025.04 | VLM       | 视频均匀采样$N_v$帧过SigLIP得到若干视觉Token，准备$N_v$个可学习的Query，紧贴在每一帧的视觉Token之后，与查询文本Token输入到Qwen2。输出包含`<sec>1.2</sec>`的时间戳纯文本相对坐标与视觉Token隐藏状态。单帧视觉Tokens与查询文本Token过交叉注意力，增强单帧视觉Tokens，与隐藏状态过交叉注意力，增强隐藏状态，接MLP预测单帧的空间坐标$(x,y,w,h)$。时间戳给定了帧序列范围，提取这些帧位置对应的空间坐标即可。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 数据增强，发布Uni-STG数据集<br>特殊Token提取隐藏状态，过交叉注意力做下游任务                                                                                                           | **16 Nvidia A800**       | **24h**     |                                                |
| [TemporalVLM](http://arxiv.org/abs/2412.02930)                                  | 2024.12 | LLM       | 视频划分为6个片段，每个片段均匀采样96帧，过ViT-G得到视觉特征，与查询文本+时间戳共同进入InstructBLIP **Q-Former**做压缩，得到帧Token，过长度为32、步长为16的Video-LLaMA **Q-Former** + 自注意力提取片段特征，过BiLSTM得到视频视觉特征，与查询文本Token送入LLaMA-2 7B/LLaMA-3 8B，直接输出纯文本时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 处理长视频<br>**增强视觉Token**<br>数据增强，发布IndustryASM数据集<br>                                                                                                      |                          |             |                                                |
| [TGB](https://arxiv.org/abs/2402.16050)                                         | 2024.10 | VLM       | 视频通过RAFT提取光流特征 + CNN + MLP + 位置编码，文本通过Tokenizer + RoPE位置编码，两者过交叉注意力得到增强的视觉特征，每个视觉特征对应的时间戳都会经过分类头，分类为`<BEGIN>`、`<NONE>`、`<END>`三种，得到预选时间片段，极大地减少了VLM的输入Token数量。<br>这些片段作为原先工作中“未经剪辑的视频”，过VLM推理。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 视频片段预筛选，降低VLM的Token数量<br>视频位置编码                                                                                                                          | **1 Nvidia A100 (80GB)** |             |                                                |
| [TimeChat](http://arxiv.org/abs/2312.02051)                                     | 2023.12 | VLM       | 视频的每一帧都通过ViT + InstructBLIP Image Q-Former，通过滑动窗口截取 + Video-LLaMA Video Q-Former获得片段Token，过线性层对齐到LLaMA-2嵌入空间，与STT字幕文本、用户查询文本一起合并，作为LLaMA-2输入，输出答案文本。仅微调两个Q-Former和LLM的LoRA。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 用GPT做数据增强，发布TimeIT数据集<br>用预训练的Q-Former提取视频特征。<br>未使用时间位置编码注入视觉特征，而是将时间信息变为文本Token，过InstructBLIP Q-Former。<br>利用了原视频的音频→字幕文本信息<br>视觉Token融入时间戳未知编码信息0<br> |                          |             |                                                |
| [TimeCraft](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00720.pdf) | 2024.10 | VLM       | 只给定原始视频与文本查询集合，不给定时刻的Ground Truth。<br>每个GVQA模型共享一套参数，包含ViT/BERT的编码器、融合文本与视觉特征的Transformer、定位头、QA文本分类头，可以输出时间区间与QA文本。<br>将原始文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-1，输出时间区间$t$、答案文本$a$、对偶文本查询$\tilde{q}$；将答案文本$a$与视频特征$\mathrm{F}_v$输入到GQVA-2，得到对偶答案文本$\tilde{a}$；将对偶文本查询$\tilde{q}$与视频特征$\mathrm{F}_v$输入到GQVA-3，得到对偶答案文本$\tilde{a}$；将文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-4，得到答案文本$a$。对两个$a$、两个$\tilde{q}$、两个$\tilde{a}$进行对齐即可训练。                                                                                                                                                                                                                                                                                                                                            | 弱监督<br>数据增强，添加对偶问题，设计$f(f(q))=q$的正向+逆向推理机制<br>设计大量Loss函数                                                                                                 | **2 Nvidia A6000**       | **2m~0.3h** |                                                |
| [TimeMarker](https://arxiv.org/abs/2411.18211)                                  | 2024.11 | LLM       | 视频动态采样$N=\min(N_{\mathrm{max}}, 2\cdot T_v)$帧，过ViT-L + 线性层得到每帧的Patch图，按照$\begin{cases}4\times 4 &,N>\frac{N_{\mathrm{max}}}{2}\\4\times 2&,\frac{N_\mathrm{max}}{4}<N<\frac{N_\mathrm{max}}{2}\\2\times 2&,N<\frac{N_\mathrm{max}}{4}\end{cases}$的二维卷积核做平均池化，以压缩视觉Token数量，与查询文本Token注入到LLaMA-8B中。<br>不使用特殊Token分隔帧与帧之间视觉Token，而是直接用文本时间标记`Second{i}`。<br>一阶段：训练视觉编码器与线性层，使用图像-文本对做对齐模态<br>二阶段：LoRA微调LLM，做VTG任务与QA问答<br>三阶段：指令微调，提高通用推理能力                                                                                                                                                                                                                                                                                                                             | 三阶段训练<br>让时间戳文本Token作为时间位置编码<br>**动态帧采样**<br>**视觉Token压缩**<br>**线性投影层换激活函数，叠两层MLP**<br>数据增强，但未发布数据集                                                      |                          |             |                                                |
| [TimeRefine](http://arxiv.org/abs/2412.09601)                                   | 2024.12 | VLM       | 通用架构。<br>期望VLM输出进行$K$次细化，形如`<seg_start>(s0 to e0<offset>ds0 es0<refine>)+<seg_end>`。训练集的期望输出也通过高斯分布制造细化过程。其中时间戳为纯文本，但是也同时会给`<refine>`Token的隐藏状态接一个回归头做L1 Loss。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 即插即用<br>多次迭代细化输出                                                                                                                                         |                          |             |                                                |
| [TimeSuite](https://arxiv.org/abs/2410.19702)                                   | 2024.10 | VLM       | 视频分成$K$个片段，每个片段采样$T$帧，过UML-L + **Q-Former** + **Token Shuffle，即把相邻的帧特征按嵌入维度拼接，过MLP变回原先的嵌入维度**，初始时等价于平均池化。<br>受CPVT的启发，作者提出TAPE时序位置编码，作为残差直接相加。<br>视觉特征与文本Token合并后，输入到Mistral-7B中。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | **视觉Token压缩**<br>时间位置编码<br>数据增强，发布TimePro数据集<br>                                                                                                         |                          |             |                                                |
| [TPE-VLLM](https://aclanthology.org/2025.coling-main.655/)                      | 2025.01 | VLM       | 通用架构。<br>数据增强，增加边界感知任务（给定片段是否包含查询事件的二分类任务）、时间推理任务（多事件与多片段匹配、事件排序、给定事件询问发生在什么片段的选择题、给定片段询问发生什么事件的选择题）<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 即插即用<br>三阶段训练<br>数据增强，但未发布数据集                                                                                                                            |                          |             |                                                |
| [TRACE](https://arxiv.org/abs/2410.05643)                                       | 2024.11 | LLM       | 视频过ViT-L + **QFormer压缩**。对于片段描述任务，输入自带时间戳与显著性分数，对每个数字字符分配新Token，例如`<0>~<9>`、`<小数点>`。向Mistral-v0.2-7B输入视觉Token、查询文本Token、片段描述请求文本Token，对其中时间戳/显著性分数的Token，提取隐藏状态过对应的分类/回归头。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 时间Token<br>                                                                                                                                              | **16 ATN 910B**          | **10d**     |                                                |
| [UniTime](https://arxiv.org/abs/2506.18883v2)                                   | 2025.06 | LLM       | 短视频放大画面过ViT，长视频过ViT + 双线性插值降低分辨率，超长视频划分成若干片段，先长视频检索，再短视频定位。既使用RoPE位置编码，也使用纯文本模版`timestamp: $t_i$ seconds`添加到每帧Tokens之前。随后与查询文本Token送入Qwen-2-VL-7B。<br>将一个视频的多个查询文本送入LLM，同时训练，为此需要更改RoPE的索引、调整Mask使得查询文本之间不可见，避免重复加载视频的开销。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 视频自适应采样<br>**视觉Token压缩**<br>时序位置编码<br>二分递归查找定位<br>批量训练策略，加速训练                                                                                            |                          |             |                                                |
| [VideoChat-TPO](http://arxiv.org/abs/2412.19326)                                | 2024.12 | LLM       | 架构完全源于VideoChat2。<br>视频帧通过UMT-L提取帧特征，视频本题通过InternVideo2提取视频特征，过**Q-Former**对齐；查询文本通过Chinese-LLaMA-Alpaca提取文本Token，两者合并输入到Mistral-7B。<br>根据文本查询生成特殊Token，按照所执行的下游任务（跟踪、分割、时间定位）进行分类，向LLM注入对应的可学习Query作为Token，将其隐藏状态输入到下游任务头中。<br>一阶段：训练特殊Token的下游任务分类准确率<br>二阶段：训练下游任务<br>三阶段：训练混合下游任务<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 三阶段训练<br>多任务推理<br>特殊任务Token提取隐藏状态，接入专用模型做下游任务                                                                                                            | **32 Nvidia A100**       | **63.5h**   |                                                |
| [VideoExpert](https://arxiv.org/abs/2504.07519)                                 | 2025.04 | LLM       | 视频取**所有帧**过ViT，取其第0个`<CLS>`Token，作为时间Token。<br>视频借鉴H.264视频压缩编码，划分为若干组，每组均匀采样$u$帧 + 少量随机采样帧I帧，计算该帧CLS Token与Patch的相似度，按阈值筛选后聚类成若干组，若两个Patch的位置相同、时间相邻、聚类到同一组，则只保留I帧的Patch，作为空间Token。<br>LLM使用Vicuna-1.5 7B。空间LoRA输入空间Token + 查询文本Token，输出事件描述和`<LOC>`Token。时间LoRA输入时间Token + `<LOC>`Token，通过预测头输出时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 输入双流，分为空间Token与时间Token<br>**视觉Token压缩**<br>多LoRA                                                                                                         |                          |             |                                                |
| [VTG-LLM]()                                                                     | 2024.05 | LLM       | 视频均匀采样96帧，通过ViT + QFormer，使用门控从`0`到`1`叠加相对时间+绝对时间嵌入，输出后使用线性差值计算时间戳。<br>**引入视觉Token压缩，借鉴SoftMoE架构，筛出Top-K(k=256)个视觉Token，并对压缩方式做了消融实验**。<br>使用Gemini 1.5重新标注原有数据集中质量较差的条目。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 时间编码<br>**视觉Token压缩**<br>数据增强，发布VTG-IT-120K数据集                                                                                                           | **16 ATN 910B**          | **40h**     |                                                |
| [VTimeLLM](https://arxiv.org/abs/2311.18445)                                    | 2023.11 | VLM       | 三阶段训练。<br>一阶段：使用图片-文本对数据集，图片经过ViT与线性适配器，把视觉特征投影到Vicuna的文本Token空间。仅训练线性适配器，用数据集文本和Vicuna输出文本进行训练。<br>二阶段：使用时序定位的QA数据集，视频拆成100张图片，经过ViT与线性适配器，与Q合并，作为Vicuna输入，以自然语言输出A。仅训练Vicuna附着的LoRA。<br>三阶段：使用高质量的时序定位数据集，重复二阶段，减少过拟合，恢复通用能力。仅训练Vicuna附着的另一个新加的LoRA。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 三阶段预训练<br>数据增强<br>纯LLM微调为VLM                                                                                                                             | **1 Nvidia  RTX 4090**   | **30h**     |                                                |

## RL

|                                                  | 日期      | 架构  | Pipeline                                                                                                                          | 标签                                          | 算力                | 训练时间    | Charades-STA指标(`R@0.3`,`R@0.5`,`R@0.7`,`mIoU`) |
| ------------------------------------------------ | ------- | --- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ----------------- | ------- | ---------------------------------------------- |
| [VideoChat-R1](https://arxiv.org/abs/2504.06958) | 2025.04 | RL  | 给定查询文本与低分辨率视频，使用Qwen2-7B/Qwen2.5-7B生成若干粗糙回答，将检索的高分辨率片段重新输入VLM，得到精细回答。<br>让Qwen2.5-72B进行评判，得到一系列得分，据此作为奖励信号。                       | 强化学习，GRPO<br>细化答案                           |                   |         |                                                |
| [MUSEG](https://arxiv.org/abs/2505.20715)        | 2025.05 | RL  | 将多片段定位任务引入强化学习，设计格式正确的奖励、必须输出时间戳的奖励、片段匹配正确的奖励。<br>一阶段：适用所有奖励<br>二阶段：移除时间戳奖励，允许模型探索更有的推理形式，防止过拟合于模版                                | 强化学习<br>引入新任务<br>二阶段训练<br>新奖励函数             |                   |         |                                                |
| [Time-R1](https://arxiv.org/abs/2503.13377)      | 2025.05 | RL  | 视频按2FPS采样，动态调整分辨率使得总输入像素点数量小于280万。设计格式正确的奖励、片段匹配正确的奖励。<br>一阶段：少量数据用LoRA微调Qwen-2.5-VL-7B，学习`<think>`与`<answer>`标签的CoT。<br>二阶段：GRPO | 强化学习，CoT<br>冷启动，二阶段训练<br>数据增强，发布TVGBench数据集 | **8 Nvidia A100** | **15h** | `78.1, 60.8, 35.3, ????`                       |
| [TempSamp-R1](https://arxiv.org/abs/2509.18056)  | 2025.09 | RL  | GRPO生成$N$个解，选中其中一个作为Ground Truth训练其余$N-1$个解。设计格式正确的奖励、片段匹配正确的奖励。额外引入了一个非线性模块，对奖励函数进行整型，防止得分差距过大导致不收敛。                             | 强化学习，CoT                                    | **8 Nvidia A100** | **15h** |                                                |
| [Invert4TVG](https://arxiv.org/abs/2508.07388)   | 2025.10 | RL  | 引入文本→片段 + 片段→文本两种任务，称为正+反任务。设计格式正确的奖励、正/反任务片段匹配正确的奖励。                                                                             | 引入新任务<br>强化学习，CoT，GRPO                      |                   | **80h** |                                                |
| [TimeLens](http://arxiv.org/pdf/2512.14698v1)    | 2025.12 | RL  | 视频经过采样，过ViT，在每帧视觉特征之前插入纯文本时间戳，替代时间位置编码。<br>GRPO训练，但是不需要CoT，因为实验证明CoT推理能力不强，还浪费Token。优先训练准确率较低的难训练集，并且使用早停策略避免过拟合。                 | 时序位置编码<br>数据增强，发布TimeLens-100K数据集           | **8 Nvidia H20**  | **4h**  |                                                |
## 杂项

|                                                           | 日期      | 架构       | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 标签                                                             | 算力                       | 训练时间          | Charades-STA指标(`R@0.3`,`R@0.5`,`R@0.7`,`mIoU`) |
| --------------------------------------------------------- | ------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------ | ------------- | ---------------------------------------------- |
| [LLaViLo](https://ieeexplore.ieee.org/document/10350951/) | 2023.10 | DETR+VLM | 视频通过SlowFast提取片段特征$\mathbf{F}_v$。<br>查询文本通过CLIP提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过交叉注意力+Encoder，对齐到LLaMA的文本空间，得到视觉Token。仅在LLaMA的Top-N层注入视觉Token，并使用从`0`到`1`的门控因子控制视觉Token的特征明显性，防止训练初期的杂乱视觉Token影响LLaMA能力。<br>LLaMA同时输入DETR中的可训练锚点Token，走DETR的回归输出，损失函数是IoU；同时走LLaMA的文本输出，损失函数是交叉熵。DETR擅长定位，VLM擅长语义理解。                                                                                                                                                                                                             | 缝合DETR+VLM的Loss<br>完全冻结LLM，无LoRA，只训练视觉到文本Token的Adapter，参数量增长极小 |                          |               |                                                |
| [ChatVTG](https://arxiv.org/abs/2410.12813)               | 2024.10 | 对比学习     | 视频通过VideoChatGPT/VideoChat2生成Action、Place、Dressing、Emotion、Interaction五个维度的片段描述文本。<br>查询文本通过BERT得到查询文本特征。<br>两者做相似度匹配，选出最相似的片段作为粗粒度。以该片段为中心生成若干滑动窗口区间，重复相似度操作，选出最相似的片段作为细粒度最终答案。                                                                                                                                                                                                                                                                                                                                                          | 无训练，因为纯复用现有VLM-VTG的文本编码器，得到的查询文本特征与片段描述特征已经对齐。<br>纯对比学习        | **1 Nvidia A100**        | **无训练**       |                                                |
| [VERIFIED](https://arxiv.org/abs/2410.08593)              | 2024.10 | 数据集      | 视频通过PySceneDetect，调整阈值是的切分成$L$个片段，取每个片段的中间帧作为关键帧，使用LLaVA-1.6加上查询文本来提取关键帧的前景画面描述$\{D_i^{(\mathrm{fg})}\}_{i\in[1,L]}$、背景画面描述$\{D_i^{(\mathrm{bg})}\}_{i\in[1,L]}$、整体画面描述$\{D_i\}_{i\in[1,L]}$。三个画面描述加上查询文本，过Mistral-7B将查询文本改写成$N_s$个侧重点不同的静态增强查询文本。<br>查询文本通过LLM，得到$N_{qa}$个针对查询文本的问题，加上视频整体与查询文本，通过Gemini得到答案与详细画面描述，最后将问答对 + 详细画面描述 + 文本查询输入到LLM，整合成$N_d$个连贯且包含动态细节的候选描述。<br>事先用查询文本生成若干包含不存在物体、属性颠倒的负查询样本，作为训练集，微调一个UMT模型，微调时使用对比Loss迫使模型拉近视觉特征与正样本的距离，拉大与负样本的距离；使用匹配Loss做二分类判断查询文本与画面是否匹配。使用该UMT模型对$N_s+N_q$个查询打分，选择Top-1作为最终查询文本。<br> | 增强查询文本<br>数据增强，发布数据集                                           | **Undefined**            | **Undefined** | **Undefined**                                  |
| [ReVisionLLM](https://arxiv.org/abs/2411.14901)           | 2024.11 | 候选+LLM   | 视频切分成若干片段，通过ViT-L + 交叉注意力 + 自注意力生成稀疏特征，与文本查询进入Vicuna-v1.5-7B，得到片段内的候选时间戳区间。<br>根据候选时间戳区间，通过ViT-L + 全局CLS Token + 线性层生成密集特征，与文本查询进入Vicuna-v1.5-7B，对每个候选结果按照LLM输出的平均熵倒数排序确定置信度。<br>一阶段：训练短视频密集特征定位能力，引入对比负样本，迫使模型回答“指定时间在给定片段中不存在”<br>二阶段：训练长视频稀疏特征定位能力。<br>                                                                                                                                                                                                                                                                                | 二阶段训练<br>二分递归查找定位<br>分层采样，稀疏+密集                                | **8 Nvidia A100**        |               |                                                |
| [MomentSeeker](https://arxiv.org/abs/2502.12558v5)        | 2025.02 | 数据集      | 发布长视频数据集MomentSeeker。<br>在COVR/MM-RET/EsV/VLM2VEC/GPT-4o/Gemini-2.5.Pro/Qwen2.5-VL-7B/Qwen2.5-VL-32B/InternVL3/LLava-Video/TimeChat/Lita/VideoLLaMA3上做了评估。                                                                                                                                                                                                                                                                                                                                                                                | 数据增强，发布MomentSeeker数据集                                         | **8 Nvidia A800**        | **Undefined** |                                                |
| [VideoMind](http://arxiv.org/abs/2503.13444)              | 2025.04 | Agent    | 借鉴Chain-Of-LoRA架构，分为Planner、Grounder、Verifier、Answerer，它们都是基座LLM切换不同的LoRA。Planner决定调用哪些角色的VLM，Grounder负责定位，Verifier负责判别、Answerer负责逻辑推理。每一步传入的视频分辨率逐渐加大。VLM基座模型为Qwen2-VL-2B/Qwen2-VL-7B。<br>Grounder的输入有一个`<REG>`Token，提取其隐藏状态过MLP得到$e_r$，加上一个可广播、可学习的查询和正弦位置编码；将每帧的Patch过一维平均池化 + MLP得到$e_v$，加上一个可广播、可学习的查询。两者维度拼接后过Transformer，得到上下文增强的$e'_r$和$e'_v$。$e'_v$经过4层Conv1D + LayerNorm + SiLU生成4层视觉特征，拼成一行送到回归头输出最终的起始/终止时间戳，送到分类头判定片段置信度。<br>                                                                                                | 多LoRA<br>Agent<br>特殊Token提取隐藏状态做下游任务<br>**增强视觉Token**          |                          |               | `73.5, 59.1, 31.2, 50.2`                       |
| [R2-Tuning](https://arxiv.org/abs/2404.00801)             | 2024.04 | 对比学习     | R<sup>2</sup>模块：给定查询文本特征与一帧的第$k$层ViT视觉特征（包含Patch与CLS Token），两者过MLP对齐嵌入维度，过交叉注意力得到视觉增强的查询文本特征作为残差，过最大池化，乘以权重缩放后与CLS Token相加，乘以系数后与上一个隐藏状态$h^{k-1}$相加，与查询文本特征送入Transformer（多头交叉注意力 + 多头自注意力 + FFN），输出本层的隐藏状态$h^k$。<br>提取ViT的后$K$层状态，用一维卷积做下采样构建特征金字塔，作为输入特征。<br>额外设计两个对比学习Loss，核心目标都是拉近同一个训练样本的同一层ViT的视频特征与查询文本特征的距离。视频Loss远离不同样本，层级Loss原理不同ViT层。<br>后续接二维卷积 + MLP做为任务头，直接做预测。                                                                                                                                                              | **视觉Token增强**                                                  | **1 Nvidia A100 (80GB)** |               | `70.9, 50.9, 37.0, 49.7`                       |


Ranking Loss：要求正样本得分与负样本得分至少相差$\Delta$。$\mathcal{L} = \max(0, \Delta - (S_{\mathrm{pos}} - S_{\mathrm{neg}}))$。

数据增强：
- 快速闪回的内容：“那一段男人在脑海中快速回想了全部内容？”
- 查询可以改写成：空间与时间、静态与动态，前景与背景，还有吗？

实验：
- 跨数据集迁移：在A数据集训练，在B数据集推理，衡量Zero-Shot能力。

LORA与DeepSpeed（训练优化方式）

给**增强视觉Token**的工作加上**视觉Token压缩**？