
|                                                                                 | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                  | 标签                                                         | 架构    |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ----- |
| [Grounding-Prompter](https://arxiv.org/abs/2312.17117)                          | 视频通过ASR与BLIP，生成字幕与画面文本描述。<br>文本不变。<br>与文本一起，直接送入LLM，以自然语言输出时间戳。                                                                                                                                                                                                                                                                                                                                                           | 输入仅文本<br>由自然语言输出                                           | VLM   |
| [VTG-GPT](https://arxiv.org/abs/2403.02076)                                     | 视频通过VLLM，生成画面文本描述。<br>文本经过LLM，进行纠错与细化。<br>视频与文本经过相似度筛选，按阈值直接得到时间戳，后附NMS。                                                                                                                                                                                                                                                                                                                                                  | 输入仅文本<br>由文本-文本相似度矩阵输出                                     | 提议    |
| [GPTSee](http://arxiv.org/abs/2403.01437)                                       | 视频通过VLLM，生成画面文本描述，经CLIP得到视频特征。<br>文本经过LLM，生成若干语义等价的文本，以此扩充数据集。<br>画面文本与任一等价文本做相似度，得到$L_v \times 1$序列，与视频特征拼接，送入视觉编码器，与原始文本一同送入编码器，生成跨模态特征，注入可学习查询与基于相似度的位置嵌入，送入解码器，过FFN。                                                                                                                                                                                                                                                  | 输入仅文本<br>将**等价查询文本信息**注入解码器<br>输出连续值                       | DETR  |
| [GroundVQA](https://arxiv.org/abs/2312.06505)                                   | 视频通过InternVideo，提取视频特征，经线性层对齐到文本特征。<br>文本经Tokenizer，提取文本特征。<br>视频特征与文本特征送入Flan-T5编码器，然后执行下游任务。                                                                                                                                                                                                                                                                                                                            | 输入视频与文本<br>用Decoder输出做下游任务                                 | VLM   |
| [LMR](https://arxiv.org/abs/2405.12540)                                         | 视频通过MiniGPT-v2生成画面文本描述；通过CLIP/InternVideo提取视频特征$\mathbf{F}_v$。<br>画面文本与查询文本经过CLIP/LLaMA生成文本特征$\mathbf{F}'_t$/$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F_t'}$分别与$\mathbf{F}_t$做共享参数的交叉注意力融合，拼在一起进入编码器做自注意力，以$F_t$作为查询进入解码器，过FFN。                                                                                                                                                                                         | 输入视频与文本<br>将**等价视觉描述信息**注入编码器                              | VLM   |
| [TEA](https://arxiv.org/abs/2406.17880)                                         | 视频通过LLaVA-v1.5/BLIP-2生成画面文本描述，通过3D-CNN提取视频特征。<br>画面文本与查询文本通过GloVe获取文本特征。<br>视频特征与画面文本拼接后过MLP，与查询文本做交叉注意力；画面文本单独与查询文本做交叉注意力；分别送入预测器。预测器接受两个输入，按照[Span Predictor](https://arxiv.org/pdf/2004.13931)的流程输出边界。这两个边界加权平均即为输出。                                                                                                                                                                                                   | 输入视频与**细化**文本<br>将**等价视觉描述信息**注入编码器                        | DETR  |
| [TFVTG](https://arxiv.org/pdf/2408.16219)                                       | 视频通过CLIP提取视频特征。<br>查询文本由LLM拆成若干有时间约束关系的子事件，通过CLIP提取文本特征。<br>分别对子事件进行DETR推理，得到帧-置信度曲线，设计动态&静态得分函数选出TopK个提案，得到若干子事件区间，排列组合得到若干总事件区间，按时间约束关系进行过滤，根据动态&静态得分排名出最佳答案。                                                                                                                                                                                                                                                           | 无训练<br>新Loss函数<br>手工设计子查询文本与合并机制                           | DETR  |
| [EI-VLG]()                                                                      | 视频通过LLaVA生成画面文本描述，过一层可训练的BERT得到画面文本特征，与文本查询进行对比学习。<br>画面文本特征与查询文本特征经过带残差的交叉注意力，与视频特征进行带残差的拼接，送到预测头。                                                                                                                                                                                                                                                                                                                       | 使用对比学习，对查询文本与画面文本进行对齐，然后再融合                                | DETR  |
| [DeVi](https://arxiv.org/abs/2409.04388)                                        | 视频通过GPT-4生成若干时间窗口尺度的片段描述，一起输入到GPT-4o，对每个描述进行改写，生成事件集合与全局摘要；通过CLIP得到视觉特征。<br>把事件集合、全局摘要、查询文本、选项文本送入GPT-4o/Gemini-2.0，得到答案。将答案对应区间内的视频特征与查询文本做相似度匹配，若相似度过低则继续多轮对话，制度相似度达标或触发对话轮次上限为止。                                                                                                                                                                                                                                       | 无训练<br>提出密集VQA+VTG任务<br>引入查询文本-片段特征相似度作为重复推理的依据，会导致推理时间变长。 | LLM   |
| [ChatVTG](https://arxiv.org/abs/2410.12813)                                     | 视频通过VideoChatGPT/VideoChat2生成Action、Place、Dressing、Emotion、Interaction五个维度的片段描述文本。<br>查询文本通过BERT得到查询文本特征。<br>两者做相似度匹配，选出最相似的片段作为粗粒度。以该片段为中心生成若干滑动窗口区间，重复相似度操作，选出最相似的片段作为细粒度最终答案。                                                                                                                                                                                                                                          | 无训练，因为纯复用现有VLM-VTG的文本编码器，得到的查询文本特征与片段描述特征已经对齐。<br>纯对比学习    | 纯对比学习 |
| [TimeCraft](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00720.pdf) | 只给定原始视频与文本查询集合，不给定时刻的Ground Truth。<br>每个GVQA模型共享一套参数，包含ViT/BERT的编码器、融合文本与视觉特征的Transformer、定位头、QA文本分类头，可以输出时间区间与QA文本。<br>将原始文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-1，输出时间区间$t$、答案文本$a$、对偶文本查询$\tilde{q}$；将答案文本$a$与视频特征$\mathrm{F}_v$输入到GQVA-2，得到对偶答案文本$\tilde{a}$；将对偶文本查询$\tilde{q}$与视频特征$\mathrm{F}_v$输入到GQVA-3，得到对偶答案文本$\tilde{a}$；将文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-4，得到答案文本$a$。对两个$a$、两个$\tilde{q}$、两个$\tilde{a}$进行对齐即可训练。 | 弱监督<br>数据增强，添加对偶问题，设计$f(f(q))=q$的正向+逆向推理机制<br>设计大量Loss函数   | VLM   |
| [VideoLights](https://arxiv.org/abs/2412.01558)                                 |                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                            |       |



数据标注Pipeline：

| [VERIFIED](https://arxiv.org/abs/2410.08593) |     |     |     |
| -------------------------------------------- | --- | --- | --- |
