
|                                                                                 | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 标签                                                                                                      | 架构    |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | ----- |
| [Grounding-Prompter](https://arxiv.org/abs/2312.17117)                          | 视频通过ASR与BLIP，生成字幕与画面文本描述。<br>文本不变。<br>与文本一起，直接送入LLM，以自然语言输出时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 输入仅文本<br>由自然语言输出                                                                                        | VLM   |
| [VTG-GPT](https://arxiv.org/abs/2403.02076)                                     | 视频通过VLLM，生成画面文本描述。<br>文本经过LLM，进行纠错与细化。<br>视频与文本经过相似度筛选，按阈值直接得到时间戳，后附NMS。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 输入仅文本<br>由文本-文本相似度矩阵输出                                                                                  | 提议    |
| [GPTSee](http://arxiv.org/abs/2403.01437)                                       | 视频通过VLLM，生成画面文本描述，经CLIP得到视频特征。<br>文本经过LLM，生成若干语义等价的文本，以此扩充数据集。<br>画面文本与任一等价文本做相似度，得到$L_v \times 1$序列，与视频特征拼接，送入视觉编码器，与原始文本一同送入编码器，生成跨模态特征，注入可学习查询与基于相似度的位置嵌入，送入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 输入仅文本<br>将**等价查询文本信息**注入解码器<br>输出连续值                                                                    | DETR  |
| [GroundVQA](https://arxiv.org/abs/2312.06505)                                   | 视频通过InternVideo，提取视频特征，经线性层对齐到文本特征。<br>文本经Tokenizer，提取文本特征。<br>视频特征与文本特征送入Flan-T5编码器，然后执行下游任务。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 输入视频与文本<br>用Decoder输出做下游任务                                                                              | VLM   |
| [LMR](https://arxiv.org/abs/2405.12540)                                         | 视频通过MiniGPT-v2生成画面文本描述；通过CLIP/InternVideo提取视频特征$\mathbf{F}_v$。<br>画面文本与查询文本经过CLIP/LLaMA生成文本特征$\mathbf{F}'_t$/$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F_t'}$分别与$\mathbf{F}_t$做共享参数的交叉注意力融合，拼在一起进入编码器做自注意力，以$F_t$作为查询进入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 输入视频与文本<br>将**等价视觉描述信息**注入编码器                                                                           | VLM   |
| [TEA](https://arxiv.org/abs/2406.17880)                                         | 视频通过LLaVA-v1.5/BLIP-2生成画面文本描述，通过3D-CNN提取视频特征。<br>画面文本与查询文本通过GloVe获取文本特征。<br>视频特征与画面文本拼接后过MLP，与查询文本做交叉注意力；画面文本单独与查询文本做交叉注意力；分别送入预测器。预测器接受两个输入，按照[Span Predictor](https://arxiv.org/pdf/2004.13931)的流程输出边界。这两个边界加权平均即为输出。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 输入视频与**细化**文本<br>将**等价视觉描述信息**注入编码器                                                                     | DETR  |
| [TFVTG](https://arxiv.org/pdf/2408.16219)                                       | 视频通过CLIP提取视频特征。<br>查询文本由LLM拆成若干有时间约束关系的子事件，通过CLIP提取文本特征。<br>分别对子事件进行DETR推理，得到帧-置信度曲线，设计动态&静态得分函数选出TopK个提案，得到若干子事件区间，排列组合得到若干总事件区间，按时间约束关系进行过滤，根据动态&静态得分排名出最佳答案。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 无训练<br>新Loss函数<br>手工设计子查询文本与合并机制                                                                        | DETR  |
| [EI-VLG](http://arxiv.org/abs/2408.02336)                                       | 视频通过LLaVA生成画面文本描述，过一层可训练的BERT得到画面文本特征，与文本查询进行对比学习。<br>画面文本特征与查询文本特征经过带残差的交叉注意力，与视频特征进行带残差的拼接，送到预测头。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 使用对比学习，对查询文本与画面文本进行对齐，然后再融合                                                                             | DETR  |
| [DeVi](https://arxiv.org/abs/2409.04388)                                        | 视频通过GPT-4生成若干时间窗口尺度的片段描述，一起输入到GPT-4o，对每个描述进行改写，生成事件集合与全局摘要；通过CLIP得到视觉特征。<br>把事件集合、全局摘要、查询文本、选项文本送入GPT-4o/Gemini-2.0，得到答案。将答案对应区间内的视频特征与查询文本做相似度匹配，若相似度过低则继续多轮对话，制度相似度达标或触发对话轮次上限为止。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 无训练<br>提出密集VQA+VTG任务<br>引入查询文本-片段特征相似度作为重复推理的依据，会导致推理时间变长。                                              | LLM   |
| [ChatVTG](https://arxiv.org/abs/2410.12813)                                     | 视频通过VideoChatGPT/VideoChat2生成Action、Place、Dressing、Emotion、Interaction五个维度的片段描述文本。<br>查询文本通过BERT得到查询文本特征。<br>两者做相似度匹配，选出最相似的片段作为粗粒度。以该片段为中心生成若干滑动窗口区间，重复相似度操作，选出最相似的片段作为细粒度最终答案。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 无训练，因为纯复用现有VLM-VTG的文本编码器，得到的查询文本特征与片段描述特征已经对齐。<br>纯对比学习                                                 | 纯对比学习 |
| [TimeCraft](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00720.pdf) | 只给定原始视频与文本查询集合，不给定时刻的Ground Truth。<br>每个GVQA模型共享一套参数，包含ViT/BERT的编码器、融合文本与视觉特征的Transformer、定位头、QA文本分类头，可以输出时间区间与QA文本。<br>将原始文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-1，输出时间区间$t$、答案文本$a$、对偶文本查询$\tilde{q}$；将答案文本$a$与视频特征$\mathrm{F}_v$输入到GQVA-2，得到对偶答案文本$\tilde{a}$；将对偶文本查询$\tilde{q}$与视频特征$\mathrm{F}_v$输入到GQVA-3，得到对偶答案文本$\tilde{a}$；将文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-4，得到答案文本$a$。对两个$a$、两个$\tilde{q}$、两个$\tilde{a}$进行对齐即可训练。                                                                                                                                                                                                                                                                                                                                                                                                                              | 弱监督<br>数据增强，添加对偶问题，设计$f(f(q))=q$的正向+逆向推理机制<br>设计大量Loss函数                                                | VLM   |
| [VideoLights](https://arxiv.org/abs/2412.01558)                                 | 视频通过SlowFast+CLIP+BLIP拼接提取片段特征，通过FFCNN而非线性层对齐到文本空间，得到$\mathbf{F}_v$。<br>视频切成10s片段，通过BLIP生成画面文本描述，作为查询制造合成查询文本，以扩充数据集做预训练。查询文本通过CLIP+BLIP，拼接后传入FFCNN而非线性层对齐到视觉空间，得到$\mathbf{F}_t$。<br>进行细化。$\mathbf{F}_t$与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}'_v$；对$\mathbf{F}_t$做平均池化，与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}''_v$，与$\mathbf{F}_v$做矩阵乘法得到相似度矩阵$\mathrm{V}_S$；计算$\mathbf{F}_v$与$\mathbf{F}_t$的相似度矩阵$\mathbf{V}_Q$。最后将$(\mathbf{F}'_v, \mathbf{V}_Q, \mathbf{V}_S, \mathbf{F}''_v)$拼接过MLP得到新的$\mathbf{F}_v$。新的$\mathbf{F}_t$同理。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过双向注意力+交叉注意力+残差，得到文本增强的视觉Token，输入到DETR。<br>显著性预测头通过MLP预测每个片段的高光得分，有交叉熵Loss与余弦相似度Loss尝试向Ground Truth靠近；分类头预测DETR Decoder输出的固定数量的Query是否为检索片段，本质是二分类问题，有交叉熵Loss；回归头预测时间区间，有L1 Loss和IoU Loss。VTG任务的分类头输出，理应与显著性预测头一致，将两者相似度作为Loss。显著性预测头面对正样本时使用L2 Loss，面对负样本时使用L1 Loss，随Epoch增长而加倍，迫使模型学习难样本<br> | 弱监督+强监督二阶段训练<br>数据增强，仅靠视频与VLM生成的文本描述来造Ground  Truth<br>特征融合机制<br>新Loss，跨任务对齐+动态权重<br>叠Backbone接Concat刷分 | DETR  |
| [ReCorrect](https://arxiv.org/abs/2412.00811)                                   | 视频通过GPT-4o/Gemini-1.0生成大量粗粒度数据集，使用CLIP计算视频帧与文本的相似度，小于阈值则视为噪声并删除。再移动片段左右边界，使得片段内各帧分数总和减去片段外各帧分数总和取得最大值。<br>借用SimBase作为VTG的Backbone，维护一个存储统一样本在不同Epoch内的推理结果的Memory Bank，最终选择该样本内若干推理结果中，与其它推理结果IoU之和最高的推理结果，作为最终伪Ground Truth进行反向传播。<br>然后既可以直接拿SimBase推理，也可以继续后续的强监督训练。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 弱监督<br>预训练<br>数据增强，发布数据集Vid-Morp<br>Memory Bank                                                         | 提议    |
| [Moment-GPT](http://arxiv.org/abs/2501.07972)                                   | 视频通过MiniGPT-v2得到帧画面文本描述$\mathbf{F}_v$。<br>查询文本通过LLaMA-3进行纠错与重写，得到若干个等价的查询文本，通过LLaMA-3提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$做相似图，得到若干区间提议，使用Video-ChatGPT得到区间画面文本描述$\mathbf{F}'v$，计算与$\mathbf{F}_t$的相似度，对区间提议打分，过NMS。<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 相似度<br>按相似度阈值生成提议的策略与超参数                                                                                | 提议    |


|                                                           | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                 | 标签                                                                                                                                                       | 架构       |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| [SeViLA](http://arxiv.org/abs/2305.06988)                 | 视频通过ViT得到帧画面特征，过Q-Former和线性层，与文本查询一起注入到定位器FlanT5，将生成的第一个Token为"Yes"的概率作为帧得分，设置阈值超参数表示一段区间内能容许最大数量的低分帧，得到VTG任务的答案。<br>把帧得分作为权重注入到帧画面特征，过Q-Former和线性层，得到多帧特征，与文本查询一起注入到回答器FlanT5，得到QA任务的答案。然后输入单帧特征，看看QA任务答案是否正确，如果正确则标记为关键帧，生成伪标签，反向训练定位器FlanT5。                                                                                                                                                                                          | 弱监督，只需视频与查询文本，无需标注时间戳<br>QA+VTG双向辅佐训练<br>绘制"Yes"的Token生成概率曲线，按阈值生成区间提案                                                                                   | VLM      |
| [LLaViLo](https://ieeexplore.ieee.org/document/10350951/) | 视频通过SlowFast提取片段特征$\mathbf{F}_v$。<br>查询文本通过CLIP提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过交叉注意力+Encoder，对齐到LLaMA的文本空间，得到视觉Token。仅在LLaMA的Top-N层注入视觉Token，并使用从`0`到`1`的门控因子控制视觉Token的特征明显性，防止训练初期的杂乱视觉Token影响LLaMA能力。<br>LLaMA同时输入DETR中的可训练锚点Token，走DETR的回归输出，损失函数是IoU；同时走LLaMA的文本输出，损失函数是交叉熵。DETR擅长定位，VLM擅长语义理解。                                                                                                            | 缝合DETR+VLM的Loss<br>完全冻结LLM，无LoRA，只训练视觉到文本Token的Adapter，参数量增长极小                                                                                           | DETR+VLM |
| [VTimeLLM](https://arxiv.org/abs/2311.18445)              | 三阶段训练。<br>一阶段：使用图片-文本对数据集，图片经过ViT与线性适配器，把视觉特征投影到Vicuna的文本Token空间。仅训练线性适配器，用数据集文本和Vicuna输出文本进行训练。<br>二阶段：使用时序定位的QA数据集，视频拆成100张图片，经过ViT与线性适配器，与Q合并，作为Vicuna输入，以自然语言输出A。仅训练Vicuna附着的LoRA。<br>三阶段：使用高质量的时序定位数据集，重复二阶段，减少过拟合，恢复通用能力。仅训练Vicuna附着的另一个新加的LoRA。                                                                                                                                                                                   | 三阶段预训练<br>数据增强<br>纯LLM微调为VLM                                                                                                                             | VLM      |
| [TimeChat](http://arxiv.org/abs/2312.02051)               | 视频的每一帧都通过ViT + InstructBLIP Image Q-Former，通过滑动窗口截取 + Video-LLaMA Video Q-Former获得片段Token，过线性层对齐到LLaMA-2嵌入空间，与STT字幕文本、用户查询文本一起合并，作为LLaMA-2输入，输出答案文本。仅微调两个Q-Former和LLM的LoRA。                                                                                                                                                                                                                                                              | 用GPT做数据增强，发布TimeIT数据集<br>用预训练的Q-Former提取视频特征。<br>未使用时间位置编码注入视觉特征，而是将时间信息变为文本Token，过InstructBLIP Q-Former。<br>利用了原视频的音频→字幕文本信息<br>视觉Token融入时间戳未知编码信息0<br> | VLM      |
| [GroundingGPT](https://arxiv.org/abs/2401.06071)          | 图像通过ViT，视频等距采样若干帧后通过BLIP-2 Q-Former，音频截取2秒通过FFT + 时间位置编码 + ImageBind，过线性层对齐到Vicuna-1.5的文本嵌入空间，直接输出文本。<br>一阶段：仅训练线性层，训练多模态<br>二阶段：仅训练线性层 + LLM，训练输出时间+空间坐标<br>三阶段：仅训练线性层 + LLM，优化模型回复                                                                                                                                                                                                                                                     | 多阶段<br>与[VTimeLLM](https://arxiv.org/abs/2311.18445)完全一致，多加了一个音频模态                                                                                       | LLM      |
| [HawkEye](https://arxiv.org/abs/2403.10228)               | 完全基于VideoChat2架构，在原先第三步的微调阶段使用了自己发布的InternVid-G数据集。<br>LLM不再直接输出文本时间戳，而是输出前面/中间/后面/全程等闭集类别，通过二分递归定位锁定片段                                                                                                                                                                                                                                                                                                                                  | 数据增强，构建正负样本对，发布InternVid-G数据集<br>LLM不输出文本时间戳，而是返回二分搜索的`check()`值。                                                                                        | LLM      |
| [LITA](https://arxiv.org/abs/2403.19046)                  | 视频均匀采样100帧，通过ViT + SlowFast + 线性层得到视觉Token，与文本Token合并注入到Vicuna。<br>**引入新的相对时间Token`<1>~<100>`**，与帧率解耦。<br>多任务联合微调：密集视频描述、事件定位、视频问答、自然语言视觉问答、推理时序定位                                                                                                                                                                                                                                                                                       | 数据增强，考验推理能力，发布数据集ActivityNet-RTL<br>多任务微调                                                                                                                | LLM      |
| [VTG-LLM]()                                               | 视频均匀采样96帧，通过ViT + QFormer，使用门控从`0`到`1`叠加相对时间+绝对时间嵌入，输出后使用线性差值计算时间戳。<br>**引入视觉Token压缩，借鉴SoftMoE架构，筛出Top-K(k=256)个视觉Token，并对压缩方式做了消融实验**。<br>使用Gemini 1.5重新标注原有数据集中质量较差的条目。                                                                                                                                                                                                                                                                | 时间编码<br>**视觉Token压缩**<br>数据增强，发布VTG-IT-120K数据集                                                                                                           | LLM      |
| [Mr.BLIP](http://arxiv.org/abs/2406.18113)                | 视频帧特征与时间戳文本Token交替插入，输出时间戳纯文本，对VLM进行微调。<br>架构与模型无关，在BLIP-2、Qwen2.5-VL、GPT-4o均有效。                                                                                                                                                                                                                                                                                                                                                         | 暴力做法                                                                                                                                                     | VLM      |
| [Momentor](https://arxiv.org/abs/2402.11435)              | 构建$N$个可学习的时间锚点，**线性插值得到连续的时间编码空间**，且有锚点滑动窗口反向机制，让时间编码空间更连续。<br>视频通过ViT + 线性层 + 时间位置编码，与文本Token合并，输入到LLaMA-2。<br>一阶段：做描述任务，仅训练线性层，把视觉Token对齐到文本嵌入空间<br>二阶段：训练VTG任务<br>三阶段：在自己构建的Moment-10M数据集上训练，使用Grounding DINO + PyScene Detect + Vicuna建语义连贯的时间片段多任务数据集，保留通用能力                                                                                                                                                                        | 多阶段<br>时间编码<br>数据增强，发布Moment-10M数据集                                                                                                                      | VLM      |
| [GeLM](http://arxiv.org/abs/2408.14469)                   | 视频通过InternVideo-MM-L-14提取视觉特征，使用Tokenzier得到查询文本Token，作为Vicuna-1.3的输入，输出包含定位Token对，得到查询向量。<br>视觉特征和查询向量进行融合：（1）将视觉特征过一个显著性头，预测每一帧的显著性得分，做交叉熵Loss；（2）做预先相似度矩阵，得到相似度得分，做NCE Loss。利用显著性得分，大于`0.7`的取并集得到粗粒度答案；利用相似度矩阵，过平滑卷积+Softmax，同样让大于`0.7`的取并集得到细粒度答案。二者均能作为答案，且能互补。<br>创建数据标注Pipeline，基于Ego4D数据集i，用Spacy解析旁白文本，构建主谓宾语法树，筛选多次出现的概念，交给GPT-4o生成问答文本对。                                                                                       | 数据增强，发布MULTIHOP-EGOQA数据集，专注多跳视频推理和定位<br>引入定位Token，解剖LLM，提取其隐藏状态做定位任务                                                                                     | LLM      |
| [SlowFocus](https://openreview.net/forum?id=FOkKndty5B)   | 对整个视频使用ViT-L进行稀疏采样，获取稀疏帧，检索相关片段。对片段进行密集采样，将稀疏采样与密集采样加上时序编码，过自设计的注意力，合并文本查询，输入到LLM。<br>时序编码由一个可学习矩阵构成，将时间分成1000个步数，作为时间Token，直接加到视觉Token中。<br>自设计注意力机制，用稀疏采样通过交叉注意力增强密集采样，随后稀疏采样、密集采样、文本查询合并，作为Vicuna-1.5 7B的输入。<br>一阶段：用图像-文本对训练线性层对齐+时间编码器，实现视觉对齐<br>二阶段：训练密集视频描述+时序定位任务<br>三阶段：适应自设计注意力机制，输入稀疏采样和密集采样。                                                                                                                                 | 三阶段训练<br>新注意力机制<br>分层采样，稀疏+密集<br>时间编码<br>数据增强，发布FineAction-CGR数据集                                                                                        | LLM      |
| [E.T.Chat](http://arxiv.org/abs/2409.18111)               | 视频按1FPS采样，通过ViT-G，**借鉴了LLaMA-VID的压缩架构**，**过Q-Former做压缩**，使用交叉注意力用文本特征来增强压缩前的特征，直接与文本相加，过平均池化得到视觉特征，过线性层与文本对齐，输入到Phi-3-Mini-3.8B，用LoRA微调。<br>输出两个个特殊的时间Token`<vid>`，提取时间Token与视觉Token的隐藏状态，过MLP后做相似度检测，取最匹配的作为时间戳，得到起始+终止的时间戳。Loss函数不使用0/1匹配，而是使用指数衰减，允许临近帧为`0.5`/`0.25`/......的值。<br>修改LLM的Mask，让视频Token之间互相可见，等价于做自注意力                                                                                                                  | 输出时间Token，提取隐藏状态做片段时间戳的回归<br>数据增强，发布E.T.Bench数据集<br>新分类Loss，平滑匹配                                                                                         | LLM      |
| [Grounded-VideoLLM](https://arxiv.org/abs/2410.03290)     | 视频分成$K=300$个片段，每个片段取中间帧过ViT+平均池化+MLP得到空间流，取整体过InternVideo2-1B+平均池化+MLP作为时间流，合并成双流。$K$个双流与查询文本拼接，作为Phi3.5-Vision或LLaVA-1.5-7B的输入。<br>构建$K=300$个离散的时间Token，数据集中的Q与A均用时间Token表示。<br>一阶段：用图像-文本对训练MLP，实现视觉对齐<br>二阶段：训练密集视频描述+时序定位任务，实现学习时间Token<br>三阶段：多任务指令微调                                                                                                                                                                               | 三阶段训练<br>离散时间Token<br>输入双流，即高分辨率单帧+低分辨率视频<br>数据增强，发布数据集Grounded VideoQA                                                                                  | VLM      |
| [TGB](https://arxiv.org/abs/2402.16050)                   | 视频通过RAFT提取光流特征 + CNN + MLP + 位置编码，文本通过Tokenizer + RoPE位置编码，两者过交叉注意力得到增强的视觉特征，每个视觉特征对应的时间戳都会经过分类头，分类为`<BEGIN>`、`<NONE>`、`<END>`三种，得到预选时间片段，极大地减少了VLM的输入Token数量。<br>这些片段作为原先工作中“未经剪辑的视频”，过VLM推理。                                                                                                                                                                                                                                           | 视频片段预筛选，降低VLM的Token数量<br>视频位置编码                                                                                                                          | VLM      |
| [TimeSuite](https://arxiv.org/abs/2410.19702)             | 视频分成$K$个片段，每个片段采样$T$帧，过UML-L + **Q-Former** + **Token Shuffle，即把相邻的帧特征按嵌入维度拼接，过MLP变回原先的嵌入维度**，初始时等价于平均池化。<br>受CPVT的启发，作者提出TAPE时序位置编码，作为残差直接相加。<br>视觉特征与文本Token合并后，输入到Mistral-7B中。                                                                                                                                                                                                                                                        | **视觉Token压缩**<br>时间位置编码<br>数据增强，发布TimePro数据集<br>                                                                                                         | VLM      |
| [TRACE](https://arxiv.org/abs/2410.05643)                 | 视频过ViT-L + **QFormer压缩**。对于片段描述任务，输入自带时间戳与显著性分数，对每个数字字符分配新Token，例如`<0>~<9>`、`<小数点>`。向Mistral-v0.2-7B输入视觉Token、查询文本Token、片段描述请求文本Token，对其中时间戳/显著性分数的Token，提取隐藏状态过对应的分类/回归头。                                                                                                                                                                                                                                                               | 时间Token<br>                                                                                                                                              | LLM      |
| [ReVisionLLM](https://arxiv.org/abs/2411.14901)           | 视频切分成若干片段，通过ViT-L + 交叉注意力 + 自注意力生成稀疏特征，与文本查询进入Vicuna-v1.5-7B，得到片段内的候选时间戳区间。<br>根据候选时间戳区间，通过ViT-L + 全局CLS Token + 线性层生成密集特征，与文本查询进入Vicuna-v1.5-7B，对每个候选结果按照LLM输出的平均熵倒数排序确定置信度。<br>一阶段：训练短视频密集特征定位能力，引入对比负样本，迫使模型回答“指定时间在给定片段中不存在”<br>二阶段：训练长视频稀疏特征定位能力。<br>                                                                                                                                                                               | 二阶段训练<br>二分递归查找定位<br>分层采样，稀疏+密集                                                                                                                          | 候选+LLM   |
| [NumPro](https://arxiv.org/abs/2411.10332)                | 直接在视频的每一帧右下角覆盖红色40号字体的数字，交给VLM直接进行下游任务。既可以免训练，也可以只训练视觉Token线性层与LoRA微调大模型。实验证明Qwen2-VL-7B、Qwen2-VL-72B、LLaVA-Video-7B、LLaVA-OneVision-7B、LongVA-7B-DPO均有效。                                                                                                                                                                                                                                                                                | 一种暴力的时间位置编码<br>数据增强，发布NumPro-enhanced数据集                                                                                                                 | VLM      |
| [LLaVA-MR](http://arxiv.org/abs/2411.14505)               | 视频经过BLIP-2视觉编码器得到帧特征，在时序上做差分 + 高斯平滑，筛选出Top-K个变化最大的关键帧，则非关键帧可降低Token数量，过Q-Former压缩，但是只保留沿时序方差最大的Top-K个Query作为视觉Token，或者直接选取相邻的K个Token做平均池化，与查询文本Token一起输入到BLIP-2中。一切数字作为文本Token，但是时间与视觉Token有自己的特殊分隔Token。BLIP-2的输入形如：`<time_begin>2<time_end>(<frame_begin>F_i<frome_end>)+文本查询`。<br>针对不同帧率和时长的数据集，若采样帧率`<1`，则对时间戳取整；若采样帧率`>1`，则使用基于帧总数的相对位置编码——因为LLM不擅长输出文本小数。<br>                                                                      | **视觉Token压缩**<br>帧率自适应时间位置编码                                                                                                                             | LLM      |
| [TimeMarker](https://arxiv.org/abs/2411.18211)            | 视频动态采样$N=\min(N_{\mathrm{max}}, 2\cdot T_v)$帧，过ViT-L + 线性层得到每帧的Patch图，按照$\begin{cases}4\times 4 &,N>\frac{N_{\mathrm{max}}}{2}\\4\times 2&,\frac{N_\mathrm{max}}{4}<N<\frac{N_\mathrm{max}}{2}\\2\times 2&,N<\frac{N_\mathrm{max}}{4}\end{cases}$的二维卷积核做平均池化，以压缩视觉Token数量，与查询文本Token注入到LLaMA-8B中。<br>不使用特殊Token分隔帧与帧之间视觉Token，而是直接用文本时间标记`Second{i}`。<br>一阶段：训练视觉编码器与线性层，使用图像-文本对做对齐模态<br>二阶段：LoRA微调LLM，做VTG任务与QA问答<br>三阶段：指令微调，提高通用推理能力 | 三阶段训练<br>让时间戳文本Token作为时间位置编码<br>**动态帧采样**<br>**视觉Token压缩**<br>**线性投影层换激活函数，叠两层MLP**<br>数据增强，但未发布数据集                                                      | LLM      |
| [Seq2Time](http://arxiv.org/abs/2411.16932)               | 架构完全源于TimeChat。<br>将时间戳映射到`[0, 1]`之间的相对坐标，取高四位小数，格式化成四个特殊的数码Token。<br>数据增强，用若干图片-文本对来模拟一个视频，用LongVA做图像描述。<br>多下游任务联合训练，包括时间定位、事件描述、时序推理                                                                                                                                                                                                                                                                                                  | 使用特殊时间戳Token<br>数据增强，但未发布数据集                                                                                                                             | VLM      |
| [VideoChat-TPO](http://arxiv.org/abs/2412.19326)          | 架构完全源于VideoChat2。<br>视频帧通过UMT-L提取帧特征，视频本题通过InternVideo2提取视频特征，过**Q-Former**对齐；查询文本通过Chinese-LLaMA-Alpaca提取文本Token，两者合并输入到Mistral-7B。<br>根据文本查询生成特殊Token，按照所执行的下游任务（跟踪、分割、时间定位）进行分类，向LLM注入对应的可学习Query作为Token，将其隐藏状态输入到下游任务头中。<br>一阶段：训练特殊Token的下游任务分类准确率<br>二阶段：训练下游任务<br>三阶段：训练混合下游任务<br>                                                                                                                                                | 三阶段训练<br>多任务推理<br>特殊任务Token提取隐藏状态，接入专用模型做下游任务                                                                                                            | LLM      |
| [MLLM-TA](https://ieeexplore.ieee.org/document/10777595/) | 视频均匀采样，过ViT-L，得到视觉特征$\mathrm{F}_v$。查询文本过CLIP文本编码器，得到文本特征$\mathrm{F}_v$。两者过交叉注意力，得到文本增强的视觉特征，过时序卷积层得到$w_{\mathrm{vis}}$，预测每帧是否在所求区间内，此处有交叉熵损失。将增强的视觉Token与文本Token输入到Vicuna-7B/Vicuna-13B，输出的文本包含很多事件的片段预测值，Split后经过CLIP文本编码器得到单个向量，于是每个帧都能对应到某个事件，某个事件都能对应到单个向量，得到$w_{\mathrm{text}}$，它应该与$w_{\mathrm{vis}}$贴近，此处有L1 Loss。                                                                                                                | **增强视觉Token**<br>**LLM输出末端有下游相似度对齐**<br>                                                                                                                 | LLM      |
| [TemporalVLM](http://arxiv.org/abs/2412.02930)            | 视频划分为6个片段，每个片段均匀采样96帧，过ViT-G得到视觉特征，与查询文本+时间戳共同进入InstructBLIP **Q-Former**做压缩，得到帧Token，过长度为32、步长为16的Video-LLaMA **Q-Former** + 自注意力提取片段特征，过BiLSTM得到视频视觉特征，与查询文本Token送入LLaMA-2 7B/LLaMA-3 8B，直接输出纯文本时间戳。                                                                                                                                                                                                                                   | 处理长视频<br>**增强视觉Token**<br>数据增强，发布IndustryASM数据集<br>                                                                                                      | LLM      |
| [TimeRefine](http://arxiv.org/abs/2412.09601)             | 通用架构。<br>期望VLM输出进行$K$次细化，形如`<seg_start>(s0 to e0<offset>ds0 es0<refine>)+<seg_end>`。训练集的期望输出也通过高斯分布制造细化过程。其中时间戳为纯文本，但是也同时会给`<refine>`Token的隐藏状态接一个回归头做L1 Loss。                                                                                                                                                                                                                                                                           | 即插即用<br>多次迭代细化输出                                                                                                                                         | VLM      |
| [LLaVA-ST](https://arxiv.org/abs/2501.08282)              | 视频均匀采样$N$帧，过SigLIP提取得到$H\times W$个Patch。为LLM扩充$N\times H \times W$个可学习的特殊Token。                                                                                                                                                                                                                                                                                                                                                          | 三阶段<br>时间位置编码                                                                                                                                            | LLM      |
|                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                          |          |



Ranking Loss：要求正样本得分与负样本得分至少相差$\Delta$。$\mathcal{L} = \max(0, \Delta - (S_{\mathrm{pos}} - S_{\mathrm{neg}}))$。

给**增强视觉Token**的工作加上**视觉Token压缩**？

| [VERIFIED](https://arxiv.org/abs/2410.08593) |     |     |     |
| -------------------------------------------- | --- | --- | --- |
