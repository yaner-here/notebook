
|                                                                                 | Pipeline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 标签                                                                                                      | 架构    |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | ----- |
| [Grounding-Prompter](https://arxiv.org/abs/2312.17117)                          | 视频通过ASR与BLIP，生成字幕与画面文本描述。<br>文本不变。<br>与文本一起，直接送入LLM，以自然语言输出时间戳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 输入仅文本<br>由自然语言输出                                                                                        | VLM   |
| [VTG-GPT](https://arxiv.org/abs/2403.02076)                                     | 视频通过VLLM，生成画面文本描述。<br>文本经过LLM，进行纠错与细化。<br>视频与文本经过相似度筛选，按阈值直接得到时间戳，后附NMS。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 输入仅文本<br>由文本-文本相似度矩阵输出                                                                                  | 提议    |
| [GPTSee](http://arxiv.org/abs/2403.01437)                                       | 视频通过VLLM，生成画面文本描述，经CLIP得到视频特征。<br>文本经过LLM，生成若干语义等价的文本，以此扩充数据集。<br>画面文本与任一等价文本做相似度，得到$L_v \times 1$序列，与视频特征拼接，送入视觉编码器，与原始文本一同送入编码器，生成跨模态特征，注入可学习查询与基于相似度的位置嵌入，送入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 输入仅文本<br>将**等价查询文本信息**注入解码器<br>输出连续值                                                                    | DETR  |
| [GroundVQA](https://arxiv.org/abs/2312.06505)                                   | 视频通过InternVideo，提取视频特征，经线性层对齐到文本特征。<br>文本经Tokenizer，提取文本特征。<br>视频特征与文本特征送入Flan-T5编码器，然后执行下游任务。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 输入视频与文本<br>用Decoder输出做下游任务                                                                              | VLM   |
| [LMR](https://arxiv.org/abs/2405.12540)                                         | 视频通过MiniGPT-v2生成画面文本描述；通过CLIP/InternVideo提取视频特征$\mathbf{F}_v$。<br>画面文本与查询文本经过CLIP/LLaMA生成文本特征$\mathbf{F}'_t$/$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F_t'}$分别与$\mathbf{F}_t$做共享参数的交叉注意力融合，拼在一起进入编码器做自注意力，以$F_t$作为查询进入解码器，过FFN。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 输入视频与文本<br>将**等价视觉描述信息**注入编码器                                                                           | VLM   |
| [TEA](https://arxiv.org/abs/2406.17880)                                         | 视频通过LLaVA-v1.5/BLIP-2生成画面文本描述，通过3D-CNN提取视频特征。<br>画面文本与查询文本通过GloVe获取文本特征。<br>视频特征与画面文本拼接后过MLP，与查询文本做交叉注意力；画面文本单独与查询文本做交叉注意力；分别送入预测器。预测器接受两个输入，按照[Span Predictor](https://arxiv.org/pdf/2004.13931)的流程输出边界。这两个边界加权平均即为输出。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 输入视频与**细化**文本<br>将**等价视觉描述信息**注入编码器                                                                     | DETR  |
| [TFVTG](https://arxiv.org/pdf/2408.16219)                                       | 视频通过CLIP提取视频特征。<br>查询文本由LLM拆成若干有时间约束关系的子事件，通过CLIP提取文本特征。<br>分别对子事件进行DETR推理，得到帧-置信度曲线，设计动态&静态得分函数选出TopK个提案，得到若干子事件区间，排列组合得到若干总事件区间，按时间约束关系进行过滤，根据动态&静态得分排名出最佳答案。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 无训练<br>新Loss函数<br>手工设计子查询文本与合并机制                                                                        | DETR  |
| [EI-VLG](http://arxiv.org/abs/2408.02336)                                       | 视频通过LLaVA生成画面文本描述，过一层可训练的BERT得到画面文本特征，与文本查询进行对比学习。<br>画面文本特征与查询文本特征经过带残差的交叉注意力，与视频特征进行带残差的拼接，送到预测头。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 使用对比学习，对查询文本与画面文本进行对齐，然后再融合                                                                             | DETR  |
| [DeVi](https://arxiv.org/abs/2409.04388)                                        | 视频通过GPT-4生成若干时间窗口尺度的片段描述，一起输入到GPT-4o，对每个描述进行改写，生成事件集合与全局摘要；通过CLIP得到视觉特征。<br>把事件集合、全局摘要、查询文本、选项文本送入GPT-4o/Gemini-2.0，得到答案。将答案对应区间内的视频特征与查询文本做相似度匹配，若相似度过低则继续多轮对话，制度相似度达标或触发对话轮次上限为止。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 无训练<br>提出密集VQA+VTG任务<br>引入查询文本-片段特征相似度作为重复推理的依据，会导致推理时间变长。                                              | LLM   |
| [ChatVTG](https://arxiv.org/abs/2410.12813)                                     | 视频通过VideoChatGPT/VideoChat2生成Action、Place、Dressing、Emotion、Interaction五个维度的片段描述文本。<br>查询文本通过BERT得到查询文本特征。<br>两者做相似度匹配，选出最相似的片段作为粗粒度。以该片段为中心生成若干滑动窗口区间，重复相似度操作，选出最相似的片段作为细粒度最终答案。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 无训练，因为纯复用现有VLM-VTG的文本编码器，得到的查询文本特征与片段描述特征已经对齐。<br>纯对比学习                                                 | 纯对比学习 |
| [TimeCraft](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00720.pdf) | 只给定原始视频与文本查询集合，不给定时刻的Ground Truth。<br>每个GVQA模型共享一套参数，包含ViT/BERT的编码器、融合文本与视觉特征的Transformer、定位头、QA文本分类头，可以输出时间区间与QA文本。<br>将原始文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-1，输出时间区间$t$、答案文本$a$、对偶文本查询$\tilde{q}$；将答案文本$a$与视频特征$\mathrm{F}_v$输入到GQVA-2，得到对偶答案文本$\tilde{a}$；将对偶文本查询$\tilde{q}$与视频特征$\mathrm{F}_v$输入到GQVA-3，得到对偶答案文本$\tilde{a}$；将文本查询$q$与视频特征$\mathrm{F}_v$输入到GQVA-4，得到答案文本$a$。对两个$a$、两个$\tilde{q}$、两个$\tilde{a}$进行对齐即可训练。                                                                                                                                                                                                                                                                                                                                                                                                                              | 弱监督<br>数据增强，添加对偶问题，设计$f(f(q))=q$的正向+逆向推理机制<br>设计大量Loss函数                                                | VLM   |
| [VideoLights](https://arxiv.org/abs/2412.01558)                                 | 视频通过SlowFast+CLIP+BLIP拼接提取片段特征，通过FFCNN而非线性层对齐到文本空间，得到$\mathbf{F}_v$。<br>视频切成10s片段，通过BLIP生成画面文本描述，作为查询制造合成查询文本，以扩充数据集做预训练。查询文本通过CLIP+BLIP，拼接后传入FFCNN而非线性层对齐到视觉空间，得到$\mathbf{F}_t$。<br>进行细化。$\mathbf{F}_t$与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}'_v$；对$\mathbf{F}_t$做平均池化，与$\mathbf{F}_v$做逐元素乘法得到$\mathbf{F}''_v$，与$\mathbf{F}_v$做矩阵乘法得到相似度矩阵$\mathrm{V}_S$；计算$\mathbf{F}_v$与$\mathbf{F}_t$的相似度矩阵$\mathbf{V}_Q$。最后将$(\mathbf{F}'_v, \mathbf{V}_Q, \mathbf{V}_S, \mathbf{F}''_v)$拼接过MLP得到新的$\mathbf{F}_v$。新的$\mathbf{F}_t$同理。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过双向注意力+交叉注意力+残差，得到文本增强的视觉Token，输入到DETR。<br>显著性预测头通过MLP预测每个片段的高光得分，有交叉熵Loss与余弦相似度Loss尝试向Ground Truth靠近；分类头预测DETR Decoder输出的固定数量的Query是否为检索片段，本质是二分类问题，有交叉熵Loss；回归头预测时间区间，有L1 Loss和IoU Loss。VTG任务的分类头输出，理应与显著性预测头一致，将两者相似度作为Loss。显著性预测头面对正样本时使用L2 Loss，面对负样本时使用L1 Loss，随Epoch增长而加倍，迫使模型学习难样本<br> | 弱监督+强监督二阶段训练<br>数据增强，仅靠视频与VLM生成的文本描述来造Ground  Truth<br>特征融合机制<br>新Loss，跨任务对齐+动态权重<br>叠Backbone接Concat刷分 | DETR  |
| [ReCorrect](https://arxiv.org/abs/2412.00811)                                   | 视频通过GPT-4o/Gemini-1.0生成大量粗粒度数据集，使用CLIP计算视频帧与文本的相似度，小于阈值则视为噪声并删除。再移动片段左右边界，使得片段内各帧分数总和减去片段外各帧分数总和取得最大值。<br>借用SimBase作为VTG的Backbone，维护一个存储统一样本在不同Epoch内的推理结果的Memory Bank，最终选择该样本内若干推理结果中，与其它推理结果IoU之和最高的推理结果，作为最终伪Ground Truth进行反向传播。<br>然后既可以直接拿SimBase推理，也可以继续后续的强监督训练。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 弱监督<br>预训练<br>数据增强，发布数据集Vid-Morp<br>Memory Bank                                                         | 提议    |
| [Moment-GPT](http://arxiv.org/abs/2501.07972)                                   | 视频通过MiniGPT-v2得到帧画面文本描述$\mathbf{F}_v$。<br>查询文本通过LLaMA-3进行纠错与重写，得到若干个等价的查询文本，通过LLaMA-3提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$做相似图，得到若干区间提议，使用Video-ChatGPT得到区间画面文本描述$\mathbf{F}'v$，计算与$\mathbf{F}_t$的相似度，对区间提议打分，过NMS。<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 相似度<br>按相似度阈值生成提议的策略与超参数                                                                                | 提议    |


|                                                           |                                                                                                                                                                                                                                                 |                                                                        |          |
| --------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | -------- |
| [SeViLA](http://arxiv.org/abs/2305.06988)                 | 视频通过ViT得到帧画面特征，过Q-Former和线性层，与文本查询一起注入到定位器FlanT5，将生成的第一个Token为"Yes"的概率作为帧得分，设置阈值超参数表示一段区间内能容许最大数量的低分帧，得到VTG任务的答案。<br>把帧得分作为权重注入到帧画面特征，过Q-Former和线性层，得到多帧特征，与文本查询一起注入到回答器FlanT5，得到QA任务的答案。然后输入单帧特征，看看QA任务答案是否正确，如果正确则标记为关键帧，生成伪标签，反向训练定位器FlanT5。 | 弱监督，只需视频与查询文本，无需标注时间戳<br>QA+VTG双向辅佐训练<br>绘制"Yes"的Token生成概率曲线，按阈值生成区间提案 | VLM      |
| [LLaViLo](https://ieeexplore.ieee.org/document/10350951/) | 视频通过SlowFast提取片段特征$\mathbf{F}_v$。<br>查询文本通过CLIP提取查询文本特征$\mathbf{F}_t$。<br>$\mathbf{F}_v$与$\mathbf{F}_t$经过交叉注意力+Encoder，得到文本指导的片段Token，                                                                                                          |                                                                        | DETR+VLM |
|                                                           |                                                                                                                                                                                                                                                 |                                                                        |          |


Ranking Loss：要求正样本得分与负样本得分至少相差$\Delta$。$\mathcal{L} = \max(0, \Delta - (S_{\mathrm{pos}} - S_{\mathrm{neg}}))$。


数据标注Pipeline：

| [VERIFIED](https://arxiv.org/abs/2410.08593) |     |     |     |
| -------------------------------------------- | --- | --- | --- |
