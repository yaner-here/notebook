{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §1 Transformers库使用演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析(sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'单句': [{'label': 'NEGATIVE', 'score': 0.9987472295761108}],\n",
      " '多句': [{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
      "        {'label': 'NEGATIVE', 'score': 0.9923542737960815}]}\n"
     ]
    }
   ],
   "source": [
    "classfier = transformers.pipeline(\n",
    "    task=\"sentiment-analysis\", \n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "pprint.pprint({\n",
    "    \"单句\": classfier(\"I hate you!\"),\n",
    "    \"多句\": classfier([\"I love you\", \"Fuck you\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "零样本分类(zero-shot-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ['news', 'diary', 'finance'],\n",
      " 'scores': [0.7021396160125732, 0.20275545120239258, 0.09510485827922821],\n",
      " 'sequence': 'I attended a meeting today.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.conda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:496: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "classfier = transformers.pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "pprint.pprint(classfier(\n",
    "    \"I attended a meeting today.\",\n",
    "    candidate_labels=[\"diary\", \"news\", \"finance\"],\n",
    "    clean_up_tokenization_space=False # 保证tokenization的正向/逆向运算一致性\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "d",
     "code"
    ]
   },
   "source": [
    "文本生成(text-generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I love you, such as I can!\"\\n'\n",
      "                    '\\n'\n",
      "                    'Maggie and her friend were out in'},\n",
      " {'generated_text': 'I love you, such as I am.\\n'\n",
      "                    '\\n'\n",
      "                    'I am so very sorry. I must make'},\n",
      " {'generated_text': \"I love you, such as I did, but I don't get it. It's my \"\n",
      "                    'problem'},\n",
      " {'generated_text': 'I love you, such as I did.\\n'\n",
      "                    '\\n'\n",
      "                    'You\\'ll feel great in there,\" the queen'},\n",
      " {'generated_text': 'I love you, such as it is.\" I\\'m so grateful to our '\n",
      "                    'children for helping them grow'}]\n"
     ]
    }
   ],
   "source": [
    "generator = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"openai-community/gpt2\",\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "pprint.pprint(generator(\n",
    "    \"I love you, such as\",\n",
    "    truncation=True, max_length=20,\n",
    "    num_return_sequences=5,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩码填充(fill-mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.08490116894245148,\n",
      "  'sequence': 'This is a ripe fruit, which tastes good.',\n",
      "  'token': 23318,\n",
      "  'token_str': ' ripe'},\n",
      " {'score': 0.06573793292045593,\n",
      "  'sequence': 'This is a tropical fruit, which tastes good.',\n",
      "  'token': 10602,\n",
      "  'token_str': ' tropical'},\n",
      " {'score': 0.06305697560310364,\n",
      "  'sequence': 'This is a dried fruit, which tastes good.',\n",
      "  'token': 16380,\n",
      "  'token_str': ' dried'},\n",
      " {'score': 0.04835207387804985,\n",
      "  'sequence': 'This is a fresh fruit, which tastes good.',\n",
      "  'token': 2310,\n",
      "  'token_str': ' fresh'},\n",
      " {'score': 0.047243986278772354,\n",
      "  'sequence': 'This is a juicy fruit, which tastes good.',\n",
      "  'token': 28068,\n",
      "  'token_str': ' juicy'}]\n"
     ]
    }
   ],
   "source": [
    "unmasker = transformers.pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"distilbert/distilroberta-base\",\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "pprint.pprint(unmasker(\n",
    "    \"This is a <mask> fruit, which tastes good.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名实体识别(NER, named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\.conda\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 18,\n",
      "  'entity_group': 'PER',\n",
      "  'score': 0.9981694,\n",
      "  'start': 11,\n",
      "  'word': 'Sylvain'},\n",
      " {'end': 45,\n",
      "  'entity_group': 'ORG',\n",
      "  'score': 0.9796019,\n",
      "  'start': 33,\n",
      "  'word': 'Hugging Face'},\n",
      " {'end': 57,\n",
      "  'entity_group': 'LOC',\n",
      "  'score': 0.9932106,\n",
      "  'start': 49,\n",
      "  'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "ner = transformers.pipeline(\n",
    "    task=\"ner\",\n",
    "    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    grouped_entities=True, # 多个Token可以共同构成一个Entity\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "pprint.pprint(ner(\n",
    "    \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "))\n",
    "# PER: Person\n",
    "# ORG: Organization\n",
    "# LOC: Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问答系统(question-answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'a lovely little boy',\n",
      " 'end': 27,\n",
      " 'score': 0.6046934723854065,\n",
      " 'start': 8}\n"
     ]
    }
   ],
   "source": [
    "question_answerer = transformers.pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"distilbert/distilbert-base-cased-distilled-squad\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "pprint.pprint(question_answerer(\n",
    "    {\n",
    "        \"question\": \"Who are you?\",\n",
    "        \"context\": \"You are a lovely little boy.\"\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本摘要(summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Rapidly developing economies such as China and India, as '\n",
      "                  'well as other industrial countries in Europe and Asia, '\n",
      "                  'continue to encourage and advance the'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = transformers.pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "pprint.pprint(summarizer(\n",
    "    \"Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering.\",\n",
    "    max_length=30, min_length = 20\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翻译(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.conda\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hello.'}]\n"
     ]
    }
   ],
   "source": [
    "translator = transformers.pipeline(\n",
    "    task=\"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-zh-en\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "pprint.pprint(translator(\n",
    "    \"你好\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
