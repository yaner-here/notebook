{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, datasets\n",
    "import pprint\n",
    "import typing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §6.1 微调Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07437a99273482f92af7a752eada134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30905107bab4d6292d88cb8dc0fea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6137dccf704e82bf35ab7ed2837a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25915215c30f4a979498e6c7b61290d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 准备文本数据集\n",
    "\n",
    "raw_datasets: datasets.DatasetDict = datasets.load_dataset(\n",
    "    \"code_search_net\", \"python\",\n",
    "    trust_remote_code=True\n",
    ") # type: ignore\n",
    "pprint.pprint(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成Dataloader迭代器\n",
    "\n",
    "def get_training_corpus():\n",
    "    training_corpus: typing.Generator[str, None, None] = (\n",
    "        raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "    return training_corpus\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从GPT2的Tokenizer开始训练\n",
    "\n",
    "old_tokenizer: transformers.GPT2TokenizerFast = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\"\n",
    ") # type: ignore\n",
    "tokenizer: GPT2TokenizerFast = old_tokenizer.train_new_from_iterator( # type: ignore\n",
    "    training_corpus,\n",
    "    52000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'old_tokenizer': ['def', 'Ġtrain', '(', 'train', '_', 'dir', ',', 'Ġmodel', '_', 'save', '_', 'path', '=', 'None', ',', 'Ġn', '_', 'ne', 'igh', 'bors', '=', 'None', ',', 'Ġkn', 'n', '_', 'al', 'go', \"='\", 'ball', '_', 'tree', \"',\", 'Ġverb', 'ose', '=', 'False', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠTr', 'ains', 'Ġa', 'Ġk', '-', 'ne', 'arest', 'Ġneighbors', 'Ġclass', 'ifier', 'Ġfor', 'Ġface', 'Ġrecognition', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġtrain', '_', 'dir', ':', 'Ġdirectory', 'Ġthat', 'Ġcontains', 'Ġa', 'Ġsub', '-', 'directory', 'Ġfor', 'Ġeach', 'Ġknown', 'Ġperson', ',', 'Ġwith', 'Ġits', 'Ġname', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ(', 'View', 'Ġin', 'Ġsource', 'Ġcode', 'Ġto', 'Ġsee', 'Ġtrain', '_', 'dir', 'Ġexample', 'Ġtree', 'Ġstructure', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠStructure', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ<', 'train', '_', 'dir', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '1', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '1', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '2', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ...', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '2', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '1', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶ', 'Ķ', 'âĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '2', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶ', 'Ķ', 'âĶĢâĶĢ', 'Ġ...', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġmodel', '_', 'save', '_', 'path', ':', 'Ġ(', 'optional', ')', 'Ġpath', 'Ġto', 'Ġsave', 'Ġmodel', 'Ġon', 'Ġdisk', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġn', '_', 'ne', 'igh', 'bors', ':', 'Ġ(', 'optional', ')', 'Ġnumber', 'Ġof', 'Ġneighbors', 'Ġto', 'Ġweigh', 'Ġin', 'Ġclassification', '.', 'ĠChosen', 'Ġautomatically', 'Ġif', 'Ġnot', 'Ġspecified', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġkn', 'n', '_', 'al', 'go', ':', 'Ġ(', 'optional', ')', 'Ġunderlying', 'Ġdata', 'Ġstructure', 'Ġto', 'Ġsupport', 'Ġkn', 'n', '.', 'default', 'Ġis', 'Ġball', '_', 'tree', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġverb', 'ose', ':', 'Ġverb', 'osity', 'Ġof', 'Ġtraining', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'return', ':', 'Ġreturns', 'Ġkn', 'n', 'Ġclass', 'ifier', 'Ġthat', 'Ġwas', 'Ġtrained', 'Ġon', 'Ġthe', 'Ġgiven', 'Ġdata', '.', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠX', 'Ġ=', 'Ġ[]', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġy', 'Ġ=', 'Ġ[]', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġperson', 'Ġin', 'Ġthe', 'Ġtraining', 'Ġset', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġclass', '_', 'dir', 'Ġin', 'Ġos', '.', 'list', 'dir', '(', 'train', '_', 'dir', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġnot', 'Ġos', '.', 'path', '.', 'isd', 'ir', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġcontinue', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġtraining', 'Ġimage', 'Ġfor', 'Ġthe', 'Ġcurrent', 'Ġperson', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġimg', '_', 'path', 'Ġin', 'Ġimage', '_', 'files', '_', 'in', '_', 'folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġimage', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'load', '_', 'image', '_', 'file', '(', 'img', '_', 'path', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġface', '_', 'bound', 'ing', '_', 'boxes', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'face', '_', 'loc', 'ations', '(', 'image', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġlen', '(', 'face', '_', 'bound', 'ing', '_', 'boxes', ')', 'Ġ!=', 'Ġ1', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠIf', 'Ġthere', 'Ġare', 'Ġno', 'Ġpeople', 'Ġ(', 'or', 'Ġtoo', 'Ġmany', 'Ġpeople', ')', 'Ġin', 'Ġa', 'Ġtraining', 'Ġimage', ',', 'Ġskip', 'Ġthe', 'Ġimage', '.', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġverb', 'ose', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Image', 'Ġ{}', 'Ġnot', 'Ġsuitable', 'Ġfor', 'Ġtraining', ':', 'Ġ{}', '\".', 'format', '(', 'img', '_', 'path', ',', 'Ġ\"', 'Did', 'n', \"'t\", 'Ġfind', 'Ġa', 'Ġface', '\"', 'Ġif', 'Ġlen', '(', 'face', '_', 'bound', 'ing', '_', 'boxes', ')', 'Ġ<', 'Ġ1', 'Ġelse', 'Ġ\"', 'Found', 'Ġmore', 'Ġthan', 'Ġone', 'Ġface', '\"))', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġelse', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠAdd', 'Ġface', 'Ġencoding', 'Ġfor', 'Ġcurrent', 'Ġimage', 'Ġto', 'Ġthe', 'Ġtraining', 'Ġset', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠX', '.', 'append', '(', 'face', '_', 'recogn', 'ition', '.', 'face', '_', 'enc', 'od', 'ings', '(', 'image', ',', 'Ġknown', '_', 'face', '_', 'loc', 'ations', '=', 'face', '_', 'bound', 'ing', '_', 'boxes', ')[', '0', '])', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġy', '.', 'append', '(', 'class', '_', 'dir', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠDeter', 'mine', 'Ġhow', 'Ġmany', 'Ġneighbors', 'Ġto', 'Ġuse', 'Ġfor', 'Ġweight', 'ing', 'Ġin', 'Ġthe', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġis', 'ĠNone', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġ=', 'Ġint', '(', 'round', '(', 'math', '.', 'sq', 'rt', '(', 'len', '(', 'X', '))))', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġverb', 'ose', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Ch', 'ose', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġautomatically', ':', '\",', 'Ġn', '_', 'ne', 'igh', 'bors', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠCreate', 'Ġand', 'Ġtrain', 'Ġthe', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġkn', 'n', '_', 'cl', 'f', 'Ġ=', 'Ġneighbors', '.', 'K', 'Neigh', 'bors', 'Class', 'ifier', '(', 'n', '_', 'ne', 'igh', 'bors', '=', 'n', '_', 'ne', 'igh', 'bors', ',', 'Ġalgorithm', '=', 'kn', 'n', '_', 'al', 'go', ',', 'Ġweights', \"='\", 'distance', \"')\", 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġkn', 'n', '_', 'cl', 'f', '.', 'fit', '(', 'X', ',', 'Ġy', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠSave', 'Ġthe', 'Ġtrained', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġmodel', '_', 'save', '_', 'path', 'Ġis', 'Ġnot', 'ĠNone', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġwith', 'Ġopen', '(', 'model', '_', 'save', '_', 'path', ',', \"Ġ'\", 'wb', \"')\", 'Ġas', 'Ġf', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġpick', 'le', '.', 'dump', '(', 'kn', 'n', '_', 'cl', 'f', ',', 'Ġf', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġkn', 'n', '_', 'cl', 'f']}\n",
      "\n",
      "\n",
      "{'new_tokenizer': ['def', 'Ġtrain', '(', 'train', '_', 'dir', ',', 'Ġmodel', '_', 'save', '_', 'path', '=', 'None', ',', 'Ġn', '_', 'neighbors', '=', 'None', ',', 'Ġknn', '_', 'algo', \"='\", 'ball', '_', 'tree', \"',\", 'Ġverbose', '=', 'False', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠTra', 'ins', 'Ġa', 'Ġk', '-', 'nearest', 'Ġneighbors', 'Ġclassifier', 'Ġfor', 'Ġface', 'Ġrecognition', '.', 'ĊĊĠĠĠ', 'Ġ:', 'param', 'Ġtrain', '_', 'dir', ':', 'Ġdirectory', 'Ġthat', 'Ġcontains', 'Ġa', 'Ġsub', '-', 'directory', 'Ġfor', 'Ġeach', 'Ġknown', 'Ġperson', ',', 'Ġwith', 'Ġits', 'Ġname', '.', 'ĊĊĠĠĠĠ', 'Ġ(', 'View', 'Ġin', 'Ġsource', 'Ġcode', 'Ġto', 'Ġsee', 'Ġtrain', '_', 'dir', 'Ġexample', 'Ġtree', 'Ġstructure', ')', 'ĊĊĠĠĠĠ', 'ĠStructure', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġ<', 'train', '_', 'dir', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '1', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '1', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '2', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ...', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '2', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '1', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶĶâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '2', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĶâĶĢâĶĢ', 'Ġ...', 'ĊĊĠĠĠ', 'Ġ:', 'param', 'Ġmodel', '_', 'save', '_', 'path', ':', 'Ġ(', 'optional', ')', 'Ġpath', 'Ġto', 'Ġsave', 'Ġmodel', 'Ġon', 'Ġdisk', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġn', '_', 'neighbors', ':', 'Ġ(', 'optional', ')', 'Ġnumber', 'Ġof', 'Ġneighbors', 'Ġto', 'Ġwe', 'igh', 'Ġin', 'Ġclassification', '.', 'ĠCho', 'sen', 'Ġautomatically', 'Ġif', 'Ġnot', 'Ġspecified', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġknn', '_', 'algo', ':', 'Ġ(', 'optional', ')', 'Ġunderlying', 'Ġdata', 'Ġstructure', 'Ġto', 'Ġsupport', 'Ġknn', '.', 'default', 'Ġis', 'Ġball', '_', 'tree', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġverbose', ':', 'Ġverbosity', 'Ġof', 'Ġtraining', 'ĊĠĠĠ', 'Ġ:', 'return', ':', 'Ġreturns', 'Ġknn', 'Ġclassifier', 'Ġthat', 'Ġwas', 'Ġtrained', 'Ġon', 'Ġthe', 'Ġgiven', 'Ġdata', '.', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠX', 'Ġ=', 'Ġ[]', 'ĊĠĠĠ', 'Ġy', 'Ġ=', 'Ġ[]', 'ĊĊĠĠĠ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġperson', 'Ġin', 'Ġthe', 'Ġtraining', 'Ġset', 'ĊĠĠĠ', 'Ġfor', 'Ġclass', '_', 'dir', 'Ġin', 'Ġos', '.', 'listdir', '(', 'train', '_', 'dir', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġnot', 'Ġos', '.', 'path', '.', 'isdir', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')):', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcontinue', 'ĊĊĠĠĠĠĠĠĠ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġtraining', 'Ġimage', 'Ġfor', 'Ġthe', 'Ġcurrent', 'Ġperson', 'ĊĠĠĠĠĠĠĠ', 'Ġfor', 'Ġimg', '_', 'path', 'Ġin', 'Ġimage', '_', 'files', '_', 'in', '_', 'folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')):', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġimage', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'load', '_', 'image', '_', 'file', '(', 'img', '_', 'path', ')', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġface', '_', 'bounding', '_', 'boxes', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'face', '_', 'locations', '(', 'image', ')', 'ĊĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġif', 'Ġlen', '(', 'face', '_', 'bounding', '_', 'boxes', ')', 'Ġ!=', 'Ġ1', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġ#', 'ĠIf', 'Ġthere', 'Ġare', 'Ġno', 'Ġpeople', 'Ġ(', 'or', 'Ġtoo', 'Ġmany', 'Ġpeople', ')', 'Ġin', 'Ġa', 'Ġtraining', 'Ġimage', ',', 'Ġskip', 'Ġthe', 'Ġimage', '.', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġif', 'Ġverbose', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġprint', '(\"', 'Image', 'Ġ{}', 'Ġnot', 'Ġsuitable', 'Ġfor', 'Ġtraining', ':', 'Ġ{}\".', 'format', '(', 'img', '_', 'path', ',', 'Ġ\"', 'Did', 'n', \"'t\", 'Ġfind', 'Ġa', 'Ġface', '\"', 'Ġif', 'Ġlen', '(', 'face', '_', 'bounding', '_', 'boxes', ')', 'Ġ<', 'Ġ1', 'Ġelse', 'Ġ\"', 'Found', 'Ġmore', 'Ġthan', 'Ġone', 'Ġface', '\"))', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġelse', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġ#', 'ĠAdd', 'Ġface', 'Ġencoding', 'Ġfor', 'Ġcurrent', 'Ġimage', 'Ġto', 'Ġthe', 'Ġtraining', 'Ġset', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ĠX', '.', 'append', '(', 'face', '_', 'recogn', 'ition', '.', 'face', '_', 'encodings', '(', 'image', ',', 'Ġknown', '_', 'face', '_', 'locations', '=', 'face', '_', 'bounding', '_', 'boxes', ')[', '0', '])', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġy', '.', 'append', '(', 'class', '_', 'dir', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠDetermine', 'Ġhow', 'Ġmany', 'Ġneighbors', 'Ġto', 'Ġuse', 'Ġfor', 'Ġweighting', 'Ġin', 'Ġthe', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġif', 'Ġn', '_', 'neighbors', 'Ġis', 'ĠNone', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġn', '_', 'neighbors', 'Ġ=', 'Ġint', '(', 'round', '(', 'math', '.', 'sqrt', '(', 'len', '(', 'X', '))))', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġverbose', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġprint', '(\"', 'Cho', 'se', 'Ġn', '_', 'neighbors', 'Ġautomatically', ':\",', 'Ġn', '_', 'neighbors', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠCreate', 'Ġand', 'Ġtrain', 'Ġthe', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġknn', '_', 'clf', 'Ġ=', 'Ġneighbors', '.', 'K', 'Neighbors', 'Classifier', '(', 'n', '_', 'neighbors', '=', 'n', '_', 'neighbors', ',', 'Ġalgorithm', '=', 'knn', '_', 'algo', ',', 'Ġweights', \"='\", 'distance', \"')\", 'ĊĠĠĠ', 'Ġknn', '_', 'clf', '.', 'fit', '(', 'X', ',', 'Ġy', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠSave', 'Ġthe', 'Ġtrained', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġif', 'Ġmodel', '_', 'save', '_', 'path', 'Ġis', 'Ġnot', 'ĠNone', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġwith', 'Ġopen', '(', 'model', '_', 'save', '_', 'path', ',', \"Ġ'\", 'wb', \"')\", 'Ġas', 'Ġf', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġpickle', '.', 'dump', '(', 'knn', '_', 'clf', ',', 'Ġf', ')', 'ĊĊĠĠĠ', 'Ġreturn', 'Ġknn', '_', 'clf']}\n"
     ]
    }
   ],
   "source": [
    "# 新的Tokenizer分词效率高于原GPT2的Tokenizer\n",
    "# 其中Ġ表示空格，Ċ表示换行符\n",
    "\n",
    "print({\"old_tokenizer\": old_tokenizer.tokenize(raw_datasets[\"train\"][0][\"whole_func_string\"])})\n",
    "print(\"\\n\")\n",
    "print({\"new_tokenizer\": tokenizer.tokenize(raw_datasets[\"train\"][0][\"whole_func_string\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NoComment/code-search-net-tokenizer/commit/2a569fa2d2ad3c9798e0c54513c756897a047f2c', commit_message='Upload tokenizer', commit_description='', oid='2a569fa2d2ad3c9798e0c54513c756897a047f2c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.notebook_login()\n",
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §6.2 FastTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "              'input_ids': [101,\n",
      "                            20164,\n",
      "                            10932,\n",
      "                            10289,\n",
      "                            1110,\n",
      "                            1107,\n",
      "                            6010,\n",
      "                            119,\n",
      "                            102],\n",
      "              'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
      " 'encoding.is_fast': True,\n",
      " 'encoding.tokens()': ['[CLS]',\n",
      "                       'Hu',\n",
      "                       '##gging',\n",
      "                       'Face',\n",
      "                       'is',\n",
      "                       'in',\n",
      "                       'Brooklyn',\n",
      "                       '.',\n",
      "                       '[SEP]'],\n",
      " 'encoding.word_ids()': [None, 0, 0, 1, 2, 3, 4, 5, None],\n",
      " 'tokenizer.is_fast': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 在§5中，我们提到FastTokenizer与batched=True能显著提高分词效率\n",
    "# FastTokenizer的分词结果为BatchEncoding，这是dict的子类\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"Hugging Face is in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "pprint.pprint({\n",
    "    \"encoding\": encoding,\n",
    "    \"tokenizer.is_fast\": tokenizer.is_fast,\n",
    "    \"encoding.is_fast\": encoding.is_fast,\n",
    "    \"encoding.tokens()\": encoding.tokens(),\n",
    "    # \"##\"表示该Token与前一个Token同属一个单词，仅适用于FastBertTokenizer\n",
    "\n",
    "    \"encoding.word_ids()\": encoding.word_ids(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hugging'\n"
     ]
    }
   ],
   "source": [
    "# 若一个单词被拆成多个token，可以合并起来\n",
    "\n",
    "start, end = encoding.word_to_chars(0)\n",
    "pprint.pprint(example[start: end])\n",
    "\n",
    "# encoding.word_to_chars(int)\n",
    "# encoding.word_to_tokens(int)\n",
    "# encoding.token_to_chars(int)\n",
    "# encoding.char_to_token(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1422,  1271,  1110,   156,  7777,  2497,  1394,  1105,   146,\n",
       "          1250,  1120, 20164, 10932, 10289,  1107,  6010,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建实体识别所需的变量\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 10),\n",
       "  (11, 12),\n",
       "  (12, 14),\n",
       "  (14, 16),\n",
       "  (16, 18),\n",
       "  (19, 22),\n",
       "  (23, 24),\n",
       "  (25, 29),\n",
       "  (30, 32),\n",
       "  (33, 35),\n",
       "  (35, 40),\n",
       "  (41, 45),\n",
       "  (46, 48),\n",
       "  (49, 57),\n",
       "  (57, 58),\n",
       "  (0, 0)],\n",
       " ['',\n",
       "  'My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'S',\n",
       "  'yl',\n",
       "  'va',\n",
       "  'in',\n",
       "  'and',\n",
       "  'I',\n",
       "  'work',\n",
       "  'at',\n",
       "  'Hu',\n",
       "  'gging',\n",
       "  'Face',\n",
       "  'in',\n",
       "  'Brooklyn',\n",
       "  '.',\n",
       "  ''])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用return_offsets_mapping=True返回Token在原字符串中的范围\n",
    "\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"], [example[i: j] for i, j in inputs_with_offsets[\"offset_mapping\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'My', 'name', 'is', 'S', 'yl', 'va', 'in', 'and', 'I', 'work', 'at', 'Hu', 'gging', 'Face', 'in', 'Brooklyn', '.', '']\n",
      "['O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-LOC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(\n",
    "    outputs.logits, \n",
    "    dim=-1\n",
    ")[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "\n",
    "print([example[i: j] for i, j in inputs_with_offsets[\"offset_mapping\"]])\n",
    "print([model.config.id2label[i] for i in predictions])\n",
    "\n",
    "# 连续的I-*表示同一段实体，B-*表示从此处起新建一个实体。当多个实体连接时，第一个实体全为I-*，后面的实体第一个token均为B-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
