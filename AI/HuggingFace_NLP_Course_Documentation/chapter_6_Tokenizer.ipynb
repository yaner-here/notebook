{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0463e145a28547deaece23fcb5438d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers, datasets\n",
    "import pprint\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §6.1 微调Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07437a99273482f92af7a752eada134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30905107bab4d6292d88cb8dc0fea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6137dccf704e82bf35ab7ed2837a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25915215c30f4a979498e6c7b61290d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 准备文本数据集\n",
    "\n",
    "raw_datasets: datasets.DatasetDict = datasets.load_dataset(\n",
    "    \"code_search_net\", \"python\",\n",
    "    trust_remote_code=True\n",
    ") # type: ignore\n",
    "pprint.pprint(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成Dataloader迭代器\n",
    "\n",
    "def get_training_corpus():\n",
    "    training_corpus: typing.Generator[str, None, None] = (\n",
    "        raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "    return training_corpus\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从GPT2的Tokenizer开始训练\n",
    "\n",
    "old_tokenizer: transformers.GPT2TokenizerFast = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\"\n",
    ") # type: ignore\n",
    "tokenizer: GPT2TokenizerFast = old_tokenizer.train_new_from_iterator( # type: ignore\n",
    "    training_corpus,\n",
    "    52000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'old_tokenizer': ['def', 'Ġtrain', '(', 'train', '_', 'dir', ',', 'Ġmodel', '_', 'save', '_', 'path', '=', 'None', ',', 'Ġn', '_', 'ne', 'igh', 'bors', '=', 'None', ',', 'Ġkn', 'n', '_', 'al', 'go', \"='\", 'ball', '_', 'tree', \"',\", 'Ġverb', 'ose', '=', 'False', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠTr', 'ains', 'Ġa', 'Ġk', '-', 'ne', 'arest', 'Ġneighbors', 'Ġclass', 'ifier', 'Ġfor', 'Ġface', 'Ġrecognition', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġtrain', '_', 'dir', ':', 'Ġdirectory', 'Ġthat', 'Ġcontains', 'Ġa', 'Ġsub', '-', 'directory', 'Ġfor', 'Ġeach', 'Ġknown', 'Ġperson', ',', 'Ġwith', 'Ġits', 'Ġname', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ(', 'View', 'Ġin', 'Ġsource', 'Ġcode', 'Ġto', 'Ġsee', 'Ġtrain', '_', 'dir', 'Ġexample', 'Ġtree', 'Ġstructure', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠStructure', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ<', 'train', '_', 'dir', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '1', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '1', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '2', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ...', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '2', '>', '/', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '1', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶĤ', 'Ġ', 'Ġ', 'ĠâĶ', 'Ķ', 'âĶĢâĶĢ', 'Ġ<', 's', 'omen', 'ame', '2', '>.', 'j', 'peg', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠâĶ', 'Ķ', 'âĶĢâĶĢ', 'Ġ...', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġmodel', '_', 'save', '_', 'path', ':', 'Ġ(', 'optional', ')', 'Ġpath', 'Ġto', 'Ġsave', 'Ġmodel', 'Ġon', 'Ġdisk', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġn', '_', 'ne', 'igh', 'bors', ':', 'Ġ(', 'optional', ')', 'Ġnumber', 'Ġof', 'Ġneighbors', 'Ġto', 'Ġweigh', 'Ġin', 'Ġclassification', '.', 'ĠChosen', 'Ġautomatically', 'Ġif', 'Ġnot', 'Ġspecified', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġkn', 'n', '_', 'al', 'go', ':', 'Ġ(', 'optional', ')', 'Ġunderlying', 'Ġdata', 'Ġstructure', 'Ġto', 'Ġsupport', 'Ġkn', 'n', '.', 'default', 'Ġis', 'Ġball', '_', 'tree', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġverb', 'ose', ':', 'Ġverb', 'osity', 'Ġof', 'Ġtraining', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'return', ':', 'Ġreturns', 'Ġkn', 'n', 'Ġclass', 'ifier', 'Ġthat', 'Ġwas', 'Ġtrained', 'Ġon', 'Ġthe', 'Ġgiven', 'Ġdata', '.', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠX', 'Ġ=', 'Ġ[]', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġy', 'Ġ=', 'Ġ[]', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġperson', 'Ġin', 'Ġthe', 'Ġtraining', 'Ġset', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġclass', '_', 'dir', 'Ġin', 'Ġos', '.', 'list', 'dir', '(', 'train', '_', 'dir', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġnot', 'Ġos', '.', 'path', '.', 'isd', 'ir', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġcontinue', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġtraining', 'Ġimage', 'Ġfor', 'Ġthe', 'Ġcurrent', 'Ġperson', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġimg', '_', 'path', 'Ġin', 'Ġimage', '_', 'files', '_', 'in', '_', 'folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġimage', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'load', '_', 'image', '_', 'file', '(', 'img', '_', 'path', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġface', '_', 'bound', 'ing', '_', 'boxes', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'face', '_', 'loc', 'ations', '(', 'image', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġlen', '(', 'face', '_', 'bound', 'ing', '_', 'boxes', ')', 'Ġ!=', 'Ġ1', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠIf', 'Ġthere', 'Ġare', 'Ġno', 'Ġpeople', 'Ġ(', 'or', 'Ġtoo', 'Ġmany', 'Ġpeople', ')', 'Ġin', 'Ġa', 'Ġtraining', 'Ġimage', ',', 'Ġskip', 'Ġthe', 'Ġimage', '.', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġverb', 'ose', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Image', 'Ġ{}', 'Ġnot', 'Ġsuitable', 'Ġfor', 'Ġtraining', ':', 'Ġ{}', '\".', 'format', '(', 'img', '_', 'path', ',', 'Ġ\"', 'Did', 'n', \"'t\", 'Ġfind', 'Ġa', 'Ġface', '\"', 'Ġif', 'Ġlen', '(', 'face', '_', 'bound', 'ing', '_', 'boxes', ')', 'Ġ<', 'Ġ1', 'Ġelse', 'Ġ\"', 'Found', 'Ġmore', 'Ġthan', 'Ġone', 'Ġface', '\"))', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġelse', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠAdd', 'Ġface', 'Ġencoding', 'Ġfor', 'Ġcurrent', 'Ġimage', 'Ġto', 'Ġthe', 'Ġtraining', 'Ġset', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠX', '.', 'append', '(', 'face', '_', 'recogn', 'ition', '.', 'face', '_', 'enc', 'od', 'ings', '(', 'image', ',', 'Ġknown', '_', 'face', '_', 'loc', 'ations', '=', 'face', '_', 'bound', 'ing', '_', 'boxes', ')[', '0', '])', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġy', '.', 'append', '(', 'class', '_', 'dir', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠDeter', 'mine', 'Ġhow', 'Ġmany', 'Ġneighbors', 'Ġto', 'Ġuse', 'Ġfor', 'Ġweight', 'ing', 'Ġin', 'Ġthe', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġis', 'ĠNone', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġ=', 'Ġint', '(', 'round', '(', 'math', '.', 'sq', 'rt', '(', 'len', '(', 'X', '))))', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġverb', 'ose', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Ch', 'ose', 'Ġn', '_', 'ne', 'igh', 'bors', 'Ġautomatically', ':', '\",', 'Ġn', '_', 'ne', 'igh', 'bors', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠCreate', 'Ġand', 'Ġtrain', 'Ġthe', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġkn', 'n', '_', 'cl', 'f', 'Ġ=', 'Ġneighbors', '.', 'K', 'Neigh', 'bors', 'Class', 'ifier', '(', 'n', '_', 'ne', 'igh', 'bors', '=', 'n', '_', 'ne', 'igh', 'bors', ',', 'Ġalgorithm', '=', 'kn', 'n', '_', 'al', 'go', ',', 'Ġweights', \"='\", 'distance', \"')\", 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġkn', 'n', '_', 'cl', 'f', '.', 'fit', '(', 'X', ',', 'Ġy', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠSave', 'Ġthe', 'Ġtrained', 'ĠK', 'NN', 'Ġclass', 'ifier', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġmodel', '_', 'save', '_', 'path', 'Ġis', 'Ġnot', 'ĠNone', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġwith', 'Ġopen', '(', 'model', '_', 'save', '_', 'path', ',', \"Ġ'\", 'wb', \"')\", 'Ġas', 'Ġf', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġpick', 'le', '.', 'dump', '(', 'kn', 'n', '_', 'cl', 'f', ',', 'Ġf', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġkn', 'n', '_', 'cl', 'f']}\n",
      "\n",
      "\n",
      "{'new_tokenizer': ['def', 'Ġtrain', '(', 'train', '_', 'dir', ',', 'Ġmodel', '_', 'save', '_', 'path', '=', 'None', ',', 'Ġn', '_', 'neighbors', '=', 'None', ',', 'Ġknn', '_', 'algo', \"='\", 'ball', '_', 'tree', \"',\", 'Ġverbose', '=', 'False', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠTra', 'ins', 'Ġa', 'Ġk', '-', 'nearest', 'Ġneighbors', 'Ġclassifier', 'Ġfor', 'Ġface', 'Ġrecognition', '.', 'ĊĊĠĠĠ', 'Ġ:', 'param', 'Ġtrain', '_', 'dir', ':', 'Ġdirectory', 'Ġthat', 'Ġcontains', 'Ġa', 'Ġsub', '-', 'directory', 'Ġfor', 'Ġeach', 'Ġknown', 'Ġperson', ',', 'Ġwith', 'Ġits', 'Ġname', '.', 'ĊĊĠĠĠĠ', 'Ġ(', 'View', 'Ġin', 'Ġsource', 'Ġcode', 'Ġto', 'Ġsee', 'Ġtrain', '_', 'dir', 'Ġexample', 'Ġtree', 'Ġstructure', ')', 'ĊĊĠĠĠĠ', 'ĠStructure', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġ<', 'train', '_', 'dir', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '1', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '1', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '2', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ...', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'person', '2', '>/', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶľâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '1', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĤ', 'ĠĠ', 'ĠâĶĶâĶĢâĶĢ', 'Ġ<', 'som', 'ename', '2', '>.', 'jpeg', 'ĊĠĠĠĠĠĠĠ', 'ĠâĶĶâĶĢâĶĢ', 'Ġ...', 'ĊĊĠĠĠ', 'Ġ:', 'param', 'Ġmodel', '_', 'save', '_', 'path', ':', 'Ġ(', 'optional', ')', 'Ġpath', 'Ġto', 'Ġsave', 'Ġmodel', 'Ġon', 'Ġdisk', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġn', '_', 'neighbors', ':', 'Ġ(', 'optional', ')', 'Ġnumber', 'Ġof', 'Ġneighbors', 'Ġto', 'Ġwe', 'igh', 'Ġin', 'Ġclassification', '.', 'ĠCho', 'sen', 'Ġautomatically', 'Ġif', 'Ġnot', 'Ġspecified', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġknn', '_', 'algo', ':', 'Ġ(', 'optional', ')', 'Ġunderlying', 'Ġdata', 'Ġstructure', 'Ġto', 'Ġsupport', 'Ġknn', '.', 'default', 'Ġis', 'Ġball', '_', 'tree', 'ĊĠĠĠ', 'Ġ:', 'param', 'Ġverbose', ':', 'Ġverbosity', 'Ġof', 'Ġtraining', 'ĊĠĠĠ', 'Ġ:', 'return', ':', 'Ġreturns', 'Ġknn', 'Ġclassifier', 'Ġthat', 'Ġwas', 'Ġtrained', 'Ġon', 'Ġthe', 'Ġgiven', 'Ġdata', '.', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠX', 'Ġ=', 'Ġ[]', 'ĊĠĠĠ', 'Ġy', 'Ġ=', 'Ġ[]', 'ĊĊĠĠĠ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġperson', 'Ġin', 'Ġthe', 'Ġtraining', 'Ġset', 'ĊĠĠĠ', 'Ġfor', 'Ġclass', '_', 'dir', 'Ġin', 'Ġos', '.', 'listdir', '(', 'train', '_', 'dir', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġnot', 'Ġos', '.', 'path', '.', 'isdir', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')):', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcontinue', 'ĊĊĠĠĠĠĠĠĠ', 'Ġ#', 'ĠLoop', 'Ġthrough', 'Ġeach', 'Ġtraining', 'Ġimage', 'Ġfor', 'Ġthe', 'Ġcurrent', 'Ġperson', 'ĊĠĠĠĠĠĠĠ', 'Ġfor', 'Ġimg', '_', 'path', 'Ġin', 'Ġimage', '_', 'files', '_', 'in', '_', 'folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train', '_', 'dir', ',', 'Ġclass', '_', 'dir', ')):', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġimage', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'load', '_', 'image', '_', 'file', '(', 'img', '_', 'path', ')', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġface', '_', 'bounding', '_', 'boxes', 'Ġ=', 'Ġface', '_', 'recogn', 'ition', '.', 'face', '_', 'locations', '(', 'image', ')', 'ĊĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġif', 'Ġlen', '(', 'face', '_', 'bounding', '_', 'boxes', ')', 'Ġ!=', 'Ġ1', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġ#', 'ĠIf', 'Ġthere', 'Ġare', 'Ġno', 'Ġpeople', 'Ġ(', 'or', 'Ġtoo', 'Ġmany', 'Ġpeople', ')', 'Ġin', 'Ġa', 'Ġtraining', 'Ġimage', ',', 'Ġskip', 'Ġthe', 'Ġimage', '.', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġif', 'Ġverbose', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġprint', '(\"', 'Image', 'Ġ{}', 'Ġnot', 'Ġsuitable', 'Ġfor', 'Ġtraining', ':', 'Ġ{}\".', 'format', '(', 'img', '_', 'path', ',', 'Ġ\"', 'Did', 'n', \"'t\", 'Ġfind', 'Ġa', 'Ġface', '\"', 'Ġif', 'Ġlen', '(', 'face', '_', 'bounding', '_', 'boxes', ')', 'Ġ<', 'Ġ1', 'Ġelse', 'Ġ\"', 'Found', 'Ġmore', 'Ġthan', 'Ġone', 'Ġface', '\"))', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġelse', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġ#', 'ĠAdd', 'Ġface', 'Ġencoding', 'Ġfor', 'Ġcurrent', 'Ġimage', 'Ġto', 'Ġthe', 'Ġtraining', 'Ġset', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ĠX', '.', 'append', '(', 'face', '_', 'recogn', 'ition', '.', 'face', '_', 'encodings', '(', 'image', ',', 'Ġknown', '_', 'face', '_', 'locations', '=', 'face', '_', 'bounding', '_', 'boxes', ')[', '0', '])', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġy', '.', 'append', '(', 'class', '_', 'dir', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠDetermine', 'Ġhow', 'Ġmany', 'Ġneighbors', 'Ġto', 'Ġuse', 'Ġfor', 'Ġweighting', 'Ġin', 'Ġthe', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġif', 'Ġn', '_', 'neighbors', 'Ġis', 'ĠNone', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġn', '_', 'neighbors', 'Ġ=', 'Ġint', '(', 'round', '(', 'math', '.', 'sqrt', '(', 'len', '(', 'X', '))))', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġverbose', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġprint', '(\"', 'Cho', 'se', 'Ġn', '_', 'neighbors', 'Ġautomatically', ':\",', 'Ġn', '_', 'neighbors', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠCreate', 'Ġand', 'Ġtrain', 'Ġthe', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġknn', '_', 'clf', 'Ġ=', 'Ġneighbors', '.', 'K', 'Neighbors', 'Classifier', '(', 'n', '_', 'neighbors', '=', 'n', '_', 'neighbors', ',', 'Ġalgorithm', '=', 'knn', '_', 'algo', ',', 'Ġweights', \"='\", 'distance', \"')\", 'ĊĠĠĠ', 'Ġknn', '_', 'clf', '.', 'fit', '(', 'X', ',', 'Ġy', ')', 'ĊĊĠĠĠ', 'Ġ#', 'ĠSave', 'Ġthe', 'Ġtrained', 'ĠK', 'NN', 'Ġclassifier', 'ĊĠĠĠ', 'Ġif', 'Ġmodel', '_', 'save', '_', 'path', 'Ġis', 'Ġnot', 'ĠNone', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġwith', 'Ġopen', '(', 'model', '_', 'save', '_', 'path', ',', \"Ġ'\", 'wb', \"')\", 'Ġas', 'Ġf', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġpickle', '.', 'dump', '(', 'knn', '_', 'clf', ',', 'Ġf', ')', 'ĊĊĠĠĠ', 'Ġreturn', 'Ġknn', '_', 'clf']}\n"
     ]
    }
   ],
   "source": [
    "# 新的Tokenizer分词效率高于原GPT2的Tokenizer\n",
    "# 其中Ġ表示空格，Ċ表示换行符\n",
    "\n",
    "print({\"old_tokenizer\": old_tokenizer.tokenize(raw_datasets[\"train\"][0][\"whole_func_string\"])})\n",
    "print(\"\\n\")\n",
    "print({\"new_tokenizer\": tokenizer.tokenize(raw_datasets[\"train\"][0][\"whole_func_string\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NoComment/code-search-net-tokenizer/commit/2a569fa2d2ad3c9798e0c54513c756897a047f2c', commit_message='Upload tokenizer', commit_description='', oid='2a569fa2d2ad3c9798e0c54513c756897a047f2c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.notebook_login()\n",
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §6.2 FastTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "              'input_ids': [101,\n",
      "                            20164,\n",
      "                            10932,\n",
      "                            10289,\n",
      "                            1110,\n",
      "                            1107,\n",
      "                            6010,\n",
      "                            119,\n",
      "                            102],\n",
      "              'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
      " 'encoding.is_fast': True,\n",
      " 'encoding.tokens()': ['[CLS]',\n",
      "                       'Hu',\n",
      "                       '##gging',\n",
      "                       'Face',\n",
      "                       'is',\n",
      "                       'in',\n",
      "                       'Brooklyn',\n",
      "                       '.',\n",
      "                       '[SEP]'],\n",
      " 'encoding.word_ids()': [None, 0, 0, 1, 2, 3, 4, 5, None],\n",
      " 'tokenizer.is_fast': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 在§5中，我们提到FastTokenizer与batched=True能显著提高分词效率\n",
    "# FastTokenizer的分词结果为BatchEncoding，这是dict的子类\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"Hugging Face is in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "pprint.pprint({\n",
    "    \"encoding\": encoding,\n",
    "    \"tokenizer.is_fast\": tokenizer.is_fast,\n",
    "    \"encoding.is_fast\": encoding.is_fast,\n",
    "    \"encoding.tokens()\": encoding.tokens(),\n",
    "    # \"##\"表示该Token与前一个Token同属一个单词，仅适用于FastBertTokenizer\n",
    "\n",
    "    \"encoding.word_ids()\": encoding.word_ids(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hugging'\n"
     ]
    }
   ],
   "source": [
    "# 若一个单词被拆成多个token，可以合并起来\n",
    "\n",
    "start, end = encoding.word_to_chars(0)\n",
    "pprint.pprint(example[start: end])\n",
    "\n",
    "# encoding.word_to_chars(int)\n",
    "# encoding.word_to_tokens(int)\n",
    "# encoding.token_to_chars(int)\n",
    "# encoding.char_to_token(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
       "          -2.0860,  0.3335],\n",
       "         [ 8.4973, -2.3986, -1.3582, -2.7887,  0.7575, -1.8873,  0.4344,\n",
       "          -1.9900, -0.3397],\n",
       "         [ 9.4719, -2.2261, -0.9849, -2.6116,  0.1219, -2.0627, -0.1259,\n",
       "          -1.8758, -0.0609],\n",
       "         [ 9.8670, -2.2175, -1.3125, -2.4866, -0.2550, -1.8536,  0.0856,\n",
       "          -1.7520, -0.6437],\n",
       "         [-0.2011, -2.1873, -1.5316, -2.7110,  8.4025, -2.4168, -0.6980,\n",
       "          -3.0337, -0.0997],\n",
       "         [ 0.1065, -2.0520, -1.4787, -2.8139,  7.4525, -2.8399, -0.0626,\n",
       "          -3.3666, -0.4683],\n",
       "         [ 0.5985, -2.2538, -1.1926, -3.0111,  7.0070, -2.8675,  0.3492,\n",
       "          -3.3129, -0.2878],\n",
       "         [-0.0584, -2.2660, -1.4335, -3.1940,  8.3225, -2.6212, -0.0348,\n",
       "          -2.9780, -0.2957],\n",
       "         [ 9.6889, -2.4281, -1.5653, -2.5225, -0.9693, -1.5668,  0.4285,\n",
       "          -1.9413, -0.6774],\n",
       "         [ 9.0116, -2.1216, -1.4140, -2.6964,  0.2728, -1.7851,  0.3635,\n",
       "          -1.8407, -0.5922],\n",
       "         [ 9.5258, -2.2616, -1.4557, -2.9603, -0.1311, -1.7799,  0.9169,\n",
       "          -2.2549, -0.9692],\n",
       "         [ 9.1087, -2.2834, -1.3437, -2.8742, -0.2521, -1.5712,  1.1501,\n",
       "          -2.0786, -0.8658],\n",
       "         [ 2.5185, -3.1537, -1.6923, -3.4240,  1.4335, -1.8089,  6.5008,\n",
       "          -3.0264, -0.2619],\n",
       "         [ 1.7707, -2.4992, -0.1088, -3.2825,  0.4034, -1.4262,  5.9701,\n",
       "          -2.6502, -0.1259],\n",
       "         [ 0.6466, -2.9276, -0.1020, -3.0776,  0.7036, -1.2746,  6.3889,\n",
       "          -2.7266,  0.3822],\n",
       "         [ 9.2571, -2.6779, -1.2145, -2.7276, -0.9370, -1.5445,  1.1025,\n",
       "          -1.8477, -0.3661],\n",
       "         [-0.2206, -2.5108, -1.2976, -2.9758, -0.5795, -2.2071,  1.8236,\n",
       "          -1.6484,  7.0975],\n",
       "         [ 8.7507, -2.2626, -1.5300, -2.2890, -0.6513, -2.0016, -0.0112,\n",
       "          -2.0860,  0.3335],\n",
       "         [ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
       "          -2.0860,  0.3335]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
