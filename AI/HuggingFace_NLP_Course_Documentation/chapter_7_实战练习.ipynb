{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, datasets, accelerate\n",
    "import pprint\n",
    "import typing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §7.1 实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 下载数据集\n",
    "\n",
    "raw_datasets: datasets.DatasetDict = datasets.load_dataset(\n",
    "    \"conll2003\",\n",
    "    trust_remote_code=True\n",
    ") # type: ignore\n",
    "\n",
    "pprint.pprint(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner_tags_map': ['O',\n",
      "                  'B-PER',\n",
      "                  'I-PER',\n",
      "                  'B-ORG',\n",
      "                  'I-ORG',\n",
      "                  'B-LOC',\n",
      "                  'I-LOC',\n",
      "                  'B-MISC',\n",
      "                  'I-MISC']}\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 使用.features()方法获取对应关系\n",
    "\n",
    "ner_tags_map: list[str] = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "pprint.pprint({\"ner_tags_map\": ner_tags_map})\n",
    "\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])\n",
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print([ner_tags_map[i] for i in raw_datasets[\"train\"][0][\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_split_into_words=False': ['[CLS]', 'EU', '[SEP]'],\n",
      " 'is_split_into_words=True': ['[CLS]',\n",
      "                              'EU',\n",
      "                              'rejects',\n",
      "                              'German',\n",
      "                              'call',\n",
      "                              'to',\n",
      "                              'boycott',\n",
      "                              'British',\n",
      "                              'la',\n",
      "                              '##mb',\n",
      "                              '.',\n",
      "                              '[SEP]']}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\"\n",
    ")\n",
    "\n",
    "pprint.pprint({\n",
    "    \"is_split_into_words=True\": tokenizer(\n",
    "        raw_datasets[\"train\"][0][\"tokens\"], \n",
    "        is_split_into_words=True # 当输入为list[str<token>]时使用\n",
    "    ).tokens(),\n",
    "    \"is_split_into_words=False\": tokenizer(\n",
    "        raw_datasets[\"train\"][0][\"tokens\"], \n",
    "        is_split_into_words=False # 当输入为str时使用\n",
    "    ).tokens()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'example_final_alignment_result': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100],\n",
      " 'example_labels': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
      " 'example_tokens': ['EU',\n",
      "                    'rejects',\n",
      "                    'German',\n",
      "                    'call',\n",
      "                    'to',\n",
      "                    'boycott',\n",
      "                    'British',\n",
      "                    'lamb',\n",
      "                    '.'],\n",
      " 'example_word_ids': [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]}\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels: list[int], word_ids: list[int]) -> list[int]:\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(-100 if word_id is None else labels[word_id])\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            new_labels.append(labels[word_id] + (1 if labels[word_id] % 2 == 1 else 0)) # type: ignore\n",
    "    return new_labels\n",
    "\n",
    "example_tokens: list[int] = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "example_labels: list[int] = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "example_word_ids: list[int | None] = tokenizer(\n",
    "    raw_datasets[\"train\"][0][\"tokens\"],\n",
    "    is_split_into_words=True\n",
    ").word_ids()\n",
    "\n",
    "pprint.pprint({\n",
    "    \"example_tokens\": example_tokens,\n",
    "    \"example_labels\": example_labels,\n",
    "    \"example_word_ids\": example_word_ids,\n",
    "    \"example_final_alignment_result\": align_labels_with_tokens(\n",
    "        example_labels, example_word_ids # type: ignore\n",
    "    )\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b596ac06991440e6928c1429b772a828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9a9638605749a09b154819bab6b66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7f3ea3bbcd4a178115da0623bb74be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
