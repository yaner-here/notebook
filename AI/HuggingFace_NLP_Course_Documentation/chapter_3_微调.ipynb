{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, datasets, evaluate\n",
    "\n",
    "import torch, torch.utils, torch.utils.data\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "import numpy, scipy, sklearn\n",
    "\n",
    "import tqdm.auto\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(checkpoint).to(\"cuda:0\")\n",
    "\n",
    "sequences = [\n",
    "    \"I like apple\",\n",
    "    \"Apple is a fruit\"\n",
    "]\n",
    "\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "batch[\"labels\"] = torch.tensor([1, 1]).cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §3.1 datasets库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_datasets': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "}),\n",
      " 'raw_datasets_feature': {'idx': Value(dtype='int32', id=None),\n",
      "                          'label': ClassLabel(names=['not_equivalent',\n",
      "                                                     'equivalent'],\n",
      "                                              id=None),\n",
      "                          'sentence1': Value(dtype='string', id=None),\n",
      "                          'sentence2': Value(dtype='string', id=None)},\n",
      " 'raw_one_data_demo': {'idx': 0,\n",
      "                       'label': 1,\n",
      "                       'sentence1': \"PCCW 's chief operating officer , Mike \"\n",
      "                                    'Butcher , and Alex Arena , the chief '\n",
      "                                    'financial officer , will report directly '\n",
      "                                    'to Mr So .',\n",
      "                       'sentence2': 'Current Chief Operating Officer Mike '\n",
      "                                    'Butcher and Group Chief Financial Officer '\n",
      "                                    'Alex Arena will report to So .'}}\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"glue\", \"mrpc\")\n",
    "pprint.pprint({\n",
    "    \"raw_datasets\": raw_datasets,\n",
    "    \"raw_datasets_feature\": raw_datasets[\"test\"].features,\n",
    "    \"raw_one_data_demo\": raw_datasets[\"test\"][0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_dataset': {'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]),\n",
      "                       'input_ids': tensor([[  101,  2572,  3217,  ...,     0,     0,     0],\n",
      "        [  101,  9805,  3540,  ...,     0,     0,     0],\n",
      "        [  101,  2027,  2018,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1000,  2057,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 26828,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2382,  ...,     0,     0,     0]]),\n",
      "                       'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])},\n",
      " 'tokenized_dataset_shape': {'attention_mask': torch.Size([3668, 103]),\n",
      "                             'input_ids': torch.Size([3668, 103]),\n",
      "                             'token_type_ids': torch.Size([3668, 103])}}\n"
     ]
    }
   ],
   "source": [
    "# 一次性加载所有数据集到内存中，for()单独每条数据处理\n",
    "\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pprint.pprint({\n",
    "    \"tokenized_dataset\": tokenized_dataset,\n",
    "    \"tokenized_dataset_shape\": {key_name: tokenized_dataset[key_name].shape for key_name in tokenized_dataset.keys()}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3548f77fbaa4ea181cbd3f5bc916ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_dataset': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "}),\n",
      " 'tokenized_dataset_shape': {'test': (1725, 7),\n",
      "                             'train': (3668, 7),\n",
      "                             'validation': (408, 7)},\n",
      " '句子转化成长度不同的token': tensor([49, 72, 60,  ..., 35, 74, 81])}\n"
     ]
    }
   ],
   "source": [
    "# 一次性加载所有数据集到内存中，map(batch=True)批量处理，速度快\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(\n",
    "    function=lambda examples: tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True), \n",
    "    batched=True\n",
    ")\n",
    "pprint.pprint({\n",
    "    \"tokenized_dataset\": tokenized_dataset,\n",
    "    \"tokenized_dataset_shape\": {key_name: tokenized_dataset[key_name].shape for key_name in tokenized_dataset.keys()},\n",
    "    \"句子转化成长度不同的token\":  torch.tensor([len(data[\"input_ids\"]) for data in tokenized_dataset[\"test\"]])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_shape': {'attention_mask': torch.Size([3668, 103]),\n",
      "                 'input_ids': torch.Size([3668, 103]),\n",
      "                 'labels': torch.Size([3668])},\n",
      " 'samples_content': {'input_ids': <class 'list'>, 'label': <class 'list'>},\n",
      " 'type(batch)': <class 'transformers.tokenization_utils_base.BatchEncoding'>,\n",
      " 'type(samples)': <class 'dict'>}\n"
     ]
    }
   ],
   "source": [
    "# Padding到相同长度\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# tokenized_dataset = data_collator(tokenized_dataset[:8],)\n",
    "\n",
    "samples = tokenized_dataset[\"train\"][:]\n",
    "# samples: dict[\n",
    "#     \"sentence1\": list[str],\n",
    "#     \"sentence2\": list[str],\n",
    "#     \"label\": list[int],\n",
    "#     \"idx\": list[int],\n",
    "#     \"input_ids\": list[list[int]]\n",
    "# ]\n",
    "\n",
    "samples = {k: v for k, v in samples.items() if k in [\"label\", \"input_ids\"]}\n",
    "# samples: dict[\n",
    "#     \"label\": list[int],\n",
    "#     \"input_ids\": list[list[int]]\n",
    "# ]\n",
    "\n",
    "batch = data_collator(samples)\n",
    "\n",
    "pprint.pprint({\n",
    "    \"type(samples)\": type(samples),\n",
    "    \"samples_content\": {key: type(value) for key, value in samples.items()},\n",
    "    \"type(batch)\": type(batch),\n",
    "    \"batch_shape\": {k: v.shape for k, v in batch.items()},\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §3.2 Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = datasets.load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    function=tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac149afd0e0f4bacb9ccf955634c7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5363, 'grad_norm': 1.1965887546539307, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2833, 'grad_norm': 0.08787903189659119, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n",
      "{'train_runtime': 1052.9603, 'train_samples_per_second': 10.451, 'train_steps_per_second': 1.308, 'train_loss': 0.34164859491928645, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.34164859491928645, metrics={'train_runtime': 1052.9603, 'train_samples_per_second': 10.451, 'train_steps_per_second': 1.308, 'total_flos': 405114969714960.0, 'train_loss': 0.34164859491928645, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "training_args = transformers.TrainingArguments(\"test-trainer\") # 模型训练的保存目录\n",
    "\n",
    "# 定义模型\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# tokenized_datasets[\"train\"]: Dataset({\n",
    "#     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#     num_rows: 3668\n",
    "# })\n",
    "\n",
    "# tokenized_datasets[\"validation\"]: Dataset({\n",
    "#     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#     num_rows: 408\n",
    "# })\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fd65ae83f745f2ac10c88234e57c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "# trainer.predict(tokenized_datasets[\"validation\"]): Dataset({\n",
    "#     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#     num_rows: 408\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': PredictionOutput(predictions=array([[-3.015744  ,  3.6877723 ],\n",
      "       [ 2.5303028 , -3.4010143 ],\n",
      "       [ 1.1536517 , -0.8440161 ],\n",
      "       [-2.9627337 ,  3.666817  ],\n",
      "       [ 2.534016  , -3.3643885 ],\n",
      "       [-2.8641746 ,  3.587876  ],\n",
      "       [-2.7236996 ,  3.5345864 ],\n",
      "       [-2.983623  ,  3.6432285 ],\n",
      "       [-2.4337814 ,  3.3980978 ],\n",
      "       [-3.0381052 ,  3.6667738 ],\n",
      "       [-2.9769688 ,  3.6420445 ],\n",
      "       [ 2.5517476 , -3.466066  ],\n",
      "       [ 2.352597  , -3.2868183 ],\n",
      "       [-2.7957087 ,  3.5809133 ],\n",
      "       [-3.0151188 ,  3.6849408 ],\n",
      "       [-1.1385697 ,  2.0387857 ],\n",
      "       [-3.0073507 ,  3.6618736 ],\n",
      "       [ 2.2607965 , -2.9414856 ],\n",
      "       [-3.006884  ,  3.653608  ],\n",
      "       [ 2.2340896 , -2.6657424 ],\n",
      "       [ 2.487991  , -3.3157384 ],\n",
      "       [-1.5030047 ,  2.4938443 ],\n",
      "       [ 1.172013  , -1.5260674 ],\n",
      "       [-2.9847126 ,  3.6555583 ],\n",
      "       [-2.947463  ,  3.6641955 ],\n",
      "       [-1.3008718 ,  2.2326744 ],\n",
      "       [-2.488938  ,  3.366145  ],\n",
      "       [-3.0472627 ,  3.6620755 ],\n",
      "       [-2.4241064 ,  3.320619  ],\n",
      "       [-2.7691166 ,  3.5794284 ],\n",
      "       [ 0.46797088, -0.1873151 ],\n",
      "       [-3.0261497 ,  3.6516387 ],\n",
      "       [-2.340435  ,  3.1693144 ],\n",
      "       [-2.75164   ,  3.588074  ],\n",
      "       [-2.9658694 ,  3.6522615 ],\n",
      "       [-2.6664805 ,  3.4532683 ],\n",
      "       [ 1.0168123 , -1.596672  ],\n",
      "       [ 2.6470053 , -3.460923  ],\n",
      "       [-2.6689334 ,  3.5264556 ],\n",
      "       [-3.0557866 ,  3.6598885 ],\n",
      "       [ 2.766772  , -3.4631581 ],\n",
      "       [-2.9129581 ,  3.6400976 ],\n",
      "       [-0.0888933 , -0.39949745],\n",
      "       [ 2.3102717 , -3.0369184 ],\n",
      "       [ 0.8971225 , -0.21460904],\n",
      "       [-2.8277707 ,  3.6235878 ],\n",
      "       [-2.953574  ,  3.655971  ],\n",
      "       [ 2.2926953 , -3.323365  ],\n",
      "       [-2.9645112 ,  3.6684783 ],\n",
      "       [-2.949696  ,  3.65852   ],\n",
      "       [-2.6543386 ,  3.5134368 ],\n",
      "       [-2.7031376 ,  3.5001652 ],\n",
      "       [-2.78691   ,  3.6195061 ],\n",
      "       [-2.8858705 ,  3.6762333 ],\n",
      "       [-3.003302  ,  3.6771    ],\n",
      "       [-2.8430228 ,  3.601465  ],\n",
      "       [-1.9164948 ,  2.8612761 ],\n",
      "       [-3.0194778 ,  3.6762044 ],\n",
      "       [-3.0119958 ,  3.68226   ],\n",
      "       [-2.6957257 ,  3.5018177 ],\n",
      "       [-2.6969085 ,  3.5191042 ],\n",
      "       [-1.6268376 ,  2.4678564 ],\n",
      "       [-2.9941342 ,  3.6794503 ],\n",
      "       [-2.316599  ,  3.2751129 ],\n",
      "       [-2.9050636 ,  3.6421835 ],\n",
      "       [ 2.0183587 , -2.081143  ],\n",
      "       [-3.0278866 ,  3.658667  ],\n",
      "       [-3.0232582 ,  3.6699834 ],\n",
      "       [-1.4672288 ,  2.1638477 ],\n",
      "       [-2.9723237 ,  3.642596  ],\n",
      "       [-3.0170898 ,  3.6703324 ],\n",
      "       [ 2.1529706 , -2.9550524 ],\n",
      "       [-3.003817  ,  3.6449914 ],\n",
      "       [-2.5031347 ,  3.3383784 ],\n",
      "       [-2.5307367 ,  3.4421191 ],\n",
      "       [-2.82807   ,  3.5657814 ],\n",
      "       [-2.5865471 ,  3.4820151 ],\n",
      "       [-3.0190947 ,  3.6905155 ],\n",
      "       [-2.7884767 ,  3.5958142 ],\n",
      "       [-2.790467  ,  3.547135  ],\n",
      "       [-1.9858667 ,  2.9776556 ],\n",
      "       [-2.985182  ,  3.6630635 ],\n",
      "       [-2.9900682 ,  3.6613486 ],\n",
      "       [ 2.2156842 , -3.0644364 ],\n",
      "       [-2.8810325 ,  3.6080432 ],\n",
      "       [-1.7730172 ,  2.6620135 ],\n",
      "       [-2.7860672 ,  3.559581  ],\n",
      "       [-1.3814858 ,  2.3086016 ],\n",
      "       [-3.0056574 ,  3.6840694 ],\n",
      "       [-3.0144374 ,  3.6537151 ],\n",
      "       [-2.2791007 ,  3.1155994 ],\n",
      "       [-2.8833408 ,  3.6614358 ],\n",
      "       [-2.6869771 ,  3.476392  ],\n",
      "       [-2.753639  ,  3.5844283 ],\n",
      "       [-2.995918  ,  3.6726341 ],\n",
      "       [-2.9752836 ,  3.6459358 ],\n",
      "       [ 2.4738653 , -2.83227   ],\n",
      "       [-2.711705  ,  3.5761695 ],\n",
      "       [ 0.880778  , -0.6007544 ],\n",
      "       [-2.8379478 ,  3.643045  ],\n",
      "       [-2.9448354 ,  3.6537118 ],\n",
      "       [ 2.3879175 , -3.091029  ],\n",
      "       [-2.7493758 ,  3.5104284 ],\n",
      "       [-3.0029137 ,  3.6772642 ],\n",
      "       [-0.8728035 ,  1.590597  ],\n",
      "       [-2.8554704 ,  3.6135824 ],\n",
      "       [-2.5493412 ,  3.372612  ],\n",
      "       [ 2.3250346 , -3.3005261 ],\n",
      "       [ 2.4540825 , -3.2734668 ],\n",
      "       [-2.9866233 ,  3.6698563 ],\n",
      "       [-2.3912718 ,  3.260228  ],\n",
      "       [-2.759007  ,  3.5235236 ],\n",
      "       [-2.7204137 ,  3.5903492 ],\n",
      "       [-3.0282376 ,  3.6750424 ],\n",
      "       [-2.6439614 ,  3.5017    ],\n",
      "       [ 2.546879  , -3.3016067 ],\n",
      "       [-2.8171346 ,  3.6058314 ],\n",
      "       [-2.9312465 ,  3.6177404 ],\n",
      "       [-2.986196  ,  3.65845   ],\n",
      "       [-3.0425305 ,  3.657776  ],\n",
      "       [-2.950623  ,  3.6524768 ],\n",
      "       [-2.3731267 ,  3.2800307 ],\n",
      "       [ 2.4415758 , -3.3440008 ],\n",
      "       [-2.778154  ,  3.593956  ],\n",
      "       [-3.0223513 ,  3.6665673 ],\n",
      "       [-2.96287   ,  3.6715038 ],\n",
      "       [-2.896291  ,  3.6486263 ],\n",
      "       [ 2.4779565 , -3.2839565 ],\n",
      "       [-3.0242767 ,  3.6545465 ],\n",
      "       [-2.9731798 ,  3.6593187 ],\n",
      "       [-2.8657882 ,  3.6250024 ],\n",
      "       [ 2.2487235 , -2.8707743 ],\n",
      "       [-2.6346524 ,  3.4972973 ],\n",
      "       [-0.70124835,  0.64628166],\n",
      "       [-2.0123258 ,  2.9963577 ],\n",
      "       [-2.5120099 ,  3.3732347 ],\n",
      "       [ 2.345614  , -3.1780303 ],\n",
      "       [ 2.6034155 , -3.368936  ],\n",
      "       [-3.0008368 ,  3.6755383 ],\n",
      "       [-2.877335  ,  3.6180677 ],\n",
      "       [-3.0110009 ,  3.6767328 ],\n",
      "       [-0.04928911,  0.6325459 ],\n",
      "       [ 2.7070165 , -3.4893117 ],\n",
      "       [-2.654366  ,  3.4994607 ],\n",
      "       [ 2.6754503 , -3.4977274 ],\n",
      "       [-2.6568666 ,  3.498456  ],\n",
      "       [-3.0165071 ,  3.6509047 ],\n",
      "       [-1.7002215 ,  2.5729265 ],\n",
      "       [ 2.6036966 , -3.4332414 ],\n",
      "       [-2.9154496 ,  3.6385844 ],\n",
      "       [ 2.4869943 , -3.3375757 ],\n",
      "       [-2.6656592 ,  3.4987347 ],\n",
      "       [-2.2197413 ,  3.2109563 ],\n",
      "       [-3.0078561 ,  3.682342  ],\n",
      "       [ 0.48733753,  0.06274812],\n",
      "       [-2.9607    ,  3.6696868 ],\n",
      "       [-2.9369006 ,  3.641857  ],\n",
      "       [-2.5504665 ,  3.4222276 ],\n",
      "       [ 1.9724178 , -2.6306841 ],\n",
      "       [-2.6452317 ,  3.4861405 ],\n",
      "       [-2.6080742 ,  3.4771905 ],\n",
      "       [-2.9462361 ,  3.6721067 ],\n",
      "       [-3.0610178 ,  3.6581829 ],\n",
      "       [-2.9943619 ,  3.6939337 ],\n",
      "       [-2.8564653 ,  3.6105416 ],\n",
      "       [-2.7453368 ,  3.5853765 ],\n",
      "       [-2.4202852 ,  3.3121595 ],\n",
      "       [ 1.945562  , -2.8554003 ],\n",
      "       [-2.5243921 ,  3.4510467 ],\n",
      "       [ 2.5379088 , -3.3279436 ],\n",
      "       [ 2.3520463 , -3.0329063 ],\n",
      "       [-1.8023881 ,  2.6976993 ],\n",
      "       [-1.934641  ,  2.8797042 ],\n",
      "       [-2.8184953 ,  3.6080086 ],\n",
      "       [-1.5956949 ,  2.4796991 ],\n",
      "       [-2.7241638 ,  3.5832028 ],\n",
      "       [-2.8193727 ,  3.5846477 ],\n",
      "       [ 2.4839401 , -3.3085825 ],\n",
      "       [-2.995852  ,  3.671318  ],\n",
      "       [-2.9126697 ,  3.6482313 ],\n",
      "       [-2.181533  ,  3.1220844 ],\n",
      "       [-0.8109911 ,  1.5462185 ],\n",
      "       [-2.9660618 ,  3.6479533 ],\n",
      "       [-2.8662941 ,  3.638143  ],\n",
      "       [-1.0444689 ,  1.6191134 ],\n",
      "       [-2.7702188 ,  3.5848553 ],\n",
      "       [ 2.2136564 , -3.1556468 ],\n",
      "       [-1.5911454 ,  2.42483   ],\n",
      "       [ 2.58606   , -3.452636  ],\n",
      "       [-2.4176142 ,  3.31776   ],\n",
      "       [-2.9500794 ,  3.6884995 ],\n",
      "       [ 2.6872387 , -3.4820337 ],\n",
      "       [-1.3369087 ,  2.3243752 ],\n",
      "       [-2.996261  ,  3.6749277 ],\n",
      "       [ 0.74684364, -1.3267835 ],\n",
      "       [-2.2896686 ,  3.290135  ],\n",
      "       [-2.9602277 ,  3.6584783 ],\n",
      "       [-1.9560826 ,  2.9204478 ],\n",
      "       [-2.6647327 ,  3.505692  ],\n",
      "       [-2.8164902 ,  3.5983973 ],\n",
      "       [-2.7462282 ,  3.5140636 ],\n",
      "       [-2.611605  ,  3.4434073 ],\n",
      "       [-1.8083876 ,  2.9056168 ],\n",
      "       [-2.7326224 ,  3.5458393 ],\n",
      "       [-2.7555468 ,  3.5579038 ],\n",
      "       [ 2.573042  , -3.4259245 ],\n",
      "       [-2.942259  ,  3.6754284 ],\n",
      "       [-2.368815  ,  3.2540886 ],\n",
      "       [-1.9192362 ,  2.8798518 ],\n",
      "       [ 0.37099457,  0.07940461],\n",
      "       [ 0.9811397 , -1.7647026 ],\n",
      "       [-2.9377983 ,  3.6580172 ],\n",
      "       [-2.1529052 ,  3.213991  ],\n",
      "       [ 2.1461105 , -2.4306953 ],\n",
      "       [-2.7080708 ,  3.5781283 ],\n",
      "       [-3.0269566 ,  3.6522322 ],\n",
      "       [-2.727915  ,  3.516139  ],\n",
      "       [-2.9217212 ,  3.633797  ],\n",
      "       [ 2.5302494 , -3.3475742 ],\n",
      "       [-2.6582553 ,  3.516448  ],\n",
      "       [-2.0714138 ,  2.8786416 ],\n",
      "       [-2.4673686 ,  3.3694623 ],\n",
      "       [-2.972384  ,  3.638686  ],\n",
      "       [ 2.5935202 , -3.4489126 ],\n",
      "       [-2.8954294 ,  3.611965  ],\n",
      "       [-2.9892795 ,  3.6660447 ],\n",
      "       [-3.0335567 ,  3.6487033 ],\n",
      "       [-2.6437755 ,  3.4473705 ],\n",
      "       [-2.6385427 ,  3.434991  ],\n",
      "       [-2.938422  ,  3.6422107 ],\n",
      "       [-3.0080237 ,  3.6418288 ],\n",
      "       [-2.7784169 ,  3.571849  ],\n",
      "       [-0.31961325,  1.1970788 ],\n",
      "       [ 2.003944  , -2.7116947 ],\n",
      "       [-2.5212448 ,  3.3948255 ],\n",
      "       [-0.4363703 ,  1.2140146 ],\n",
      "       [-2.8867278 ,  3.616543  ],\n",
      "       [ 2.5096037 , -3.2620327 ],\n",
      "       [-2.028684  ,  2.9643822 ],\n",
      "       [-2.6190634 ,  3.38825   ],\n",
      "       [-2.4844215 ,  3.4354086 ],\n",
      "       [ 2.7426345 , -3.4993336 ],\n",
      "       [-2.3710263 ,  3.3222313 ],\n",
      "       [-2.4477115 ,  3.3432167 ],\n",
      "       [-3.0282607 ,  3.6569507 ],\n",
      "       [-2.926302  ,  3.6389468 ],\n",
      "       [-2.849435  ,  3.5849426 ],\n",
      "       [-2.869796  ,  3.597243  ],\n",
      "       [-2.8969297 ,  3.6506898 ],\n",
      "       [-2.7911963 ,  3.57682   ],\n",
      "       [-2.8333006 ,  3.616253  ],\n",
      "       [ 2.0365732 , -2.6888444 ],\n",
      "       [ 1.7149551 , -2.707507  ],\n",
      "       [ 0.6782801 , -0.17214909],\n",
      "       [-0.20528667,  0.573386  ],\n",
      "       [ 2.632167  , -3.4870763 ],\n",
      "       [-2.884338  ,  3.6157296 ],\n",
      "       [-2.9126453 ,  3.6427786 ],\n",
      "       [-1.4872549 ,  2.3596714 ],\n",
      "       [ 0.7544136 , -1.073291  ],\n",
      "       [-2.7796302 ,  3.5801609 ],\n",
      "       [-2.5931559 ,  3.4109325 ],\n",
      "       [-2.7713442 ,  3.5811062 ],\n",
      "       [-2.7392328 ,  3.5794852 ],\n",
      "       [-2.743102  ,  3.5888245 ],\n",
      "       [-1.2852961 ,  2.1927748 ],\n",
      "       [-2.4789104 ,  3.3818507 ],\n",
      "       [-1.5356225 ,  2.4693143 ],\n",
      "       [ 2.0708091 , -2.8480995 ],\n",
      "       [-2.9600787 ,  3.6541462 ],\n",
      "       [ 2.504472  , -3.4405339 ],\n",
      "       [-3.0087056 ,  3.6507115 ],\n",
      "       [-2.996585  ,  3.6631942 ],\n",
      "       [-3.0337975 ,  3.6594553 ],\n",
      "       [-2.8138237 ,  3.617656  ],\n",
      "       [-2.779073  ,  3.5574203 ],\n",
      "       [-2.5720623 ,  3.4818485 ],\n",
      "       [-2.2247858 ,  3.2206757 ],\n",
      "       [-1.2263991 ,  2.1450064 ],\n",
      "       [ 2.7337239 , -3.4868555 ],\n",
      "       [-2.2180007 ,  3.174197  ],\n",
      "       [-2.2439258 ,  3.1441102 ],\n",
      "       [-2.0905206 ,  2.925664  ],\n",
      "       [ 2.6298728 , -3.4046195 ],\n",
      "       [ 0.6120589 , -0.73751056],\n",
      "       [-3.0122952 ,  3.6804488 ],\n",
      "       [-2.9593747 ,  3.6818607 ],\n",
      "       [-2.2413538 ,  3.2777827 ],\n",
      "       [-2.9921856 ,  3.671329  ],\n",
      "       [-1.7865599 ,  2.7138195 ],\n",
      "       [ 0.18278776,  0.38172734],\n",
      "       [ 2.1892943 , -3.2642822 ],\n",
      "       [-3.004495  ,  3.6918223 ],\n",
      "       [-2.3458033 ,  3.2465503 ],\n",
      "       [-2.846455  ,  3.6502986 ],\n",
      "       [ 2.4229589 , -3.4061084 ],\n",
      "       [ 2.0059931 , -3.050699  ],\n",
      "       [-2.5945723 ,  3.4658208 ],\n",
      "       [-3.014423  ,  3.6725345 ],\n",
      "       [-1.5789881 ,  2.432701  ],\n",
      "       [-2.985943  ,  3.680492  ],\n",
      "       [-2.816451  ,  3.593765  ],\n",
      "       [-2.7055748 ,  3.5261474 ],\n",
      "       [ 1.3714635 , -2.0706785 ],\n",
      "       [-2.9658344 ,  3.6522777 ],\n",
      "       [-2.9899852 ,  3.6305313 ],\n",
      "       [ 2.5908723 , -3.343211  ],\n",
      "       [-2.973735  ,  3.6799748 ],\n",
      "       [ 2.1390388 , -2.8254664 ],\n",
      "       [-2.6732855 ,  3.5400338 ],\n",
      "       [-0.25315386,  0.9434422 ],\n",
      "       [-2.8511252 ,  3.6297348 ],\n",
      "       [ 2.113717  , -2.8635476 ],\n",
      "       [ 2.1535676 , -3.136684  ],\n",
      "       [-3.0467808 ,  3.6467896 ],\n",
      "       [-1.7043798 ,  2.6484625 ],\n",
      "       [-2.4982722 ,  3.3460877 ],\n",
      "       [-3.0085757 ,  3.6617715 ],\n",
      "       [ 2.470183  , -3.4146972 ],\n",
      "       [ 2.3343444 , -3.1966336 ],\n",
      "       [ 2.6710932 , -3.4660504 ],\n",
      "       [ 2.6462588 , -3.3709795 ],\n",
      "       [ 2.627886  , -3.4577842 ],\n",
      "       [-2.307893  ,  3.2370763 ],\n",
      "       [ 2.12677   , -2.9741504 ],\n",
      "       [-3.041655  ,  3.6569014 ],\n",
      "       [-2.5893142 ,  3.4616184 ],\n",
      "       [-3.0085175 ,  3.6623187 ],\n",
      "       [-2.990167  ,  3.640546  ],\n",
      "       [-2.490756  ,  3.3314066 ],\n",
      "       [-3.0153797 ,  3.6663632 ],\n",
      "       [-3.0203001 ,  3.6333551 ],\n",
      "       [-2.5597553 ,  3.4052255 ],\n",
      "       [-2.7592692 ,  3.567223  ],\n",
      "       [-3.0053117 ,  3.667718  ],\n",
      "       [-2.9266825 ,  3.6406581 ],\n",
      "       [-3.0008135 ,  3.6560624 ],\n",
      "       [-3.0043046 ,  3.6506732 ],\n",
      "       [ 1.2987201 , -1.9432038 ],\n",
      "       [-2.5808716 ,  3.381163  ],\n",
      "       [-2.9094133 ,  3.6464827 ],\n",
      "       [-2.8276072 ,  3.6376257 ],\n",
      "       [ 2.5249233 , -3.216827  ],\n",
      "       [-0.99468565,  1.9180113 ],\n",
      "       [-2.95218   ,  3.6590257 ],\n",
      "       [-3.0164459 ,  3.653878  ],\n",
      "       [-3.0385137 ,  3.6738877 ],\n",
      "       [-3.0101871 ,  3.678194  ],\n",
      "       [-2.5418558 ,  3.4475746 ],\n",
      "       [-2.7558126 ,  3.5800397 ],\n",
      "       [ 2.3289886 , -3.3117678 ],\n",
      "       [-2.8788273 ,  3.6413994 ],\n",
      "       [-2.3655143 ,  3.290958  ],\n",
      "       [-2.733358  ,  3.5710258 ],\n",
      "       [-2.3547952 ,  3.2405136 ],\n",
      "       [-1.7726265 ,  2.7553341 ],\n",
      "       [-2.9342647 ,  3.585378  ],\n",
      "       [-2.4583185 ,  3.2831888 ],\n",
      "       [-2.8100255 ,  3.6037922 ],\n",
      "       [-2.7172756 ,  3.55783   ],\n",
      "       [-1.3397838 ,  2.0240152 ],\n",
      "       [-2.7200923 ,  3.5524087 ],\n",
      "       [-2.995906  ,  3.678627  ],\n",
      "       [ 0.89635617, -0.39093658],\n",
      "       [-1.7967836 ,  2.453554  ],\n",
      "       [-3.0090013 ,  3.6630118 ],\n",
      "       [-2.6949005 ,  3.527667  ],\n",
      "       [-2.3350022 ,  3.2146363 ],\n",
      "       [ 2.533051  , -3.1985345 ],\n",
      "       [ 2.1934412 , -3.1021109 ],\n",
      "       [-2.3748252 ,  3.2683978 ],\n",
      "       [-2.9643862 ,  3.6756015 ],\n",
      "       [-2.7608144 ,  3.5474222 ],\n",
      "       [-2.4091518 ,  3.2741566 ],\n",
      "       [-1.7532026 ,  2.6087573 ],\n",
      "       [ 2.1849241 , -2.853856  ],\n",
      "       [ 0.2696586 ,  0.11372518],\n",
      "       [ 2.6533606 , -3.4534256 ],\n",
      "       [-2.8291752 ,  3.5874    ],\n",
      "       [-3.0271783 ,  3.6690986 ],\n",
      "       [-1.9263504 ,  2.9013228 ],\n",
      "       [-2.9979787 ,  3.6926286 ],\n",
      "       [ 2.30495   , -3.011009  ],\n",
      "       [-3.0061936 ,  3.6752412 ],\n",
      "       [-2.8511908 ,  3.5955884 ],\n",
      "       [ 2.3474855 , -3.16307   ],\n",
      "       [-2.3344429 ,  3.3568919 ],\n",
      "       [-1.8335112 ,  2.7655394 ],\n",
      "       [-2.9995854 ,  3.6759179 ],\n",
      "       [ 2.4894288 , -3.1925862 ],\n",
      "       [-2.9959338 ,  3.6773937 ],\n",
      "       [-2.1532052 ,  3.085735  ],\n",
      "       [-2.907979  ,  3.6681387 ],\n",
      "       [-2.9464839 ,  3.6533055 ],\n",
      "       [-2.8751583 ,  3.6464033 ],\n",
      "       [-2.7013273 ,  3.5154176 ],\n",
      "       [-3.0430686 ,  3.652527  ],\n",
      "       [ 1.8295661 , -2.2716358 ],\n",
      "       [-2.4378753 ,  3.3517864 ],\n",
      "       [-2.9939148 ,  3.6376693 ],\n",
      "       [ 2.1705384 , -3.1568606 ],\n",
      "       [-2.9038014 ,  3.631376  ],\n",
      "       [-2.6427984 ,  3.547215  ],\n",
      "       [ 2.6065457 , -3.4680402 ],\n",
      "       [-0.5589398 ,  1.3887074 ],\n",
      "       [-3.0410233 ,  3.6595337 ],\n",
      "       [ 1.70057   , -2.3784435 ],\n",
      "       [-2.5737956 ,  3.4057214 ]], dtype=float32), label_ids=array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1], dtype=int64), metrics={'test_loss': 0.6822231411933899, 'test_runtime': 4.3208, 'test_samples_per_second': 94.428, 'test_steps_per_second': 11.803}),\n",
      " 'predictions.label_ids.shape': (408,),\n",
      " 'predictions.predictions.shape': (408, 2)}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint({\n",
    "    \"predictions\": predictions,\n",
    "    \"predictions.predictions.shape\": predictions.predictions.shape,\n",
    "    \"predictions.label_ids.shape\": predictions.label_ids.shape\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1], dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "preds = numpy.argmax(predictions.predictions, axis=-1)\n",
    "pprint.pprint(\n",
    "    preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8602941176470589, 'f1': 0.9038785834738617}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型性能\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(\n",
    "    predictions=preds,\n",
    "    references=predictions.label_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 给Trainer集成metric评估函数\n",
    "\n",
    "raw_datasets = datasets.load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    function=tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    \"test-trainer\", \n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=2\n",
    ")\n",
    "model.to(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "temp = 1\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metirc: evaluate.module.EvaluationModule = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    temp = eval_preds\n",
    "    pprint.pprint({\n",
    "        \"eval_preds\": eval_preds\n",
    "    })\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §3.3 完整的微调步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.加载原始str数据集raw_datasets\n",
    "# 2.创建Tokenizer及其DataCollatorWithPadding\n",
    "# 3.使用raw_datasets.map(lambda)批量处理原始数据集，得到Tokenized数据集tokenized_datasets\n",
    "# 4.创建Bert二分类model\n",
    "\n",
    "raw_datasets: datasets.DatasetDict = datasets.load_dataset(\"glue\", \"mrpc\")\n",
    "# raw_datasets: DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#         num_rows: 3668\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#         num_rows: 408\n",
    "#     })\n",
    "#     test: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#         num_rows: 1725\n",
    "#     })\n",
    "# })\n",
    "# raw_datasets.__getitem__(__index: int) -> dict[\n",
    "#     {\n",
    "#         'sentence1': str,\n",
    "#         'sentence2': str,\n",
    "#         'label': int,\n",
    "#         'idx': __index\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "checkpoint: str = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer: transformers.BertTokenizerFast = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "# tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
    "# \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# }\n",
    "# tokenizer(\n",
    "#     text: str | list[str],\n",
    "#     text_pair: str | list[str]\n",
    "# ) -> dict[\n",
    "#     {\n",
    "#         'input_ids': list[int] | list[list[int]], \n",
    "#         'token_type_ids': list[int] | list[list[int]], \n",
    "#         'attention_mask': list[int] | list[list[int]]\n",
    "#     }\n",
    "# ]\n",
    "# 当tokenizer.__call__()中的函数形参text和text_pair同时使用时，视为一段序列的两个部分\n",
    "# 例如tokenizer(\"hello\", \"hello\")的生成的input_ids经解码后为'[CLS] hello [SEP] hello [SEP]'\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# data:collator: DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
    "# \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "# }, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n",
    "\n",
    "def tokenize_function(example: dict) -> dict:\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "# tokenizer_function(\n",
    "#     example: dict 即 type(raw_datasets.__getitem__())\n",
    "# ) -> dict[{\n",
    "#     'input_ids': list[int] | list[list[int]], \n",
    "#     'token_type_ids': list[int] | list[list][int], # 前半部分为0，后半部分为1\n",
    "#     'attention_mask': list[int] | list[list][int] # 全1\n",
    "# }]\n",
    "\n",
    "tokenized_datasets: datasets.DatasetDict = raw_datasets.map(tokenize_function, batched=True)\n",
    "# tokenized_datasets: DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#         num_rows: 3668\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#         num_rows: 408\n",
    "#     })\n",
    "#     test: Dataset({\n",
    "#         features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#         num_rows: 1725\n",
    "#     })\n",
    "# })\n",
    "# tokenized_datasets.__getitem__(str): Dataset({\n",
    "#     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#     num_rows: int\n",
    "# })\n",
    "\n",
    "model: transformers.BertForSequenceClassification = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels = 2\n",
    ")\n",
    "model.to(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "# BertForSequenceClassification(\n",
    "#   (bert): BertModel(\n",
    "#     (embeddings): BertEmbeddings(\n",
    "#       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "#       (position_embeddings): Embedding(512, 768)\n",
    "#       (token_type_embeddings): Embedding(2, 768)\n",
    "#       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#       (dropout): Dropout(p=0.1, inplace=False)\n",
    "#     )\n",
    "#     (encoder): BertEncoder(\n",
    "#       (layer): ModuleList(\n",
    "#         (0-11): 12 x BertLayer(\n",
    "#           (attention): BertAttention(\n",
    "#             (self): BertSdpaSelfAttention(\n",
    "#               (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             )\n",
    "#             (output): BertSelfOutput(\n",
    "#               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#               (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             )\n",
    "#           )\n",
    "#           (intermediate): BertIntermediate(\n",
    "#             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#             (intermediate_act_fn): GELUActivation()\n",
    "#           )\n",
    "#           (output): BertOutput(\n",
    "#             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#           )\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (pooler): BertPooler(\n",
    "#       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "#       (activation): Tanh()\n",
    "#     )\n",
    "#   )\n",
    "#   (dropout): Dropout(p=0.1, inplace=False)\n",
    "#   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 5.删除tokenized_datasets中不必要的字段\n",
    "tokenized_datasets: datasets.DatasetDict = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets: datasets.DatasetDict = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "pprint.pprint(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# 6.将tokenized_datasets中的list转换为torch.Tensor\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "pprint.pprint(tokenized_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'attention_mask': torch.Size([16, 82]),\n",
      "  'input_ids': torch.Size([16, 82]),\n",
      "  'labels': torch.Size([16]),\n",
      "  'token_type_ids': torch.Size([16, 82])},\n",
      " {'attention_mask': torch.Size([16, 100]),\n",
      "  'input_ids': torch.Size([16, 100]),\n",
      "  'labels': torch.Size([16]),\n",
      "  'token_type_ids': torch.Size([16, 100])},\n",
      " {'attention_mask': torch.Size([16, 82]),\n",
      "  'input_ids': torch.Size([16, 82]),\n",
      "  'labels': torch.Size([16]),\n",
      "  'token_type_ids': torch.Size([16, 82])}]\n",
      "{'num_training_steps': 690}\n"
     ]
    }
   ],
   "source": [
    "# 7.创建torch.utils.data.DataLoader\n",
    "# 8.创建torch.optim.adamw.AdamW\n",
    "# 9.创建transformers.get_scheduler，使得学习率从5e-5向0线性递减\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "pprint.pprint(\n",
    "    [{k: v.shape for k, v in batch.items()} for batch in train_dataloader][:3]\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,\n",
    ")\n",
    "\n",
    "num_epoches = 3\n",
    "num_training_steps = num_epoches * len(train_dataloader)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "pprint.pprint({\n",
    "    \"num_training_steps\": num_training_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2632894e995649c79d5d47e3f189ba20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# 10.开始训练\n",
    "\n",
    "model.train() # 让model从评估模式转为训练模式，而不是像Trainer.train()那样直接开练\n",
    "\n",
    "progress_bar = tqdm.auto.tqdm(range(num_training_steps))\n",
    "device: torch.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "for epoch in range(num_epoches):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8553921568627451, 'f1': 0.8991452991452992}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11.对模型进行评估\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Accelerator包装torch.device\n",
    "```diff\n",
    "+ from accelerate import Accelerator\n",
    "  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "+ accelerator = Accelerator()\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "- model.to(device)\n",
    "\n",
    "+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "+     train_dataloader, eval_dataloader, model, optimizer\n",
    "+ )\n",
    "\n",
    "  num_epochs = 3\n",
    "  num_training_steps = num_epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "      for batch in train_dataloader:\n",
    "-         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "-         loss.backward()\n",
    "+         accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
