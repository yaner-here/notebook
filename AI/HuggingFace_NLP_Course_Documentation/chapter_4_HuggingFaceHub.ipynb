{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pformat() missing 1 required positional argument: 'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mpprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: pformat() missing 1 required positional argument: 'object'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §4.1 `Auto*`类\n",
    "\n",
    "对于特定的`checkpoint: str`，我们有以下三种使用的方法：\n",
    "\n",
    "1. 使用`transformers.pipeline`\n",
    "2. 使用`transformers`的特定`<MODEL>Tokenizer`/`<MODEL>For<TASK>`\n",
    "3. 使用`transformers`的通用`AutoTokenizer`/`AutoModelFor<TASK>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.13117577135562897,\n",
      "  'sequence': 'The capital of France is the city.',\n",
      "  'token': 23151,\n",
      "  'token_str': 'city'},\n",
      " {'score': 0.09899164736270905,\n",
      "  'sequence': 'The capital of France is the City.',\n",
      "  'token': 6383,\n",
      "  'token_str': 'City'},\n",
      " {'score': 0.04552188888192177,\n",
      "  'sequence': 'The capital of France is the French.',\n",
      "  'token': 11098,\n",
      "  'token_str': 'French'},\n",
      " {'score': 0.038993123918771744,\n",
      "  'sequence': 'The capital of France is the London.',\n",
      "  'token': 15970,\n",
      "  'token_str': 'London'},\n",
      " {'score': 0.026898596435785294,\n",
      "  'sequence': 'The capital of France is the world.',\n",
      "  'token': 18909,\n",
      "  'token_str': 'world'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'city'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'city'\n"
     ]
    }
   ],
   "source": [
    "checkpoint: str = \"camembert-base\"\n",
    "raw_data = \"The capital of France is the <mask>.\"\n",
    "\n",
    "# 方法1\n",
    "pipeline = transformers.pipeline(\"fill-mask\", checkpoint)\n",
    "pprint.pprint(pipeline(raw_data))\n",
    "\n",
    "# 方法2\n",
    "tokenizer = transformers.CamembertTokenizer.from_pretrained(checkpoint)\n",
    "model = transformers.CamembertForMaskedLM.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(raw_data, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero()[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "pprint.pprint(tokenizer.decode(predicted_token_id))\n",
    "\n",
    "# 方法3\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(raw_data, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero()[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "pprint.pprint(tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs.input_ids == tokenizer.mask_token_id)[0].nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §4.2 上传HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50261be36ff4a268718061adf6497b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "training_args = transformers.TrainingArguments(\n",
    "    \"bert-finetuned-mrpc\", \n",
    "    save_strategy=\"epoch\", \n",
    "    push_to_hub=True, # 开启上传开关，每个Trainer的Epoch就上传一次\n",
    "    hub_model_id=\"组织名称/仓库名称\"\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model\n",
    ")\n",
    "# trainer.train()\n",
    "# trainer.push_to_hub() # 最后上传一次\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcec6da457d4ed6a326d40a1e9da398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ed09cd10304ba49062905ff2977a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d147e35f2094cba9c06998d649cfd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 执行之后访问huggingface.co/<USERNAME>/<REPONAME>\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# tokenizer/model.push_to_hub(\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     repo_id: str = \"...\",\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     organization: typing.Optional[str] = \"...\",\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#     use_auth_key: typing.Optional[str] = \"<TOKEN>\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\modeling_utils.py:2844\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[0;32m   2843\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[1;32m-> 2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\utils\\hub.py:935\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# Update model card if needed:\u001b[39;00m\n\u001b[0;32m    933\u001b[0m model_card\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(work_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 935\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_modified_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\utils\\hub.py:799\u001b[0m, in \u001b[0;36mPushToHubMixin._upload_modified_files\u001b[1;34m(self, working_dir, repo_id, files_timestamps, commit_message, token, create_pr, revision, commit_description)\u001b[0m\n\u001b[0;32m    796\u001b[0m     create_branch(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, branch\u001b[38;5;241m=\u001b[39mrevision, token\u001b[38;5;241m=\u001b[39mtoken, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    798\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading the following files to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(modified_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1398\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3770\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[1;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[0;32m   3767\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[0;32m   3768\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[1;32m-> 3770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3772\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[0;32m   3776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[0;32m   3779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3781\u001b[0m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n\u001b[0;32m   3782\u001b[0m operations_without_no_op \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4317\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[1;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[0;32m   4311\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   4312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped upload for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions_to_upload)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS file(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ignored by gitignore file).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4314\u001b[0m     )\n\u001b[0;32m   4316\u001b[0m \u001b[38;5;66;03m# Upload new LFS files\u001b[39;00m\n\u001b[1;32m-> 4317\u001b[0m \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4318\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lfs_additions_to_upload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4324\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If `create_pr`, we don't want to check user permission on the revision as users with read permission\u001b[39;49;00m\n\u001b[0;32m   4325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# should still be able to create PRs even if they don't have write permission on the target branch of the\u001b[39;49;00m\n\u001b[0;32m   4326\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# PR (i.e. `revision`).\u001b[39;49;00m\n\u001b[0;32m   4327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[0;32m   4330\u001b[0m     addition\u001b[38;5;241m.\u001b[39m_is_uploaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:441\u001b[0m, in \u001b[0;36m_upload_lfs_files\u001b[1;34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_actions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    440\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading 1 LFS file to the Hub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43m_wrapped_lfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS files to the Hub using up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m threads concurrently\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:431\u001b[0m, in \u001b[0;36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[1;34m(batch_action)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     operation \u001b[38;5;241m=\u001b[39m oid2addop[batch_action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m--> 431\u001b[0m     \u001b[43mlfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlfs_batch_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while uploading \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;241m.\u001b[39mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\lfs.py:246\u001b[0m, in \u001b[0;36mlfs_upload\u001b[1;34m(operation, lfs_batch_action, token, headers, endpoint)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43m_upload_multi_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     _upload_single_part(operation\u001b[38;5;241m=\u001b[39moperation, upload_url\u001b[38;5;241m=\u001b[39mupload_url)\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\lfs.py:346\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[1;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[0;32m    337\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m upload\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m     )\n\u001b[0;32m    341\u001b[0m     use_hf_transfer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    343\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    344\u001b[0m     _upload_parts_hf_transfer(operation\u001b[38;5;241m=\u001b[39moperation, sorted_parts_urls\u001b[38;5;241m=\u001b[39msorted_parts_urls, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_hf_transfer\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_upload_parts_iteratively\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# 3. Send completion request\u001b[39;00m\n\u001b[0;32m    350\u001b[0m completion_res \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    351\u001b[0m     upload_url,\n\u001b[0;32m    352\u001b[0m     json\u001b[38;5;241m=\u001b[39m_get_completion_payload(response_headers, operation\u001b[38;5;241m.\u001b[39mupload_info\u001b[38;5;241m.\u001b[39msha256\u001b[38;5;241m.\u001b[39mhex()),\n\u001b[0;32m    353\u001b[0m     headers\u001b[38;5;241m=\u001b[39mLFS_HEADERS,\n\u001b[0;32m    354\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\lfs.py:403\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[1;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_idx, part_upload_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_parts_urls):\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SliceFileObj(\n\u001b[0;32m    398\u001b[0m         fileobj,\n\u001b[0;32m    399\u001b[0m         seek_from\u001b[38;5;241m=\u001b[39mchunk_size \u001b[38;5;241m*\u001b[39m part_idx,\n\u001b[0;32m    400\u001b[0m         read_limit\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[0;32m    401\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m fileobj_slice:\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;66;03m# S3 might raise a transient 500 error -> let's retry if that happens\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m         part_upload_res \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_upload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m         hf_raise_for_status(part_upload_res)\n\u001b[0;32m    407\u001b[0m         headers\u001b[38;5;241m.\u001b[39mappend(part_upload_res\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:280\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 上传演示\n",
    "\n",
    "checkpoint = \"camembert-base\"\n",
    "\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 执行之后访问huggingface.co/<USERNAME>/<REPONAME>\n",
    "model.push_to_hub(\"test-model\")\n",
    "tokenizer.push_to_hub(\"test-tokenizer\") \n",
    "# tokenizer/model.push_to_hub(\n",
    "#     repo_id: str = \"...\",\n",
    "#     organization: typing.Optional[str] = \"...\",\n",
    "#     use_auth_key: typing.Optional[str] = \"<TOKEN>\"\n",
    "# )\n",
    "\n",
    "model.save_pretrained(\"本地路径\")\n",
    "tokenizer.save_pretrained(\"本地路径\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auth': {'accessToken': {'createdAt': '2024-09-06T09:09:44.796Z',\n",
      "                          'displayName': 'VSCode',\n",
      "                          'fineGrained': {'canReadGatedRepos': True,\n",
      "                                          'global': ['inference.serverless.write',\n",
      "                                                     'discussion.write',\n",
      "                                                     'post.write'],\n",
      "                                          'scoped': [{'entity': {'_id': '63fe1ef80c1bbe8e29d40edc',\n",
      "                                                                 'name': 'NoComment',\n",
      "                                                                 'type': 'user'},\n",
      "                                                      'permissions': ['user.webhooks.read',\n",
      "                                                                      'repo.content.read',\n",
      "                                                                      'repo.write',\n",
      "                                                                      'inference.endpoints.infer.write',\n",
      "                                                                      'inference.endpoints.write',\n",
      "                                                                      'user.webhooks.write',\n",
      "                                                                      'collection.read',\n",
      "                                                                      'collection.write',\n",
      "                                                                      'discussion.write',\n",
      "                                                                      'user.billing.read']}]},\n",
      "                          'role': 'fineGrained'},\n",
      "          'type': 'access_token'},\n",
      " 'avatarUrl': '/avatars/1bc89aacde7bf0e45473dbfad9322a2e.svg',\n",
      " 'canPay': False,\n",
      " 'fullname': 'Yaner',\n",
      " 'id': '63fe1ef80c1bbe8e29d40edc',\n",
      " 'isPro': False,\n",
      " 'name': 'NoComment',\n",
      " 'orgs': [],\n",
      " 'periodEnd': None,\n",
      " 'type': 'user'}\n",
      "<generator object HfApi.list_models at 0x000001F380975FC0>\n",
      "<generator object HfApi.list_datasets at 0x000001F382105300>\n",
      "[MetricInfo(id='accuracy',\n",
      "            space_id='evaluate-metric/accuracy',\n",
      "            description='Accuracy is the proportion of correct predictions '\n",
      "                        'among the total number of cases processed. It can be '\n",
      "                        'computed with: Accuracy = (TP + TN) / (TP + TN + FP + '\n",
      "                        'FN) Where: TP: True positive TN: True negative FP: '\n",
      "                        'False positive FN: False negative'),\n",
      " MetricInfo(id='bertscore',\n",
      "            space_id='evaluate-metric/bertscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='bleu',\n",
      "            space_id='evaluate-metric/bleu',\n",
      "            description='BLEU (Bilingual Evaluation Understudy) is an '\n",
      "                        'algorithm for evaluating the quality of text which '\n",
      "                        'has been machine-translated from one natural language '\n",
      "                        'to another. Quality is considered to be the '\n",
      "                        \"correspondence between a machine's output and that of \"\n",
      "                        'a human: \"the closer a machine translation is to a '\n",
      "                        'professional human translation, the better it is\" – '\n",
      "                        'this is the central idea behind BLEU. BLEU was one of '\n",
      "                        'the first metrics to claim a high correlation with '\n",
      "                        'human judgements of quality, and remains one of the '\n",
      "                        'most popular automated and inexpensive metrics.\\n'\n",
      "                        'Scores are calculated for individual translated '\n",
      "                        'segments—generally sentences—by comparing them with a '\n",
      "                        'set of good quality reference translations. Those '\n",
      "                        'scores are then averaged over the whole corpus to '\n",
      "                        \"reach an estimate of the translation's overall \"\n",
      "                        'quality. Neither intelligibility nor grammatical '\n",
      "                        'correctness are not taken into account.'),\n",
      " MetricInfo(id='bleurt',\n",
      "            space_id='evaluate-metric/bleurt',\n",
      "            description='BLEURT a learnt evaluation metric for Natural '\n",
      "                        'Language Generation. It is built using multiple '\n",
      "                        'phases of transfer learning starting from a '\n",
      "                        'pretrained BERT model (Devlin et al. 2018) and then '\n",
      "                        'employing another pre-training phrase using synthetic '\n",
      "                        'data. Finally it is trained on WMT human annotations. '\n",
      "                        'You may run BLEURT out-of-the-box or fine-tune it for '\n",
      "                        'your specific application (the latter is expected to '\n",
      "                        'perform better).\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/google-research/bleurt#readme for '\n",
      "                        'more information.'),\n",
      " MetricInfo(id='brier_score',\n",
      "            space_id='evaluate-metric/brier_score',\n",
      "            description='The Brier score is a measure of the error between two '\n",
      "                        'probability distributions.'),\n",
      " MetricInfo(id='cer',\n",
      "            space_id='evaluate-metric/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='character',\n",
      "            space_id='evaluate-metric/character',\n",
      "            description='CharacTer is a character-level metric inspired by the '\n",
      "                        'commonly applied translation edit rate (TER).'),\n",
      " MetricInfo(id='charcut_mt',\n",
      "            space_id='evaluate-metric/charcut_mt',\n",
      "            description='CharCut is a character-based machine translation '\n",
      "                        'evaluation metric.'),\n",
      " MetricInfo(id='chrf',\n",
      "            space_id='evaluate-metric/chrf',\n",
      "            description='ChrF and ChrF++ are two MT evaluation metrics. They '\n",
      "                        'both use the F-score statistic for character n-gram '\n",
      "                        'matches, and ChrF++ adds word n-grams as well which '\n",
      "                        'correlates more strongly with direct assessment. We '\n",
      "                        'use the implementation that is already present in '\n",
      "                        'sacrebleu.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#chrf--chrf for '\n",
      "                        'more information.'),\n",
      " MetricInfo(id='code_eval',\n",
      "            space_id='evaluate-metric/code_eval',\n",
      "            description='This metric implements the evaluation harness for the '\n",
      "                        'HumanEval problem solving dataset described in the '\n",
      "                        'paper \"Evaluating Large Language Models Trained on '\n",
      "                        'Code\" (https://arxiv.org/abs/2107.03374).'),\n",
      " MetricInfo(id='comet',\n",
      "            space_id='evaluate-metric/comet',\n",
      "            description='Crosslingual Optimized Metric for Evaluation of '\n",
      "                        'Translation (COMET) is an open-source framework used '\n",
      "                        'to train Machine Translation metrics that achieve '\n",
      "                        'high levels of correlation with different types of '\n",
      "                        \"human judgments (HTER, DA's or MQM). With the release \"\n",
      "                        'of the framework the authors also released fully '\n",
      "                        'trained models that were used to compete in the WMT20 '\n",
      "                        'Metrics Shared Task achieving SOTA in that years '\n",
      "                        'competition.\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://unbabel.github.io/COMET/html/models.html for '\n",
      "                        'more information.'),\n",
      " MetricInfo(id='competition_math',\n",
      "            space_id='evaluate-metric/competition_math',\n",
      "            description='This metric is used to assess performance on the '\n",
      "                        'Mathematics Aptitude Test of Heuristics (MATH) '\n",
      "                        'dataset. It first canonicalizes the inputs (e.g., '\n",
      "                        'converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes '\n",
      "                        'accuracy.'),\n",
      " MetricInfo(id='confusion_matrix',\n",
      "            space_id='evaluate-metric/confusion_matrix',\n",
      "            description='The confusion matrix evaluates classification '\n",
      "                        'accuracy. \\n'\n",
      "                        'Each row in a confusion matrix represents a true '\n",
      "                        'class and each column represents the instances in a '\n",
      "                        'predicted class.'),\n",
      " MetricInfo(id='coval',\n",
      "            space_id='evaluate-metric/coval',\n",
      "            description='CoVal is a coreference evaluation tool for the CoNLL '\n",
      "                        'and ARRAU datasets which implements of the common '\n",
      "                        'evaluation metrics including MUC [Vilain et al, '\n",
      "                        '1995], B-cubed [Bagga and Baldwin, 1998], CEAFe [Luo '\n",
      "                        'et al., 2005], LEA [Moosavi and Strube, 2016] and the '\n",
      "                        'averaged CoNLL score (the average of the F1 values of '\n",
      "                        'MUC, B-cubed and CEAFe) [Denis and Baldridge, 2009a; '\n",
      "                        'Pradhan et al., 2011].\\n'\n",
      "                        'This wrapper of CoVal currently only work with CoNLL '\n",
      "                        'line format: The CoNLL format has one word per line '\n",
      "                        'with all the annotation for this word in column '\n",
      "                        'separated by spaces: Column\\tType\\tDescription 1\\t'\n",
      "                        'Document ID\\tThis is a variation on the document '\n",
      "                        'filename 2\\tPart number\\tSome files are divided into '\n",
      "                        'multiple parts numbered as 000, 001, 002, ... etc. 3\\t'\n",
      "                        'Word number 4\\tWord itself\\tThis is the token as '\n",
      "                        'segmented/tokenized in the Treebank. Initially the '\n",
      "                        '*_skel file contain the placeholder [WORD] which gets '\n",
      "                        'replaced by the actual token from the Treebank which '\n",
      "                        'is part of the OntoNotes release. 5\\tPart-of-Speech '\n",
      "                        '6\\tParse bit\\tThis is the bracketed structure broken '\n",
      "                        'before the first open parenthesis in the parse, and '\n",
      "                        'the word/part-of-speech leaf replaced with a *. The '\n",
      "                        'full parse can be created by substituting the asterix '\n",
      "                        'with the \"([pos] [word])\" string (or leaf) and '\n",
      "                        'concatenating the items in the rows of that column. '\n",
      "                        '7\\tPredicate lemma\\tThe predicate lemma is mentioned '\n",
      "                        'for the rows for which we have semantic role '\n",
      "                        'information. All other rows are marked with a \"-\" 8\\t'\n",
      "                        'Predicate Frameset ID\\tThis is the PropBank frameset '\n",
      "                        'ID of the predicate in Column 7. 9\\tWord sense\\tThis '\n",
      "                        'is the word sense of the word in Column 3. 10\\t'\n",
      "                        'Speaker/Author\\tThis is the speaker or author name '\n",
      "                        'where available. Mostly in Broadcast Conversation and '\n",
      "                        'Web Log data. 11\\tNamed Entities\\tThese columns '\n",
      "                        'identifies the spans representing various named '\n",
      "                        'entities. 12:N\\tPredicate Arguments\\tThere is one '\n",
      "                        'column each of predicate argument structure '\n",
      "                        'information for the predicate mentioned in Column 7. '\n",
      "                        'N\\tCoreference\\tCoreference chain information encoded '\n",
      "                        'in a parenthesis structure. More informations on the '\n",
      "                        'format can be found here (section \"*_conll File '\n",
      "                        'Format\"): '\n",
      "                        'http://www.conll.cemantix.org/2012/data.html\\n'\n",
      "                        'Details on the evaluation on CoNLL can be found here: '\n",
      "                        'https://github.com/ns-moosavi/coval/blob/master/conll/README.md\\n'\n",
      "                        'CoVal code was written by @ns-moosavi. Some parts are '\n",
      "                        'borrowed from '\n",
      "                        'https://github.com/clarkkev/deep-coref/blob/master/evaluation.py '\n",
      "                        'The test suite is taken from '\n",
      "                        'https://github.com/conll/reference-coreference-scorers/ '\n",
      "                        'Mention evaluation and the test suite are added by '\n",
      "                        '@andreasvc. Parsing CoNLL files is developed by Leo '\n",
      "                        'Born.'),\n",
      " MetricInfo(id='cuad',\n",
      "            space_id='evaluate-metric/cuad',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 1 of the Contract Understanding Atticus '\n",
      "                        'Dataset (CUAD).\\n'\n",
      "                        'Contract Understanding Atticus Dataset (CUAD) v1 is a '\n",
      "                        'corpus of more than 13,000 labels in 510  commercial '\n",
      "                        'legal contracts that have been manually labeled to '\n",
      "                        'identify 41 categories of important clauses that '\n",
      "                        'lawyers look for when reviewing contracts in '\n",
      "                        'connection with corporate transactions.'),\n",
      " MetricInfo(id='exact_match',\n",
      "            space_id='evaluate-metric/exact_match',\n",
      "            description='Returns the rate at which the input predicted strings '\n",
      "                        'exactly match their references, ignoring any strings '\n",
      "                        'input as part of the regexes_to_ignore list.'),\n",
      " MetricInfo(id='f1',\n",
      "            space_id='evaluate-metric/f1',\n",
      "            description='The F1 score is the harmonic mean of the precision '\n",
      "                        'and recall. It can be computed with the equation: F1 '\n",
      "                        '= 2 * (precision * recall) / (precision + recall)'),\n",
      " MetricInfo(id='frugalscore',\n",
      "            space_id='evaluate-metric/frugalscore',\n",
      "            description='FrugalScore is a reference-based metric for NLG '\n",
      "                        'models evaluation. It is based on a distillation '\n",
      "                        'approach that allows to learn a fixed, low cost '\n",
      "                        'version of any expensive NLG metric, while retaining '\n",
      "                        'most of its original performance.'),\n",
      " MetricInfo(id='glue',\n",
      "            space_id='evaluate-metric/glue',\n",
      "            description='GLUE, the General Language Understanding Evaluation '\n",
      "                        'benchmark (https://gluebenchmark.com/) is a '\n",
      "                        'collection of resources for training, evaluating, and '\n",
      "                        'analyzing natural language understanding systems.'),\n",
      " MetricInfo(id='google_bleu',\n",
      "            space_id='evaluate-metric/google_bleu',\n",
      "            description='The BLEU score has some undesirable properties when '\n",
      "                        'used for single sentences, as it was designed to be a '\n",
      "                        'corpus measure. We therefore  use a slightly '\n",
      "                        'different score for our RL experiments which we call '\n",
      "                        \"the 'GLEU score'. For the GLEU score, we record all \"\n",
      "                        'sub-sequences of 1, 2, 3 or 4 tokens in output and '\n",
      "                        'target sequence (n-grams). We then compute a recall, '\n",
      "                        'which is the ratio of the number of matching n-grams '\n",
      "                        'to the number of total n-grams in the target (ground '\n",
      "                        'truth) sequence, and a precision, which is the ratio '\n",
      "                        'of the number of matching n-grams to the number of '\n",
      "                        'total n-grams in the generated output sequence. Then '\n",
      "                        'GLEU score is simply the minimum of recall and '\n",
      "                        \"precision. This GLEU score's range is always between \"\n",
      "                        '0 (no matches) and 1 (all match) and it is '\n",
      "                        'symmetrical when switching output and target. '\n",
      "                        'According to our experiments, GLEU score correlates '\n",
      "                        'quite well with the BLEU metric on a corpus level but '\n",
      "                        'does not have its drawbacks for our per sentence '\n",
      "                        'reward objective.'),\n",
      " MetricInfo(id='indic_glue',\n",
      "            space_id='evaluate-metric/indic_glue',\n",
      "            description='IndicGLUE is a natural language understanding '\n",
      "                        'benchmark for Indian languages. It contains a wide '\n",
      "                        'variety of tasks and covers 11 major Indian languages '\n",
      "                        '- as, bn, gu, hi, kn, ml, mr, or, pa, ta, te.'),\n",
      " MetricInfo(id='mae',\n",
      "            space_id='evaluate-metric/mae',\n",
      "            description='Mean Absolute Error (MAE) is the mean of the '\n",
      "                        'magnitude of difference between the predicted and '\n",
      "                        'actual values.'),\n",
      " MetricInfo(id='mahalanobis',\n",
      "            space_id='evaluate-metric/mahalanobis',\n",
      "            description='Compute the Mahalanobis Distance\\n'\n",
      "                        'Mahalonobis distance is the distance between a point '\n",
      "                        'and a distribution. And not between two distinct '\n",
      "                        'points. It is effectively a multivariate equivalent '\n",
      "                        'of the Euclidean distance. It was introduced by Prof. '\n",
      "                        'P. C. Mahalanobis in 1936 and has been used in '\n",
      "                        'various statistical applications ever since [source: '\n",
      "                        'https://www.machinelearningplus.com/statistics/mahalanobis-distance/]'),\n",
      " MetricInfo(id='mape',\n",
      "            space_id='evaluate-metric/mape',\n",
      "            description='Mean Absolute Percentage Error (MAPE) is the mean '\n",
      "                        'percentage error difference between the predicted and '\n",
      "                        'actual values.'),\n",
      " MetricInfo(id='mase',\n",
      "            space_id='evaluate-metric/mase',\n",
      "            description='Mean Absolute Scaled Error (MASE) is the mean '\n",
      "                        'absolute error of the forecast values, divided by the '\n",
      "                        'mean absolute error of the in-sample one-step naive '\n",
      "                        'forecast on the training set.'),\n",
      " MetricInfo(id='matthews_correlation',\n",
      "            space_id='evaluate-metric/matthews_correlation',\n",
      "            description='Compute the Matthews correlation coefficient (MCC)\\n'\n",
      "                        'The Matthews correlation coefficient is used in '\n",
      "                        'machine learning as a measure of the quality of '\n",
      "                        'binary and multiclass classifications. It takes into '\n",
      "                        'account true and false positives and negatives and is '\n",
      "                        'generally regarded as a balanced measure which can be '\n",
      "                        'used even if the classes are of very different sizes. '\n",
      "                        'The MCC is in essence a correlation coefficient value '\n",
      "                        'between -1 and +1. A coefficient of +1 represents a '\n",
      "                        'perfect prediction, 0 an average random prediction '\n",
      "                        'and -1 an inverse prediction.  The statistic is also '\n",
      "                        'known as the phi coefficient. [source: Wikipedia]'),\n",
      " MetricInfo(id='mauve',\n",
      "            space_id='evaluate-metric/mauve',\n",
      "            description='MAUVE is a measure of the statistical gap between two '\n",
      "                        'text distributions, e.g., how far the text written by '\n",
      "                        'a model is the distribution of human text, using '\n",
      "                        'samples from both distributions.\\n'\n",
      "                        'MAUVE is obtained by computing Kullback–Leibler (KL) '\n",
      "                        'divergences between the two distributions in a '\n",
      "                        'quantized embedding space of a large language model. '\n",
      "                        'It can quantify differences in the quality of '\n",
      "                        'generated text based on the size of the model, the '\n",
      "                        'decoding algorithm, and the length of the generated '\n",
      "                        'text. MAUVE was found to correlate the strongest with '\n",
      "                        'human evaluations over baseline metrics for '\n",
      "                        'open-ended text generation.'),\n",
      " MetricInfo(id='mean_iou',\n",
      "            space_id='evaluate-metric/mean_iou',\n",
      "            description='IoU is the area of overlap between the predicted '\n",
      "                        'segmentation and the ground truth divided by the area '\n",
      "                        'of union between the predicted segmentation and the '\n",
      "                        'ground truth. For binary (two classes) or multi-class '\n",
      "                        'segmentation, the mean IoU of the image is calculated '\n",
      "                        'by taking the IoU of each class and averaging them.'),\n",
      " MetricInfo(id='meteor',\n",
      "            space_id='evaluate-metric/meteor',\n",
      "            description='METEOR, an automatic metric for machine translation '\n",
      "                        'evaluation that is based on a generalized concept of '\n",
      "                        'unigram matching between the machine-produced '\n",
      "                        'translation and human-produced reference '\n",
      "                        'translations. Unigrams can be matched based on their '\n",
      "                        'surface forms, stemmed forms, and meanings; '\n",
      "                        'furthermore, METEOR can be easily extended to include '\n",
      "                        'more advanced matching strategies. Once all '\n",
      "                        'generalized unigram matches between the two strings '\n",
      "                        'have been found, METEOR computes a score for this '\n",
      "                        'matching using a combination of unigram-precision, '\n",
      "                        'unigram-recall, and a measure of fragmentation that '\n",
      "                        'is designed to directly capture how well-ordered the '\n",
      "                        'matched words in the machine translation are in '\n",
      "                        'relation to the reference.\\n'\n",
      "                        'METEOR gets an R correlation value of 0.347 with '\n",
      "                        'human evaluation on the Arabic data and 0.331 on the '\n",
      "                        'Chinese data. This is shown to be an improvement on '\n",
      "                        'using simply unigram-precision, unigram-recall and '\n",
      "                        'their harmonic F1 combination.'),\n",
      " MetricInfo(id='mse',\n",
      "            space_id='evaluate-metric/mse',\n",
      "            description='Mean Squared Error(MSE) is the average of the square '\n",
      "                        'of difference between the predicted and actual '\n",
      "                        'values.'),\n",
      " MetricInfo(id='nist_mt',\n",
      "            space_id='evaluate-metric/nist_mt',\n",
      "            description='DARPA commissioned NIST to develop an MT evaluation '\n",
      "                        'facility based on the BLEU score.'),\n",
      " MetricInfo(id='pearsonr',\n",
      "            space_id='evaluate-metric/pearsonr',\n",
      "            description='Pearson correlation coefficient and p-value for '\n",
      "                        'testing non-correlation. The Pearson correlation '\n",
      "                        'coefficient measures the linear relationship between '\n",
      "                        'two datasets. The calculation of the p-value relies '\n",
      "                        'on the assumption that each dataset is normally '\n",
      "                        'distributed. Like other correlation coefficients, '\n",
      "                        'this one varies between -1 and +1 with 0 implying no '\n",
      "                        'correlation. Correlations of -1 or +1 imply an exact '\n",
      "                        'linear relationship. Positive correlations imply that '\n",
      "                        'as x increases, so does y. Negative correlations '\n",
      "                        'imply that as x increases, y decreases. The p-value '\n",
      "                        'roughly indicates the probability of an uncorrelated '\n",
      "                        'system producing datasets that have a Pearson '\n",
      "                        'correlation at least as extreme as the one computed '\n",
      "                        'from these datasets.'),\n",
      " MetricInfo(id='perplexity',\n",
      "            space_id='evaluate-metric/perplexity',\n",
      "            description='Perplexity (PPL) is one of the most common metrics '\n",
      "                        'for evaluating language models. It is defined as the '\n",
      "                        'exponentiated average negative log-likelihood of a '\n",
      "                        'sequence, calculated with exponent base `e`.\\n'\n",
      "                        'For more information on perplexity, see [this '\n",
      "                        'tutorial](https://huggingface.co/docs/transformers/perplexity).'),\n",
      " MetricInfo(id='poseval',\n",
      "            space_id='evaluate-metric/poseval',\n",
      "            description='The poseval metric can be used to evaluate POS '\n",
      "                        'taggers. Since seqeval does not work well with POS '\n",
      "                        'data  that is not in IOB format the poseval is an '\n",
      "                        'alternative. It treats each token in the dataset as '\n",
      "                        'independant  observation and computes the precision, '\n",
      "                        'recall and F1-score irrespective of sentences. It '\n",
      "                        \"uses scikit-learns's classification report to compute \"\n",
      "                        'the scores.'),\n",
      " MetricInfo(id='precision',\n",
      "            space_id='evaluate-metric/precision',\n",
      "            description='Precision is the fraction of correctly labeled '\n",
      "                        'positive examples out of all of the examples that '\n",
      "                        'were labeled as positive. It is computed via the '\n",
      "                        'equation: Precision = TP / (TP + FP) where TP is the '\n",
      "                        'True positives (i.e. the examples correctly labeled '\n",
      "                        'as positive) and FP is the False positive examples '\n",
      "                        '(i.e. the examples incorrectly labeled as positive).'),\n",
      " MetricInfo(id='r_squared',\n",
      "            space_id='evaluate-metric/r_squared',\n",
      "            description='The R^2 (R Squared) metric is a measure of the '\n",
      "                        'goodness of fit of a linear regression model. It is '\n",
      "                        'the proportion of the variance in the dependent '\n",
      "                        'variable that is predictable from the independent '\n",
      "                        'variable.'),\n",
      " MetricInfo(id='recall',\n",
      "            space_id='evaluate-metric/recall',\n",
      "            description='Recall is the fraction of the positive examples that '\n",
      "                        'were correctly labeled by the model as positive. It '\n",
      "                        'can be computed with the equation: Recall = TP / (TP '\n",
      "                        '+ FN) Where TP is the true positives and FN is the '\n",
      "                        'false negatives.'),\n",
      " MetricInfo(id='rl_reliability',\n",
      "            space_id='evaluate-metric/rl_reliability',\n",
      "            description='Computes the RL reliability metrics from a set of '\n",
      "                        'experiments. There is an `\"online\"` and `\"offline\"` '\n",
      "                        'configuration for evaluation.'),\n",
      " MetricInfo(id='roc_auc',\n",
      "            space_id='evaluate-metric/roc_auc',\n",
      "            description='This metric computes the area under the curve (AUC) '\n",
      "                        'for the Receiver Operating Characteristic Curve '\n",
      "                        '(ROC). The return values represent how well the model '\n",
      "                        'used is predicting the correct classes, based on the '\n",
      "                        'input data. A score of `0.5` means that the model is '\n",
      "                        \"predicting exactly at chance, i.e. the model's \"\n",
      "                        'predictions are correct at the same rate as if the '\n",
      "                        'predictions were being decided by the flip of a fair '\n",
      "                        'coin or the roll of a fair die. A score above `0.5` '\n",
      "                        'indicates that the model is doing better than chance, '\n",
      "                        'while a score below `0.5` indicates that the model is '\n",
      "                        'doing worse than chance.\\n'\n",
      "                        'This metric has three separate use cases: - binary: '\n",
      "                        'The case in which there are only two different label '\n",
      "                        'classes, and each example gets only one label. This '\n",
      "                        'is the default implementation. - multiclass: The case '\n",
      "                        'in which there can be more than two different label '\n",
      "                        'classes, but each example still gets only one label. '\n",
      "                        '- multilabel: The case in which there can be more '\n",
      "                        'than two different label classes, and each example '\n",
      "                        'can have more than one label.'),\n",
      " MetricInfo(id='rouge',\n",
      "            space_id='evaluate-metric/rouge',\n",
      "            description='ROUGE, or Recall-Oriented Understudy for Gisting '\n",
      "                        'Evaluation, is a set of metrics and a software '\n",
      "                        'package used for evaluating automatic summarization '\n",
      "                        'and machine translation software in natural language '\n",
      "                        'processing. The metrics compare an automatically '\n",
      "                        'produced summary or translation against a reference '\n",
      "                        'or a set of references (human-produced) summary or '\n",
      "                        'translation.\\n'\n",
      "                        'Note that ROUGE is case insensitive, meaning that '\n",
      "                        'upper case letters are treated the same way as lower '\n",
      "                        'case letters.\\n'\n",
      "                        'This metrics is a wrapper around Google Research '\n",
      "                        'reimplementation of ROUGE: '\n",
      "                        'https://github.com/google-research/google-research/tree/master/rouge'),\n",
      " MetricInfo(id='sacrebleu',\n",
      "            space_id='evaluate-metric/sacrebleu',\n",
      "            description='SacreBLEU provides hassle-free computation of '\n",
      "                        'shareable, comparable, and reproducible BLEU scores. '\n",
      "                        \"Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \"\n",
      "                        'it produces the official WMT scores but works with '\n",
      "                        'plain text. It also knows all the standard test sets '\n",
      "                        'and handles downloading, processing, and tokenization '\n",
      "                        'for you.\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='sari',\n",
      "            space_id='evaluate-metric/sari',\n",
      "            description='SARI is a metric used for evaluating automatic text '\n",
      "                        'simplification systems. The metric compares the '\n",
      "                        'predicted simplified sentences against the reference '\n",
      "                        'and the source sentences. It explicitly measures the '\n",
      "                        'goodness of words that are added, deleted and kept by '\n",
      "                        'the system. Sari = (F1_add + F1_keep + P_del) / 3 '\n",
      "                        'where F1_add: n-gram F1 score for add operation '\n",
      "                        'F1_keep: n-gram F1 score for keep operation P_del: '\n",
      "                        'n-gram precision score for delete operation n = 4, as '\n",
      "                        'in the original paper.\\n'\n",
      "                        \"This implementation is adapted from Tensorflow's \"\n",
      "                        'tensor2tensor implementation [3]. It has two '\n",
      "                        'differences with the original GitHub [1] '\n",
      "                        'implementation: (1) Defines 0/0=1 instead of 0 to '\n",
      "                        'give higher scores for predictions that match a '\n",
      "                        'target exactly. (2) Fixes an alleged bug [2] in the '\n",
      "                        'keep score computation. [1] '\n",
      "                        'https://github.com/cocoxu/simplification/blob/master/SARI.py '\n",
      "                        '(commit 0210f15) [2] '\n",
      "                        'https://github.com/cocoxu/simplification/issues/6 [3] '\n",
      "                        'https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/sari_hook.py'),\n",
      " MetricInfo(id='seqeval',\n",
      "            space_id='evaluate-metric/seqeval',\n",
      "            description='seqeval is a Python framework for sequence labeling '\n",
      "                        'evaluation. seqeval can evaluate the performance of '\n",
      "                        'chunking tasks such as named-entity recognition, '\n",
      "                        'part-of-speech tagging, semantic role labeling and so '\n",
      "                        'on.\\n'\n",
      "                        'This is well-tested by using the Perl script '\n",
      "                        'conlleval, which can be used for measuring the '\n",
      "                        'performance of a system that has processed the '\n",
      "                        'CoNLL-2000 shared task data.\\n'\n",
      "                        'seqeval supports following formats: IOB1 IOB2 IOE1 '\n",
      "                        'IOE2 IOBES\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/chakki-works/seqeval for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='smape',\n",
      "            space_id='evaluate-metric/smape',\n",
      "            description='Symmetric Mean Absolute Percentage Error (sMAPE) is '\n",
      "                        'the symmetric mean percentage error difference '\n",
      "                        'between the predicted and actual values defined by '\n",
      "                        'Chen and Yang (2004).'),\n",
      " MetricInfo(id='spearmanr',\n",
      "            space_id='evaluate-metric/spearmanr',\n",
      "            description='The Spearman rank-order correlation coefficient is a '\n",
      "                        'measure of the relationship between two datasets. '\n",
      "                        'Like other correlation coefficients, this one varies '\n",
      "                        'between -1 and +1 with 0 implying no correlation. '\n",
      "                        'Positive correlations imply that as data in dataset x '\n",
      "                        'increases, so does data in dataset y. Negative '\n",
      "                        'correlations imply that as x increases, y decreases. '\n",
      "                        'Correlations of -1 or +1 imply an exact monotonic '\n",
      "                        'relationship.\\n'\n",
      "                        'Unlike the Pearson correlation, the Spearman '\n",
      "                        'correlation does not assume that both datasets are '\n",
      "                        'normally distributed.\\n'\n",
      "                        'The p-value roughly indicates the probability of an '\n",
      "                        'uncorrelated system producing datasets that have a '\n",
      "                        'Spearman correlation at least as extreme as the one '\n",
      "                        'computed from these datasets. The p-values are not '\n",
      "                        'entirely reliable but are probably reasonable for '\n",
      "                        'datasets larger than 500 or so.'),\n",
      " MetricInfo(id='squad',\n",
      "            space_id='evaluate-metric/squad',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 1 of the Stanford Question Answering Dataset '\n",
      "                        '(SQuAD).\\n'\n",
      "                        'Stanford Question Answering Dataset (SQuAD) is a '\n",
      "                        'reading comprehension dataset, consisting of '\n",
      "                        'questions posed by crowdworkers on a set of Wikipedia '\n",
      "                        'articles, where the answer to every question is a '\n",
      "                        'segment of text, or span, from the corresponding '\n",
      "                        'reading passage, or the question might be '\n",
      "                        'unanswerable.'),\n",
      " MetricInfo(id='squad_v2',\n",
      "            space_id='evaluate-metric/squad_v2',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 2 of the Stanford Question Answering Dataset '\n",
      "                        '(SQuAD).\\n'\n",
      "                        'Stanford Question Answering Dataset (SQuAD) is a '\n",
      "                        'reading comprehension dataset, consisting of '\n",
      "                        'questions posed by crowdworkers on a set of Wikipedia '\n",
      "                        'articles, where the answer to every question is a '\n",
      "                        'segment of text, or span, from the corresponding '\n",
      "                        'reading passage, or the question might be '\n",
      "                        'unanswerable.\\n'\n",
      "                        'SQuAD2.0 combines the 100,000 questions in SQuAD1.1 '\n",
      "                        'with over 50,000 unanswerable questions  written '\n",
      "                        'adversarially by crowdworkers to look similar to '\n",
      "                        'answerable ones. To do well on SQuAD2.0, systems must '\n",
      "                        'not only answer questions when possible, but also '\n",
      "                        'determine when no answer is supported by the '\n",
      "                        'paragraph and abstain from answering.'),\n",
      " MetricInfo(id='super_glue',\n",
      "            space_id='evaluate-metric/super_glue',\n",
      "            description='SuperGLUE (https://super.gluebenchmark.com/) is a new '\n",
      "                        'benchmark styled after GLUE with a new set of more '\n",
      "                        'difficult language understanding tasks, improved '\n",
      "                        'resources, and a new public leaderboard.'),\n",
      " MetricInfo(id='ter',\n",
      "            space_id='evaluate-metric/ter',\n",
      "            description='TER (Translation Edit Rate, also called Translation '\n",
      "                        'Error Rate) is a metric to quantify the edit '\n",
      "                        'operations that a hypothesis requires to match a '\n",
      "                        'reference translation. We use the implementation that '\n",
      "                        'is already present in sacrebleu '\n",
      "                        '(https://github.com/mjpost/sacreBLEU#ter), which in '\n",
      "                        'turn is inspired by the TERCOM implementation, which '\n",
      "                        'can be found here: '\n",
      "                        'https://github.com/jhclark/tercom.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#ter for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='trec_eval',\n",
      "            space_id='evaluate-metric/trec_eval',\n",
      "            description='The TREC Eval metric combines a number of information '\n",
      "                        'retrieval metrics such as precision and nDCG. It is '\n",
      "                        'used to score rankings of retrieved documents with '\n",
      "                        'reference values.'),\n",
      " MetricInfo(id='wer',\n",
      "            space_id='evaluate-metric/wer',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='wiki_split',\n",
      "            space_id='evaluate-metric/wiki_split',\n",
      "            description='WIKI_SPLIT is the combination of three metrics SARI, '\n",
      "                        'EXACT and SACREBLEU It can be used to evaluate the '\n",
      "                        'quality of machine-generated texts.'),\n",
      " MetricInfo(id='xnli',\n",
      "            space_id='evaluate-metric/xnli',\n",
      "            description='XNLI is a subset of a few thousand examples from MNLI '\n",
      "                        'which has been translated into a 14 different '\n",
      "                        'languages (some low-ish resource). As with MNLI, the '\n",
      "                        'goal is to predict textual entailment (does sentence '\n",
      "                        'A imply/contradict/neither sentence B) and is a '\n",
      "                        'classification task (given two sentences, predict one '\n",
      "                        'of three labels).'),\n",
      " MetricInfo(id='xtreme_s',\n",
      "            space_id='evaluate-metric/xtreme_s',\n",
      "            description='XTREME-S is a benchmark to evaluate universal '\n",
      "                        'cross-lingual speech representations in many '\n",
      "                        'languages. XTREME-S covers four task families: speech '\n",
      "                        'recognition, classification, speech-to-text '\n",
      "                        'translation and retrieval.'),\n",
      " MetricInfo(id='Aledade/extraction_evaluation',\n",
      "            space_id='Aledade/extraction_evaluation',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
      "            space_id='AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
      "            description=None),\n",
      " MetricInfo(id='Alirezamp/seqeval',\n",
      "            space_id='Alirezamp/seqeval',\n",
      "            description='seqeval is a Python framework for sequence labeling '\n",
      "                        'evaluation. seqeval can evaluate the performance of '\n",
      "                        'chunking tasks such as named-entity recognition, '\n",
      "                        'part-of-speech tagging, semantic role labeling and so '\n",
      "                        'on.\\n'\n",
      "                        'This is well-tested by using the Perl script '\n",
      "                        'conlleval, which can be used for measuring the '\n",
      "                        'performance of a system that has processed the '\n",
      "                        'CoNLL-2000 shared task data.\\n'\n",
      "                        'seqeval supports following formats: IOB1 IOB2 IOE1 '\n",
      "                        'IOE2 IOBES\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/chakki-works/seqeval for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='Aye10032/loss_metric',\n",
      "            space_id='Aye10032/loss_metric',\n",
      "            description=None),\n",
      " MetricInfo(id='Bekhouche/NED',\n",
      "            space_id='Bekhouche/NED',\n",
      "            description='The Normalized Edit Distance (NED) is a metric used '\n",
      "                        'to quantify the dissimilarity between two sequences, '\n",
      "                        'typically strings, by measuring the minimum number of '\n",
      "                        'editing operations required to transform one sequence '\n",
      "                        'into the other, normalized by the length of the '\n",
      "                        'longer sequence.  The NED ranges from 0 to 1, where 0 '\n",
      "                        'indicates identical sequences and 1 indicates '\n",
      "                        'completely dissimilar sequences. It is particularly '\n",
      "                        'useful in tasks such as spell checking, speech '\n",
      "                        'recognition, and OCR. The normalized edit distance '\n",
      "                        'can be calculated using the formula: NED = (1 - '\n",
      "                        '(ED(pred, gt) / max(length(pred), length(gt)))) '\n",
      "                        'Where: gt: ground-truth sequence pred: predicted '\n",
      "                        'sequence ED: Edit Distance, the minimum number of '\n",
      "                        'editing operations (insertions, deletions, '\n",
      "                        'substitutions) needed to transform one sequence into '\n",
      "                        'the other.'),\n",
      " MetricInfo(id='BridgeAI-Lab/Sem-nCG',\n",
      "            space_id='BridgeAI-Lab/Sem-nCG',\n",
      "            description='Sem-nCG (Semantic Normalized Cumulative Gain) Metric '\n",
      "                        'evaluates the quality of predicted sentences  '\n",
      "                        '(abstractive/extractive) in relation to reference '\n",
      "                        'sentences and documents using Semantic Normalized '\n",
      "                        'Cumulative Gain  (NCG). It computes gain values and '\n",
      "                        'NCG scores based on cosine similarity between '\n",
      "                        'sentence embeddings, leveraging a  Sentence-BERT '\n",
      "                        'encoder. This metric is designed to assess the '\n",
      "                        'relevance and ranking of predicted sentences, making '\n",
      "                        'it  useful for tasks such as summarization and '\n",
      "                        'information retrieval.'),\n",
      " MetricInfo(id='BridgeAI-Lab/SemF1',\n",
      "            space_id='BridgeAI-Lab/SemF1',\n",
      "            description='SEM-F1 metric leverages the pre-trained contextual '\n",
      "                        'embeddings and evaluates the model generated semantic '\n",
      "                        'overlap  summary with the reference overlap summary. '\n",
      "                        'It evaluates the semantic overlap summary at the '\n",
      "                        'sentence level and  computes precision, recall and F1 '\n",
      "                        'scores.\\n'\n",
      "                        'Refer to the paper `SEM-F1: an Automatic Way for '\n",
      "                        'Semantic Evaluation of Multi-Narrative Overlap '\n",
      "                        'Summaries at Scale`  for more details. '),\n",
      " MetricInfo(id='BucketHeadP65/confusion_matrix',\n",
      "            space_id='BucketHeadP65/confusion_matrix',\n",
      "            description='Compute confusion matrix to evaluate the accuracy of '\n",
      "                        'a classification. By definition a confusion matrix '\n",
      "                        ':math:C is such that :math:C_{i, j} is equal to the '\n",
      "                        'number of observations known to be in group :math:i '\n",
      "                        'and predicted to be in group :math:j. Thus in binary '\n",
      "                        'classification, the count of true negatives is '\n",
      "                        ':math:C_{0,0}, false negatives is :math:C_{1,0}, true '\n",
      "                        'positives is :math:C_{1,1} and false positives is '\n",
      "                        ':math:C_{0,1}.'),\n",
      " MetricInfo(id='BucketHeadP65/roc_curve',\n",
      "            space_id='BucketHeadP65/roc_curve',\n",
      "            description='Compute Receiver operating characteristic (ROC). '\n",
      "                        'Note: this implementation is restricted to the binary '\n",
      "                        'classification task.'),\n",
      " MetricInfo(id='CZLC/rouge_raw',\n",
      "            space_id='CZLC/rouge_raw',\n",
      "            description='ROUGE RAW is language-agnostic variant of ROUGE '\n",
      "                        'without stemmer, stop words and synonymas.  This is a '\n",
      "                        'wrapper around the original '\n",
      "                        'http://hdl.handle.net/11234/1-2615 script.'),\n",
      " MetricInfo(id='DaliaCaRo/accents_unplugged_eval',\n",
      "            space_id='DaliaCaRo/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='DarrenChensformer/action_generation',\n",
      "            space_id='DarrenChensformer/action_generation',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='DarrenChensformer/eval_keyphrase',\n",
      "            space_id='DarrenChensformer/eval_keyphrase',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='DarrenChensformer/relation_extraction',\n",
      "            space_id='DarrenChensformer/relation_extraction',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='DoctorSlimm/bangalore_score',\n",
      "            space_id='DoctorSlimm/bangalore_score',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='DoctorSlimm/kaushiks_criteria',\n",
      "            space_id='DoctorSlimm/kaushiks_criteria',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Drunper/metrica_tesi',\n",
      "            space_id='Drunper/metrica_tesi',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Felipehonorato/eer',\n",
      "            space_id='Felipehonorato/eer',\n",
      "            description='Equal Error Rate (EER) is a measure that shows the '\n",
      "                        'performance of a biometric system, like fingerprint '\n",
      "                        \"or facial recognition. It's the point where the \"\n",
      "                        \"system's False Acceptance Rate (letting the wrong \"\n",
      "                        'person in) and False Rejection Rate (blocking the '\n",
      "                        'right person) are equal. The lower the EER value, the '\n",
      "                        \"better the system's performance.\\n\"\n",
      "                        'EER is used in various security applications, such as '\n",
      "                        'airports, banks, and personal devices like '\n",
      "                        'smartphones and laptops, to evaluate the '\n",
      "                        'effectiveness of the biometric system in correctly '\n",
      "                        'identifying users.'),\n",
      " MetricInfo(id='Fritz02/execution_accuracy',\n",
      "            space_id='Fritz02/execution_accuracy',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='GMFTBY/dailydialog_evaluate',\n",
      "            space_id='GMFTBY/dailydialog_evaluate',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='GMFTBY/dailydialogevaluate',\n",
      "            space_id='GMFTBY/dailydialogevaluate',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='He-Xingwei/sari_metric',\n",
      "            space_id='He-Xingwei/sari_metric',\n",
      "            description='SARI is a metric used for evaluating automatic text '\n",
      "                        'simplification systems. The metric compares the '\n",
      "                        'predicted simplified sentences against the reference '\n",
      "                        'and the source sentences. It explicitly measures the '\n",
      "                        'goodness of words that are added, deleted and kept by '\n",
      "                        'the system. Sari = (F1_add + F1_keep + P_del) / 3 '\n",
      "                        'where F1_add: n-gram F1 score for add operation '\n",
      "                        'F1_keep: n-gram F1 score for keep operation P_del: '\n",
      "                        'n-gram precision score for delete operation n = 4, as '\n",
      "                        'in the original paper.\\n'\n",
      "                        \"This implementation is adapted from Tensorflow's \"\n",
      "                        'tensor2tensor implementation [3]. It has two '\n",
      "                        'differences with the original GitHub [1] '\n",
      "                        'implementation: (1) Defines 0/0=1 instead of 0 to '\n",
      "                        'give higher scores for predictions that match a '\n",
      "                        'target exactly. (2) Fixes an alleged bug [2] in the '\n",
      "                        'keep score computation. [1] '\n",
      "                        'https://github.com/cocoxu/simplification/blob/master/SARI.py '\n",
      "                        '(commit 0210f15) [2] '\n",
      "                        'https://github.com/cocoxu/simplification/issues/6 [3] '\n",
      "                        'https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/sari_hook.py'),\n",
      " MetricInfo(id='Ikala-allen/relation_extraction',\n",
      "            space_id='Ikala-allen/relation_extraction',\n",
      "            description='This metric is used for evaluating the F1 accuracy of '\n",
      "                        'input references and predictions.'),\n",
      " MetricInfo(id='JP-SystemsX/nDCG',\n",
      "            space_id='JP-SystemsX/nDCG',\n",
      "            description='The Discounted Cumulative Gain is a measure of '\n",
      "                        'ranking quality.  It is used to evaluate Information '\n",
      "                        'Retrieval Systems under the following 2 assumptions:\\n'\n",
      "                        '        1. Highly relevant documents/Labels are more '\n",
      "                        'useful when appearing earlier in the results\\n'\n",
      "                        '        2. Documents/Labels are relevant to different '\n",
      "                        'degrees\\n'\n",
      "                        'It is defined as the Sum over all relevances of the '\n",
      "                        'retrieved documents reduced logarithmically '\n",
      "                        'proportional to  the position in which they were '\n",
      "                        'retrieved. The Normalized DCG (nDCG) divides the '\n",
      "                        'resulting value by the best possible value to get a '\n",
      "                        'value between  0 and 1 s.t. a perfect retrieval '\n",
      "                        'achieves a nDCG of 1.'),\n",
      " MetricInfo(id='Josh98/nl2bash_m',\n",
      "            space_id='Josh98/nl2bash_m',\n",
      "            description='Accuracy is the proportion of correct predictions '\n",
      "                        'among the total number of cases processed. It can be '\n",
      "                        'computed with: Accuracy = (TP + TN) / (TP + TN + FP + '\n",
      "                        'FN) Where: TP: True positive TN: True negative FP: '\n",
      "                        'False positive FN: False negative'),\n",
      " MetricInfo(id='KaliSurfKukt/brier_score',\n",
      "            space_id='KaliSurfKukt/brier_score',\n",
      "            description='The Brier score is a measure of the error between two '\n",
      "                        'probability distributions.'),\n",
      " MetricInfo(id='KevinSpaghetti/accuracyk',\n",
      "            space_id='KevinSpaghetti/accuracyk',\n",
      "            description='computes the accuracy at k for a set of predictions '\n",
      "                        'as labels'),\n",
      " MetricInfo(id='LottieW/accents_unplugged_eval',\n",
      "            space_id='LottieW/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='LuckiestOne/valid_efficiency_score',\n",
      "            space_id='LuckiestOne/valid_efficiency_score',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='MathewShen/bleu',\n",
      "            space_id='MathewShen/bleu',\n",
      "            description='BLEU (Bilingual Evaluation Understudy) is an '\n",
      "                        'algorithm for evaluating the quality of text which '\n",
      "                        'has been machine-translated from one natural language '\n",
      "                        'to another. Quality is considered to be the '\n",
      "                        \"correspondence between a machine's output and that of \"\n",
      "                        'a human: \"the closer a machine translation is to a '\n",
      "                        'professional human translation, the better it is\" – '\n",
      "                        'this is the central idea behind BLEU. BLEU was one of '\n",
      "                        'the first metrics to claim a high correlation with '\n",
      "                        'human judgements of quality, and remains one of the '\n",
      "                        'most popular automated and inexpensive metrics.\\n'\n",
      "                        'Scores are calculated for individual translated '\n",
      "                        'segments—generally sentences—by comparing them with a '\n",
      "                        'set of good quality reference translations. Those '\n",
      "                        'scores are then averaged over the whole corpus to '\n",
      "                        \"reach an estimate of the translation's overall \"\n",
      "                        'quality. Neither intelligibility nor grammatical '\n",
      "                        'correctness are not taken into account.'),\n",
      " MetricInfo(id='Merle456/accents_unplugged_eval',\n",
      "            space_id='Merle456/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='Muennighoff/code_eval_octopack',\n",
      "            space_id='Muennighoff/code_eval_octopack',\n",
      "            description='This metric implements code evaluation with execution '\n",
      "                        'across multiple languages as used in the paper '\n",
      "                        '\"OctoPack: Instruction Tuning Code Large Language '\n",
      "                        'Models\" (https://arxiv.org/abs/2308.07124).'),\n",
      " MetricInfo(id='NCSOFT/harim_plus',\n",
      "            space_id='NCSOFT/harim_plus',\n",
      "            description='HaRiM+ is reference-less metric for summary quality '\n",
      "                        'evaluation which hurls the power of summarization '\n",
      "                        'model to estimate the quality of the summary-article '\n",
      "                        'pair. <br /> Note that this metric is reference-free '\n",
      "                        'and do not require training. It is ready to go '\n",
      "                        'without reference text to compare with the generation '\n",
      "                        'nor any model training for scoring.'),\n",
      " MetricInfo(id='Natooz/ece',\n",
      "            space_id='Natooz/ece',\n",
      "            description='Expected calibration error (ECE)'),\n",
      " MetricInfo(id='Ndyyyy/bertscore',\n",
      "            space_id='Ndyyyy/bertscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='NikitaMartynov/spell-check-metric',\n",
      "            space_id='NikitaMartynov/spell-check-metric',\n",
      "            description='This module calculates classification metrics e.g. '\n",
      "                        'precision, recall, F1, on spell-checking task.'),\n",
      " MetricInfo(id='NimaBoscarino/weat',\n",
      "            space_id='NimaBoscarino/weat',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Ochiroo/rouge_mn',\n",
      "            space_id='Ochiroo/rouge_mn',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Pipatpong/perplexity',\n",
      "            space_id='Pipatpong/perplexity',\n",
      "            description='Perplexity (PPL) is one of the most common metrics '\n",
      "                        'for evaluating language models. It is defined as the '\n",
      "                        'exponentiated average negative log-likelihood of a '\n",
      "                        'sequence, calculated with exponent base `e`.\\n'\n",
      "                        'For more information on perplexity, see [this '\n",
      "                        'tutorial](https://huggingface.co/docs/transformers/perplexity).'),\n",
      " MetricInfo(id='Qui-nn/accents_unplugged_eval',\n",
      "            space_id='Qui-nn/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='Ransaka/cer',\n",
      "            space_id='Ransaka/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='RiciHuggingFace/accents_unplugged_eval',\n",
      "            space_id='RiciHuggingFace/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='SEA-AI/box-metrics',\n",
      "            space_id='SEA-AI/box-metrics',\n",
      "            description='built upon yolov5 iou functions. Outputs metrics '\n",
      "                        'regarding box fit'),\n",
      " MetricInfo(id='SEA-AI/det-metrics',\n",
      "            space_id='SEA-AI/det-metrics',\n",
      "            description='Modified cocoevals.py which is wrapped into '\n",
      "                        \"torchmetrics' mAP metric with numpy instead of torch \"\n",
      "                        'dependency.'),\n",
      " MetricInfo(id='SEA-AI/horizon-metrics',\n",
      "            space_id='SEA-AI/horizon-metrics',\n",
      "            description='This huggingface metric calculates horizon evaluation '\n",
      "                        'metrics using `seametrics.horizon.HorizonMetrics`.'),\n",
      " MetricInfo(id='SEA-AI/mot-metrics',\n",
      "            space_id='SEA-AI/mot-metrics',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='SEA-AI/panoptic-quality',\n",
      "            space_id='SEA-AI/panoptic-quality',\n",
      "            description='PanopticQuality score'),\n",
      " MetricInfo(id='Soroor/cer',\n",
      "            space_id='Soroor/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='SpfIo/wer_checker',\n",
      "            space_id='SpfIo/wer_checker',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='Splend1dchan/cosine_similarity',\n",
      "            space_id='Splend1dchan/cosine_similarity',\n",
      "            description='calculate the cosine similarity of two'),\n",
      " MetricInfo(id='TelEl/accents_unplugged_eval',\n",
      "            space_id='TelEl/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='TwentyNine/sacrebleu',\n",
      "            space_id='TwentyNine/sacrebleu',\n",
      "            description='SacreBLEU provides hassle-free computation of '\n",
      "                        'shareable, comparable, and reproducible BLEU scores. '\n",
      "                        \"Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \"\n",
      "                        'it produces the official WMT scores but works with '\n",
      "                        'plain text. It also knows all the standard test sets '\n",
      "                        'and handles downloading, processing, and tokenization '\n",
      "                        'for you.\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='Vallp/ter',\n",
      "            space_id='Vallp/ter',\n",
      "            description='TER (Translation Edit Rate, also called Translation '\n",
      "                        'Error Rate) is a metric to quantify the edit '\n",
      "                        'operations that a hypothesis requires to match a '\n",
      "                        'reference translation. We use the implementation that '\n",
      "                        'is already present in sacrebleu '\n",
      "                        '(https://github.com/mjpost/sacreBLEU#ter), which in '\n",
      "                        'turn is inspired by the TERCOM implementation, which '\n",
      "                        'can be found here: '\n",
      "                        'https://github.com/jhclark/tercom.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#ter for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='Vertaix/vendiscore',\n",
      "            space_id='Vertaix/vendiscore',\n",
      "            description='The Vendi Score is a metric for evaluating diversity '\n",
      "                        \"in machine learning. See the project's README at \"\n",
      "                        'https://github.com/vertaix/Vendi-Score for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='Vickyage/accents_unplugged_eval',\n",
      "            space_id='Vickyage/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='Viona/fuzzy_reordering',\n",
      "            space_id='Viona/fuzzy_reordering',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Viona/infolm',\n",
      "            space_id='Viona/infolm',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Viona/kendall_tau',\n",
      "            space_id='Viona/kendall_tau',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Vipitis/shadermatch',\n",
      "            space_id='Vipitis/shadermatch',\n",
      "            description='compare rendered frames from shadercode, using a WGPU '\n",
      "                        'implementation'),\n",
      " MetricInfo(id='Vlasta/pr_auc',\n",
      "            space_id='Vlasta/pr_auc',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Winfred13/cocoevaluate',\n",
      "            space_id='Winfred13/cocoevaluate',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='Yeshwant123/mcc',\n",
      "            space_id='Yeshwant123/mcc',\n",
      "            description='Matthews correlation coefficient (MCC) is a '\n",
      "                        'correlation coefficient used in machine learning as a '\n",
      "                        'measure of the quality of binary and multiclass '\n",
      "                        'classifications.'),\n",
      " MetricInfo(id='abdusah/aradiawer',\n",
      "            space_id='abdusah/aradiawer',\n",
      "            description='This new module is designed to calculate an enhanced '\n",
      "                        'Dialectical Arabic (DA) WER (AraDiaWER) based on '\n",
      "                        'linguistic and semantic factors.'),\n",
      " MetricInfo(id='abidlabs/mean_iou',\n",
      "            space_id='abidlabs/mean_iou',\n",
      "            description='IoU is the area of overlap between the predicted '\n",
      "                        'segmentation and the ground truth divided by the area '\n",
      "                        'of union between the predicted segmentation and the '\n",
      "                        'ground truth. For binary (two classes) or multi-class '\n",
      "                        'segmentation, the mean IoU of the image is calculated '\n",
      "                        'by taking the IoU of each class and averaging them.'),\n",
      " MetricInfo(id='abidlabs/mean_iou2',\n",
      "            space_id='abidlabs/mean_iou2',\n",
      "            description='IoU is the area of overlap between the predicted '\n",
      "                        'segmentation and the ground truth divided by the area '\n",
      "                        'of union between the predicted segmentation and the '\n",
      "                        'ground truth. For binary (two classes) or multi-class '\n",
      "                        'segmentation, the mean IoU of the image is calculated '\n",
      "                        'by taking the IoU of each class and averaging them.'),\n",
      " MetricInfo(id='ag2435/my_metric',\n",
      "            space_id='ag2435/my_metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='agkphysics/ccc',\n",
      "            space_id='agkphysics/ccc',\n",
      "            description='Concordance correlation coefficient'),\n",
      " MetricInfo(id='akki2825/accents_unplugged_eval',\n",
      "            space_id='akki2825/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='alvinasvk/accents_unplugged_eval',\n",
      "            space_id='alvinasvk/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='amitness/perplexity',\n",
      "            space_id='amitness/perplexity',\n",
      "            description='Perplexity (PPL) is one of the most common metrics '\n",
      "                        'for evaluating language models. It is defined as the '\n",
      "                        'exponentiated average negative log-likelihood of a '\n",
      "                        'sequence, calculated with exponent base `e`.\\n'\n",
      "                        'For more information on perplexity, see [this '\n",
      "                        'tutorial](https://huggingface.co/docs/transformers/perplexity).'),\n",
      " MetricInfo(id='andstor/code_perplexity',\n",
      "            space_id='andstor/code_perplexity',\n",
      "            description='Perplexity measure for code.'),\n",
      " MetricInfo(id='angelasophie/accents_unplugged_eval',\n",
      "            space_id='angelasophie/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='angelina-wang/directional_bias_amplification',\n",
      "            space_id='angelina-wang/directional_bias_amplification',\n",
      "            description='Directional Bias Amplification is a metric that '\n",
      "                        'captures the amount of bias (i.e., a conditional '\n",
      "                        'probability) that is amplified. This metric was '\n",
      "                        'introduced in the ICML 2021 paper [\"Directional Bias '\n",
      "                        'Amplification\"](https://arxiv.org/abs/2102.12594) for '\n",
      "                        'fairness evaluation.'),\n",
      " MetricInfo(id='anz2/iliauniiccocrevaluation',\n",
      "            space_id='anz2/iliauniiccocrevaluation',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='arthurvqin/pr_auc',\n",
      "            space_id='arthurvqin/pr_auc',\n",
      "            description='This metric computes the area under the curve (AUC) '\n",
      "                        'for the Precision-Recall Curve (PR). summarizes a '\n",
      "                        'precision-recall curve as the weighted mean of '\n",
      "                        'precisions achieved at each threshold, with the '\n",
      "                        'increase in recall from the previous threshold used '\n",
      "                        'as the weight.'),\n",
      " MetricInfo(id='aryopg/roc_auc_skip_uniform_labels',\n",
      "            space_id='aryopg/roc_auc_skip_uniform_labels',\n",
      "            description='This metric computes the area under the curve (AUC) '\n",
      "                        'for the Receiver Operating Characteristic Curve '\n",
      "                        '(ROC). The return values represent how well the model '\n",
      "                        'used is predicting the correct classes, based on the '\n",
      "                        'input data. A score of `0.5` means that the model is '\n",
      "                        \"predicting exactly at chance, i.e. the model's \"\n",
      "                        'predictions are correct at the same rate as if the '\n",
      "                        'predictions were being decided by the flip of a fair '\n",
      "                        'coin or the roll of a fair die. A score above `0.5` '\n",
      "                        'indicates that the model is doing better than chance, '\n",
      "                        'while a score below `0.5` indicates that the model is '\n",
      "                        'doing worse than chance.\\n'\n",
      "                        'This metric has three separate use cases: - binary: '\n",
      "                        'The case in which there are only two different label '\n",
      "                        'classes, and each example gets only one label. This '\n",
      "                        'is the default implementation. - multiclass: The case '\n",
      "                        'in which there can be more than two different label '\n",
      "                        'classes, but each example still gets only one label. '\n",
      "                        '- multilabel: The case in which there can be more '\n",
      "                        'than two different label classes, and each example '\n",
      "                        'can have more than one label.'),\n",
      " MetricInfo(id='bascobasculino/mot-metrics',\n",
      "            space_id='bascobasculino/mot-metrics',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='bdsaglam/jer',\n",
      "            space_id='bdsaglam/jer',\n",
      "            description='Computes precision, recall, and f1 scores for joint '\n",
      "                        'entity-relation extraction.'),\n",
      " MetricInfo(id='bdsaglam/musique',\n",
      "            space_id='bdsaglam/musique',\n",
      "            description='Question answering metrics (Exact Match and F1) for '\n",
      "                        'Musique-Answerable dataset.'),\n",
      " MetricInfo(id='berkatil/map',\n",
      "            space_id='berkatil/map',\n",
      "            description='This is the mean average precision (map) metric for '\n",
      "                        'retrieval systems. It is the average of the precision '\n",
      "                        'scores computer after each relevant document is got. '\n",
      "                        'You can refer to '\n",
      "                        '[here](https://amenra.github.io/ranx/metrics/#mean-average-precision)'),\n",
      " MetricInfo(id='berkatil/mrr',\n",
      "            space_id='berkatil/mrr',\n",
      "            description='This is the mean reciprocal rank (mrr) metric for '\n",
      "                        'retrieval systems. It is the average of the precision '\n",
      "                        'scores computer after each relevant document is got. '\n",
      "                        'You can refer to '\n",
      "                        '[here](https://amenra.github.io/ranx/metrics/#mean-reciprocal-rank)'),\n",
      " MetricInfo(id='bomjin/code_eval_octopack',\n",
      "            space_id='bomjin/code_eval_octopack',\n",
      "            description='This metric implements code evaluation with execution '\n",
      "                        'across multiple languages as used in the paper '\n",
      "                        '\"OctoPack: Instruction Tuning Code Large Language '\n",
      "                        'Models\" (https://arxiv.org/abs/2308.07124).'),\n",
      " MetricInfo(id='boschar/accents_unplugged_eval',\n",
      "            space_id='boschar/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='bowdbeg/docred',\n",
      "            space_id='bowdbeg/docred',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='bowdbeg/matching_series',\n",
      "            space_id='bowdbeg/matching_series',\n",
      "            description='Matching-based time-series generation metric'),\n",
      " MetricInfo(id='bowdbeg/patch_series',\n",
      "            space_id='bowdbeg/patch_series',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='brian920128/doc_retrieve_metrics',\n",
      "            space_id='brian920128/doc_retrieve_metrics',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='bstrai/classification_report',\n",
      "            space_id='bstrai/classification_report',\n",
      "            description='Build a text report showing the main classification '\n",
      "                        'metrics that are accuracy, precision, recall and F1.'),\n",
      " MetricInfo(id='buelfhood/fbeta_score',\n",
      "            space_id='buelfhood/fbeta_score',\n",
      "            description='Calculate FBeta_Score'),\n",
      " MetricInfo(id='bugbounty1806/accuracy',\n",
      "            space_id='bugbounty1806/accuracy',\n",
      "            description='Accuracy is the proportion of correct predictions '\n",
      "                        'among the total number of cases processed. It can be '\n",
      "                        'computed with: Accuracy = (TP + TN) / (TP + TN + FP + '\n",
      "                        'FN) Where: TP: True positive TN: True negative FP: '\n",
      "                        'False positive FN: False negative'),\n",
      " MetricInfo(id='carletoncognitivescience/peak_signal_to_noise_ratio',\n",
      "            space_id='carletoncognitivescience/peak_signal_to_noise_ratio',\n",
      "            description='Image quality metric'),\n",
      " MetricInfo(id='chanelcolgate/average_precision',\n",
      "            space_id='chanelcolgate/average_precision',\n",
      "            description='Average precision score.'),\n",
      " MetricInfo(id='chimene/accents_unplugged_eval',\n",
      "            space_id='chimene/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='christopher/ndcg',\n",
      "            space_id='christopher/ndcg',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='ckb/unigram',\n",
      "            space_id='ckb/unigram',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='codeparrot/apps_metric',\n",
      "            space_id='codeparrot/apps_metric',\n",
      "            description='Evaluation metric for the APPS benchmark'),\n",
      " MetricInfo(id='cpllab/syntaxgym',\n",
      "            space_id='cpllab/syntaxgym',\n",
      "            description='Evaluates Huggingface models on SyntaxGym datasets '\n",
      "                        '(targeted syntactic evaluations).'),\n",
      " MetricInfo(id='d-matrix/dmxMetric',\n",
      "            space_id='d-matrix/dmxMetric',\n",
      "            description='Evaluation function using lm-eval with d-Matrix '\n",
      "                        'integration. This function allows for the evaluation '\n",
      "                        'of language models across various tasks,  with the '\n",
      "                        'option to use d-Matrix compressed models. For more '\n",
      "                        'information, see '\n",
      "                        'https://github.com/EleutherAI/lm-evaluation-harness '\n",
      "                        'and https://github.com/d-matrix-ai/dmx-compressor'),\n",
      " MetricInfo(id='d-matrix/dmx_perplexity',\n",
      "            space_id='d-matrix/dmx_perplexity',\n",
      "            description='Perplexity metric implemented by d-Matrix. Perplexity '\n",
      "                        '(PPL) is one of the most common metrics for '\n",
      "                        'evaluating language models. It is defined as the '\n",
      "                        'exponentiated average negative log-likelihood of a '\n",
      "                        'sequence, calculated with exponent base `e`. Note '\n",
      "                        'that this metric is intended for Causual Language '\n",
      "                        'Models, the perplexity calculation is only correct if '\n",
      "                        'model uses Cross Entropy Loss. For more information, '\n",
      "                        'see '\n",
      "                        'https://huggingface.co/docs/transformers/perplexity'),\n",
      " MetricInfo(id='daiyizheng/valid',\n",
      "            space_id='daiyizheng/valid',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='danasone/ru_errant',\n",
      "            space_id='danasone/ru_errant',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='danieldux/hierarchical_softmax_loss',\n",
      "            space_id='danieldux/hierarchical_softmax_loss',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='danieldux/isco_hierarchical_accuracy',\n",
      "            space_id='danieldux/isco_hierarchical_accuracy',\n",
      "            description='The ISCO-08 Hierarchical Accuracy Measure is an '\n",
      "                        'implementation of the measure described in '\n",
      "                        '[Functional Annotation of Genes Using Hierarchical '\n",
      "                        'Text '\n",
      "                        'Categorization](https://www.researchgate.net/publication/44046343_Functional_Annotation_of_Genes_Using_Hierarchical_Text_Categorization) '\n",
      "                        '(Kiritchenko, Svetlana and Famili, Fazel. 2005) '\n",
      "                        'applied to the ISCO-08 classification scheme by the '\n",
      "                        'International Labour Organization.'),\n",
      " MetricInfo(id='dannashao/span_metric',\n",
      "            space_id='dannashao/span_metric',\n",
      "            description='This metric calculates both Token Overlap and Span '\n",
      "                        'Agreement precision, recall and f1 scores.'),\n",
      " MetricInfo(id='davebulaval/meaningbert',\n",
      "            space_id='davebulaval/meaningbert',\n",
      "            description='MeaningBERT is an automatic and trainable metric for '\n",
      "                        'assessing meaning preservation between sentences\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/GRAAL-Research/MeaningBERT/tree/main '\n",
      "                        'for more information.'),\n",
      " MetricInfo(id='dayil100/accents_unplugged_eval',\n",
      "            space_id='dayil100/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='dayil100/accents_unplugged_eval_WER',\n",
      "            space_id='dayil100/accents_unplugged_eval_WER',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='dgfh76564/accents_unplugged_eval',\n",
      "            space_id='dgfh76564/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='dotkaio/competition_math',\n",
      "            space_id='dotkaio/competition_math',\n",
      "            description='This metric is used to assess performance on the '\n",
      "                        'Mathematics Aptitude Test of Heuristics (MATH) '\n",
      "                        'dataset. It first canonicalizes the inputs (e.g., '\n",
      "                        'converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes '\n",
      "                        'accuracy.'),\n",
      " MetricInfo(id='dvitel/codebleu',\n",
      "            space_id='dvitel/codebleu',\n",
      "            description='CodeBLEU'),\n",
      " MetricInfo(id='ecody726/bertscore',\n",
      "            space_id='ecody726/bertscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='eirsteir/perplexity',\n",
      "            space_id='eirsteir/perplexity',\n",
      "            description='Perplexity (PPL) is one of the most common metrics '\n",
      "                        'for evaluating language models. It is defined as the '\n",
      "                        'exponentiated average negative log-likelihood of a '\n",
      "                        'sequence, calculated with exponent base `e`.\\n'\n",
      "                        'For more information on perplexity, see [this '\n",
      "                        'tutorial](https://huggingface.co/docs/transformers/perplexity).'),\n",
      " MetricInfo(id='erntkn/dice_coefficient',\n",
      "            space_id='erntkn/dice_coefficient',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='florentgbelidji/f1',\n",
      "            space_id='florentgbelidji/f1',\n",
      "            description='The F1 score is the harmonic mean of the precision '\n",
      "                        'and recall. It can be computed with the equation: F1 '\n",
      "                        '= 2 * (precision * recall) / (precision + recall)'),\n",
      " MetricInfo(id='fnvls/bleu1234',\n",
      "            space_id='fnvls/bleu1234',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='fnvls/bleu_1234',\n",
      "            space_id='fnvls/bleu_1234',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='franzi2505/detection_metric',\n",
      "            space_id='franzi2505/detection_metric',\n",
      "            description='Compute multiple object detection metrics at '\n",
      "                        'different bounding box area levels.'),\n",
      " MetricInfo(id='fschlatt/ner_eval',\n",
      "            space_id='fschlatt/ner_eval',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='gabeorlanski/bc_eval',\n",
      "            space_id='gabeorlanski/bc_eval',\n",
      "            description='This metric implements the evaluation harness for '\n",
      "                        'datasets translated with the BabelCode framework as '\n",
      "                        'described in the paper \"Measuring The Impact Of '\n",
      "                        'Programming Language Distribution\" '\n",
      "                        '(https://arxiv.org/abs/2302.01973).'),\n",
      " MetricInfo(id='ginic/phone_errors',\n",
      "            space_id='ginic/phone_errors',\n",
      "            description='Error rates in terms of distance between articulatory '\n",
      "                        'phonological features can help understand '\n",
      "                        'differences  between strings in the International '\n",
      "                        'Phonetic Alphabet (IPA) in a linguistically motivated '\n",
      "                        'way.  This is useful when evaluating speech '\n",
      "                        'recognition or orthographic to IPA conversion tasks.'),\n",
      " MetricInfo(id='giulio98/code_eval_outputs',\n",
      "            space_id='giulio98/code_eval_outputs',\n",
      "            description=None),\n",
      " MetricInfo(id='giulio98/codebleu',\n",
      "            space_id='giulio98/codebleu',\n",
      "            description='CodeBLEU metric for Python and C++'),\n",
      " MetricInfo(id='gjacob/bertimbauscore',\n",
      "            space_id='gjacob/bertimbauscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='gjacob/chrf',\n",
      "            space_id='gjacob/chrf',\n",
      "            description='ChrF and ChrF++ are two MT evaluation metrics. They '\n",
      "                        'both use the F-score statistic for character n-gram '\n",
      "                        'matches, and ChrF++ adds word n-grams as well which '\n",
      "                        'correlates more strongly with direct assessment. We '\n",
      "                        'use the implementation that is already present in '\n",
      "                        'sacrebleu.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#chrf--chrf for '\n",
      "                        'more information.'),\n",
      " MetricInfo(id='gjacob/google_bleu',\n",
      "            space_id='gjacob/google_bleu',\n",
      "            description='The BLEU score has some undesirable properties when '\n",
      "                        'used for single sentences, as it was designed to be a '\n",
      "                        'corpus measure. We therefore  use a slightly '\n",
      "                        'different score for our RL experiments which we call '\n",
      "                        \"the 'GLEU score'. For the GLEU score, we record all \"\n",
      "                        'sub-sequences of 1, 2, 3 or 4 tokens in output and '\n",
      "                        'target sequence (n-grams). We then compute a recall, '\n",
      "                        'which is the ratio of the number of matching n-grams '\n",
      "                        'to the number of total n-grams in the target (ground '\n",
      "                        'truth) sequence, and a precision, which is the ratio '\n",
      "                        'of the number of matching n-grams to the number of '\n",
      "                        'total n-grams in the generated output sequence. Then '\n",
      "                        'GLEU score is simply the minimum of recall and '\n",
      "                        \"precision. This GLEU score's range is always between \"\n",
      "                        '0 (no matches) and 1 (all match) and it is '\n",
      "                        'symmetrical when switching output and target. '\n",
      "                        'According to our experiments, GLEU score correlates '\n",
      "                        'quite well with the BLEU metric on a corpus level but '\n",
      "                        'does not have its drawbacks for our per sentence '\n",
      "                        'reward objective.'),\n",
      " MetricInfo(id='gjacob/wiki_split',\n",
      "            space_id='gjacob/wiki_split',\n",
      "            description='WIKI_SPLIT is the combination of three metrics SARI, '\n",
      "                        'EXACT and SACREBLEU It can be used to evaluate the '\n",
      "                        'quality of machine-generated texts.'),\n",
      " MetricInfo(id='gnail/cosine_similarity',\n",
      "            space_id='gnail/cosine_similarity',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='gorkaartola/metric_for_tp_fp_samples',\n",
      "            space_id='gorkaartola/metric_for_tp_fp_samples',\n",
      "            description='This metric is specially designed to measure the '\n",
      "                        'performance of sentence classification models over '\n",
      "                        'multiclass test datasets containing both True '\n",
      "                        'Positive samples, meaning that the label associated '\n",
      "                        'to the sentence in the sample is correctly assigned, '\n",
      "                        'and False Positive samples, meaning that the label '\n",
      "                        'associated to the sentence in the sample is '\n",
      "                        'incorrectly assigned.'),\n",
      " MetricInfo(id='guydav/restrictedpython_code_eval',\n",
      "            space_id='guydav/restrictedpython_code_eval',\n",
      "            description='Same logic as the built-in `code_eval`, but compiling '\n",
      "                        'and running the code using `RestrictedPython`'),\n",
      " MetricInfo(id='hack/test_metric',\n",
      "            space_id='hack/test_metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='haotongye-shopee/ppl',\n",
      "            space_id='haotongye-shopee/ppl',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='harshhpareek/bertscore',\n",
      "            space_id='harshhpareek/bertscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='helena-balabin/youden_index',\n",
      "            space_id='helena-balabin/youden_index',\n",
      "            description='Youden index for finding the ideal threshold in an '\n",
      "                        'ROC AUC curve'),\n",
      " MetricInfo(id='hemulitch/cer',\n",
      "            space_id='hemulitch/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='hpi-dhc/FairEval',\n",
      "            space_id='hpi-dhc/FairEval',\n",
      "            description='Fair Evaluation for Squence labeling'),\n",
      " MetricInfo(id='huanghuayu/multiclass_brier_score',\n",
      "            space_id='huanghuayu/multiclass_brier_score',\n",
      "            description='brier_score metric for multiclass problem.'),\n",
      " MetricInfo(id='hynky/sklearn_proxy',\n",
      "            space_id='hynky/sklearn_proxy',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='hyperml/balanced_accuracy',\n",
      "            space_id='hyperml/balanced_accuracy',\n",
      "            description='Balanced Accuracy is the average of recall obtained '\n",
      "                        'on each class. It can be computed with: Balanced '\n",
      "                        'Accuracy = (TPR + TNR) / N Where: TPR: True positive '\n",
      "                        'rate TNR: True negative rate N: Number of classes'),\n",
      " MetricInfo(id='iNeil77/code_eval_octopack',\n",
      "            space_id='iNeil77/code_eval_octopack',\n",
      "            description='This metric implements code evaluation with execution '\n",
      "                        'across multiple languages as used in the paper '\n",
      "                        '\"OctoPack: Instruction Tuning Code Large Language '\n",
      "                        'Models\" (https://arxiv.org/abs/2308.07124).'),\n",
      " MetricInfo(id='idsedykh/codebleu',\n",
      "            space_id='idsedykh/codebleu',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='idsedykh/codebleu2',\n",
      "            space_id='idsedykh/codebleu2',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='idsedykh/megaglue',\n",
      "            space_id='idsedykh/megaglue',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='idsedykh/metric',\n",
      "            space_id='idsedykh/metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='illorca/FairEval',\n",
      "            space_id='illorca/FairEval',\n",
      "            description='Fair Evaluation for Squence labeling'),\n",
      " MetricInfo(id='ingyu/klue_mrc',\n",
      "            space_id='ingyu/klue_mrc',\n",
      "            description='This metric wrap the unofficial scoring script for '\n",
      "                        '[Machine Machine Reading Comprehension task of Korean '\n",
      "                        'Language Understanding Evaluation '\n",
      "                        '(KLUE-MRC)](https://huggingface.co/datasets/klue/viewer/mrc/train).\\n'\n",
      "                        'KLUE-MRC is a Korean reading comprehension dataset '\n",
      "                        'consisting of questions where the answer to every '\n",
      "                        'question is a segment of text, or span, from the '\n",
      "                        'corresponding reading passage, or the question might '\n",
      "                        'be unanswerable.\\n'\n",
      "                        'As KLUE-MRC has the same task format as SQuAD 2.0, '\n",
      "                        'this evaluation script uses the same metrics of SQuAD '\n",
      "                        '2.0 (F1 and EM).\\n'\n",
      "                        'KLUE-MRC consists of 12,286 question paraphrasing, '\n",
      "                        '7,931 multi-sentence reasoning, and 9,269 '\n",
      "                        'unanswerable questions. Totally, 29,313 examples are '\n",
      "                        'made with 22,343 documents and 23,717 passages.'),\n",
      " MetricInfo(id='jialinsong/apps_metric',\n",
      "            space_id='jialinsong/apps_metric',\n",
      "            description='Evaluation metric for the APPS benchmark'),\n",
      " MetricInfo(id='jijihuny/ecqa',\n",
      "            space_id='jijihuny/ecqa',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='jjkim0807/code_eval',\n",
      "            space_id='jjkim0807/code_eval',\n",
      "            description='This metric implements the evaluation harness for the '\n",
      "                        'HumanEval problem solving dataset described in the '\n",
      "                        'paper \"Evaluating Large Language Models Trained on '\n",
      "                        'Code\" (https://arxiv.org/abs/2107.03374).'),\n",
      " MetricInfo(id='jordyvl/ece',\n",
      "            space_id='jordyvl/ece',\n",
      "            description='binned estimator of expected calibration error'),\n",
      " MetricInfo(id='jpxkqx/peak_signal_to_noise_ratio',\n",
      "            space_id='jpxkqx/peak_signal_to_noise_ratio',\n",
      "            description='Image quality metric'),\n",
      " MetricInfo(id='jpxkqx/signal_to_reconstruction_error',\n",
      "            space_id='jpxkqx/signal_to_reconstruction_error',\n",
      "            description='Signal-to-Reconstruction Error'),\n",
      " MetricInfo(id='juliakaczor/accents_unplugged_eval',\n",
      "            space_id='juliakaczor/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='jzm-mailchimp/joshs_second_test_metric',\n",
      "            space_id='jzm-mailchimp/joshs_second_test_metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='k4black/codebleu',\n",
      "            space_id='k4black/codebleu',\n",
      "            description='Unofficial `CodeBLEU` implementation that supports '\n",
      "                        'Linux, MacOS and Windows.'),\n",
      " MetricInfo(id='kashif/mape',\n",
      "            space_id='kashif/mape',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='kbmlcoding/apps_metric',\n",
      "            space_id='kbmlcoding/apps_metric',\n",
      "            description='Evaluation metric for the APPS benchmark'),\n",
      " MetricInfo(id='kedudzic/charmatch',\n",
      "            space_id='kedudzic/charmatch',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='kgorman2205/accuracy',\n",
      "            space_id='kgorman2205/accuracy',\n",
      "            description='Accuracy is the proportion of correct predictions '\n",
      "                        'among the total number of cases processed. It can be '\n",
      "                        'computed with: Accuracy = (TP + TN) / (TP + TN + FP + '\n",
      "                        'FN) Where: TP: True positive TN: True negative FP: '\n",
      "                        'False positive FN: False negative'),\n",
      " MetricInfo(id='kyokote/my_metric2',\n",
      "            space_id='kyokote/my_metric2',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='langdonholmes/cohen_weighted_kappa',\n",
      "            space_id='langdonholmes/cohen_weighted_kappa',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='leslyarun/fbeta_score',\n",
      "            space_id='leslyarun/fbeta_score',\n",
      "            description='Calculate FBeta_Score'),\n",
      " MetricInfo(id='lhy/hamming_loss',\n",
      "            space_id='lhy/hamming_loss',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='lhy/ranking_loss',\n",
      "            space_id='lhy/ranking_loss',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='livvie/accents_unplugged_eval',\n",
      "            space_id='livvie/accents_unplugged_eval',\n",
      "            description='Word error rate (WER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'The general difficulty of measuring performance lies '\n",
      "                        'in the fact that the recognized word sequence can '\n",
      "                        'have a different length from the reference word '\n",
      "                        'sequence (supposedly the correct one). The WER is '\n",
      "                        'derived from the Levenshtein distance, working at the '\n",
      "                        'word level instead of the phoneme level. The WER is a '\n",
      "                        'valuable tool for comparing different systems as well '\n",
      "                        'as for evaluating improvements within one system. '\n",
      "                        'This kind of measurement, however, provides no '\n",
      "                        'details on the nature of translation errors and '\n",
      "                        'further work is therefore required to identify the '\n",
      "                        'main source(s) of error and to focus any research '\n",
      "                        'effort.\\n'\n",
      "                        'This problem is solved by first aligning the '\n",
      "                        'recognized word sequence with the reference (spoken) '\n",
      "                        'word sequence using dynamic string alignment. '\n",
      "                        'Examination of this issue is seen through a theory '\n",
      "                        'called the power law that states the correlation '\n",
      "                        'between perplexity and word error rate.\\n'\n",
      "                        'Word error rate can then be computed as:\\n'\n",
      "                        'WER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct words, N is the number of words in '\n",
      "                        'the reference (N=S+D+C).\\n'\n",
      "                        'This value indicates the average number of errors per '\n",
      "                        'reference word. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a WER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='loubnabnl/apps_metric2',\n",
      "            space_id='loubnabnl/apps_metric2',\n",
      "            description='Evaluation metric for the APPS benchmark'),\n",
      " MetricInfo(id='lvwerra/accuracy_score',\n",
      "            space_id='lvwerra/accuracy_score',\n",
      "            description='\"Accuracy classification score.\"'),\n",
      " MetricInfo(id='lvwerra/bary_score',\n",
      "            space_id='lvwerra/bary_score',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='lvwerra/test', space_id='lvwerra/test', description=None),\n",
      " MetricInfo(id='maksymdolgikh/seqeval_with_fbeta',\n",
      "            space_id='maksymdolgikh/seqeval_with_fbeta',\n",
      "            description='seqeval is a Python framework for sequence labeling '\n",
      "                        'evaluation. seqeval can evaluate the performance of '\n",
      "                        'chunking tasks such as named-entity recognition, '\n",
      "                        'part-of-speech tagging, semantic role labeling and so '\n",
      "                        'on.\\n'\n",
      "                        'This is well-tested by using the Perl script '\n",
      "                        'conlleval, which can be used for measuring the '\n",
      "                        'performance of a system that has processed the '\n",
      "                        'CoNLL-2000 shared task data.\\n'\n",
      "                        'seqeval supports following formats: IOB1 IOB2 IOE1 '\n",
      "                        'IOE2 IOBES\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/chakki-works/seqeval for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='manueldeprada/beer',\n",
      "            space_id='manueldeprada/beer',\n",
      "            description='BEER 2.0 (BEtter Evaluation as Ranking) is a trained '\n",
      "                        'machine translation evaluation metric with high '\n",
      "                        'correlation with human judgment both on sentence and '\n",
      "                        'corpus level. It is a linear model-based metric for '\n",
      "                        'sentence-level evaluation in machine translation (MT) '\n",
      "                        'that combines 33 relatively dense features, including '\n",
      "                        'character n-grams and reordering features. It employs '\n",
      "                        'a learning-to-rank framework to differentiate between '\n",
      "                        'function and non-function words and weighs each word '\n",
      "                        'type according to its importance for evaluation. The '\n",
      "                        'model is trained on ranking similar translations '\n",
      "                        'using a vector of feature values for each system '\n",
      "                        'output. BEER outperforms the strong baseline metric '\n",
      "                        'METEOR in five out of eight language pairs, showing '\n",
      "                        'that less sparse features at the sentence level can '\n",
      "                        'lead to state-of-the-art results. Features on '\n",
      "                        'character n-grams are crucial, and higher-order '\n",
      "                        'character n-grams are less prone to sparse counts '\n",
      "                        'than word n-grams.'),\n",
      " MetricInfo(id='maysonma/lingo_judge_metric',\n",
      "            space_id='maysonma/lingo_judge_metric',\n",
      "            description=None),\n",
      " MetricInfo(id='medmac01/bertscore-eval',\n",
      "            space_id='medmac01/bertscore-eval',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='mfumanelli/geometric_mean',\n",
      "            space_id='mfumanelli/geometric_mean',\n",
      "            description='The geometric mean (G-mean) is the root of the '\n",
      "                        'product of class-wise sensitivity.  '),\n",
      " MetricInfo(id='mgfrantz/roc_auc_macro',\n",
      "            space_id='mgfrantz/roc_auc_macro',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='mlcore/arxiv_score',\n",
      "            space_id='mlcore/arxiv_score',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='mtc/fragments',\n",
      "            space_id='mtc/fragments',\n",
      "            description='Fragments computes the extractiveness between source '\n",
      "                        'articles and their summaries. The metric computes two '\n",
      "                        'scores: coverage and density.  The code is adapted '\n",
      "                        'from the newsroom '\n",
      "                        'package(https://github.com/lil-lab/newsroom/blob/master/newsroom/analyze/fragments.py). '\n",
      "                        'All credits goes to the authors of aforementioned '\n",
      "                        'code.'),\n",
      " MetricInfo(id='nevikw39/specificity',\n",
      "            space_id='nevikw39/specificity',\n",
      "            description='Specificity is the fraction of the negatives examples '\n",
      "                        'that were correctly labeled by the model as '\n",
      "                        'negatives. It can be computed with the equation: '\n",
      "                        'Specificity = TN / (TN + FP) Where TN is the true '\n",
      "                        'negatives and FP is the false positives.'),\n",
      " MetricInfo(id='nlpln/tst',\n",
      "            space_id='nlpln/tst',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='nrmoolsarn/cer',\n",
      "            space_id='nrmoolsarn/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='ola13/precision_at_k',\n",
      "            space_id='ola13/precision_at_k',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='omidf/squad_precision_recall',\n",
      "            space_id='omidf/squad_precision_recall',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 1 of the Stanford Question Answering Dataset '\n",
      "                        '(SQuAD).\\n'\n",
      "                        'Stanford Question Answering Dataset (SQuAD) is a '\n",
      "                        'reading comprehension dataset, consisting of '\n",
      "                        'questions posed by crowdworkers on a set of Wikipedia '\n",
      "                        'articles, where the answer to every question is a '\n",
      "                        'segment of text, or span, from the corresponding '\n",
      "                        'reading passage, or the question might be '\n",
      "                        'unanswerable.'),\n",
      " MetricInfo(id='pirxus/sacrebleu',\n",
      "            space_id='pirxus/sacrebleu',\n",
      "            description='SacreBLEU provides hassle-free computation of '\n",
      "                        'shareable, comparable, and reproducible BLEU scores. '\n",
      "                        \"Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \"\n",
      "                        'it produces the official WMT scores but works with '\n",
      "                        'plain text. It also knows all the standard test sets '\n",
      "                        'and handles downloading, processing, and tokenization '\n",
      "                        'for you.\\n'\n",
      "                        'See the [README.md] file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='posicube/mean_reciprocal_rank',\n",
      "            space_id='posicube/mean_reciprocal_rank',\n",
      "            description='Mean Reciprocal Rank is a statistic measure for '\n",
      "                        'evaluating any process that produces a list of '\n",
      "                        'possible responses to a sample of queries, ordered by '\n",
      "                        'probability of correctness.'),\n",
      " MetricInfo(id='prajwall/mse',\n",
      "            space_id='prajwall/mse',\n",
      "            description='Mean Squared Error(MSE) is the average of the square '\n",
      "                        'of difference between the predicted and actual '\n",
      "                        'values.'),\n",
      " MetricInfo(id='red1bluelost/evaluate_genericify_cpp',\n",
      "            space_id='red1bluelost/evaluate_genericify_cpp',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='repllabs/mean_average_precision',\n",
      "            space_id='repllabs/mean_average_precision',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='repllabs/mean_reciprocal_rank',\n",
      "            space_id='repllabs/mean_reciprocal_rank',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='ronaldahmed/nwentfaithfulness',\n",
      "            space_id='ronaldahmed/nwentfaithfulness',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='saicharan2804/my_metric',\n",
      "            space_id='saicharan2804/my_metric',\n",
      "            description='Moses and PyTDC metrics'),\n",
      " MetricInfo(id='sakusakumura/bertscore',\n",
      "            space_id='sakusakumura/bertscore',\n",
      "            description='BERTScore leverages the pre-trained contextual '\n",
      "                        'embeddings from BERT and matches words in candidate '\n",
      "                        'and reference sentences by cosine similarity. It has '\n",
      "                        'been shown to correlate with human judgment on '\n",
      "                        'sentence-level and system-level evaluation. Moreover, '\n",
      "                        'BERTScore computes precision, recall, and F1 measure, '\n",
      "                        'which can be useful for evaluating different language '\n",
      "                        'generation tasks.\\n'\n",
      "                        \"See the project's README at \"\n",
      "                        'https://github.com/Tiiiger/bert_score#readme for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='shalakasatheesh/squad',\n",
      "            space_id='shalakasatheesh/squad',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 1 of the Stanford Question Answering Dataset '\n",
      "                        '(SQuAD).\\n'\n",
      "                        'Stanford Question Answering Dataset (SQuAD) is a '\n",
      "                        'reading comprehension dataset, consisting of '\n",
      "                        'questions posed by crowdworkers on a set of Wikipedia '\n",
      "                        'articles, where the answer to every question is a '\n",
      "                        'segment of text, or span, from the corresponding '\n",
      "                        'reading passage, or the question might be '\n",
      "                        'unanswerable.'),\n",
      " MetricInfo(id='shalakasatheesh/squad_v2',\n",
      "            space_id='shalakasatheesh/squad_v2',\n",
      "            description='This metric wrap the official scoring script for '\n",
      "                        'version 2 of the Stanford Question Answering Dataset '\n",
      "                        '(SQuAD).\\n'\n",
      "                        'Stanford Question Answering Dataset (SQuAD) is a '\n",
      "                        'reading comprehension dataset, consisting of '\n",
      "                        'questions posed by crowdworkers on a set of Wikipedia '\n",
      "                        'articles, where the answer to every question is a '\n",
      "                        'segment of text, or span, from the corresponding '\n",
      "                        'reading passage, or the question might be '\n",
      "                        'unanswerable.\\n'\n",
      "                        'SQuAD2.0 combines the 100,000 questions in SQuAD1.1 '\n",
      "                        'with over 50,000 unanswerable questions  written '\n",
      "                        'adversarially by crowdworkers to look similar to '\n",
      "                        'answerable ones. To do well on SQuAD2.0, systems must '\n",
      "                        'not only answer questions when possible, but also '\n",
      "                        'determine when no answer is supported by the '\n",
      "                        'paragraph and abstain from answering.'),\n",
      " MetricInfo(id='shirayukikun/sescore',\n",
      "            space_id='shirayukikun/sescore',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='shunzh/apps_metric',\n",
      "            space_id='shunzh/apps_metric',\n",
      "            description='Evaluation metric for the APPS benchmark'),\n",
      " MetricInfo(id='sma2023/wil', space_id='sma2023/wil', description=None),\n",
      " MetricInfo(id='sportlosos/sescore',\n",
      "            space_id='sportlosos/sescore',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='svenwey/logmetric',\n",
      "            space_id='svenwey/logmetric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='tianzhihui-isc/cer',\n",
      "            space_id='tianzhihui-isc/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='transZ/sbert_cosine',\n",
      "            space_id='transZ/sbert_cosine',\n",
      "            description='Sbert cosine is a metric to score the semantic '\n",
      "                        'similarity of text generation tasks\\n'\n",
      "                        'This is not the official implementation of cosine '\n",
      "                        'similarity using SBERT\\n'\n",
      "                        'See the project at https://www.sbert.net/ for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='transZ/test_parascore',\n",
      "            space_id='transZ/test_parascore',\n",
      "            description='ParaScore is a new metric to scoring the performance '\n",
      "                        'of paraphrase generation tasks\\n'\n",
      "                        'See the project at '\n",
      "                        'https://github.com/shadowkiller33/ParaScore for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='transformersegmentation/segmentation_scores',\n",
      "            space_id='transformersegmentation/segmentation_scores',\n",
      "            description=' metric for word segmentation scores '),\n",
      " MetricInfo(id='unnati/kendall_tau_distance',\n",
      "            space_id='unnati/kendall_tau_distance',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='venkatasg/gleu',\n",
      "            space_id='venkatasg/gleu',\n",
      "            description='Generalized Language Evaluation Understanding (GLEU) '\n",
      "                        'is a metric initially developed for Grammatical Error '\n",
      "                        'Correction (GEC), that builds upon BLEU by rewarding '\n",
      "                        'corrections while also correctly crediting unchanged '\n",
      "                        'source text.'),\n",
      " MetricInfo(id='vichyt/metric-codebleu',\n",
      "            space_id='vichyt/metric-codebleu',\n",
      "            description='Unofficial `CodeBLEU` implementation that supports '\n",
      "                        'Linux and MacOS. It is only available for python at '\n",
      "                        'the feature website.'),\n",
      " MetricInfo(id='vineelpratap/cer',\n",
      "            space_id='vineelpratap/cer',\n",
      "            description='Character error rate (CER) is a common metric of the '\n",
      "                        'performance of an automatic speech recognition '\n",
      "                        'system.\\n'\n",
      "                        'CER is similar to Word Error Rate (WER), but operates '\n",
      "                        'on character instead of word. Please refer to docs of '\n",
      "                        'WER for further information.\\n'\n",
      "                        'Character error rate can be computed as:\\n'\n",
      "                        'CER = (S + D + I) / N = (S + D + I) / (S + D + C)\\n'\n",
      "                        'where\\n'\n",
      "                        'S is the number of substitutions, D is the number of '\n",
      "                        'deletions, I is the number of insertions, C is the '\n",
      "                        'number of correct characters, N is the number of '\n",
      "                        'characters in the reference (N=S+D+C).\\n'\n",
      "                        \"CER's output is not always a number between 0 and 1, \"\n",
      "                        'in particular when there is a high number of '\n",
      "                        'insertions. This value is often associated to the '\n",
      "                        'percentage of characters that were incorrectly '\n",
      "                        'predicted. The lower the value, the better the '\n",
      "                        'performance of the ASR system with a CER of 0 being a '\n",
      "                        'perfect score.'),\n",
      " MetricInfo(id='vladman-25/ter',\n",
      "            space_id='vladman-25/ter',\n",
      "            description='TER (Translation Edit Rate, also called Translation '\n",
      "                        'Error Rate) is a metric to quantify the edit '\n",
      "                        'operations that a hypothesis requires to match a '\n",
      "                        'reference translation. We use the implementation that '\n",
      "                        'is already present in sacrebleu '\n",
      "                        '(https://github.com/mjpost/sacreBLEU#ter), which in '\n",
      "                        'turn is inspired by the TERCOM implementation, which '\n",
      "                        'can be found here: '\n",
      "                        'https://github.com/jhclark/tercom.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#ter for more '\n",
      "                        'information.'),\n",
      " MetricInfo(id='weiqis/pajm',\n",
      "            space_id='weiqis/pajm',\n",
      "            description='a metric module to do Partial Answer & Justification '\n",
      "                        'Match (pajm).'),\n",
      " MetricInfo(id='whyen-wang/cocoeval',\n",
      "            space_id='whyen-wang/cocoeval',\n",
      "            description='COCO eval'),\n",
      " MetricInfo(id='whyen-wang/my_metric',\n",
      "            space_id='whyen-wang/my_metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='xu1998hz/sescore',\n",
      "            space_id='xu1998hz/sescore',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='xu1998hz/sescore_english_coco',\n",
      "            space_id='xu1998hz/sescore_english_coco',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='xu1998hz/sescore_english_mt',\n",
      "            space_id='xu1998hz/sescore_english_mt',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='xu1998hz/sescore_english_webnlg',\n",
      "            space_id='xu1998hz/sescore_english_webnlg',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='xu1998hz/sescore_german_mt',\n",
      "            space_id='xu1998hz/sescore_german_mt',\n",
      "            description='SEScore: a text generation evaluation metric'),\n",
      " MetricInfo(id='ybelkada/cocoevaluate',\n",
      "            space_id='ybelkada/cocoevaluate',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='yonting/average_precision_score',\n",
      "            space_id='yonting/average_precision_score',\n",
      "            description='Average precision score.'),\n",
      " MetricInfo(id='yqsong/execution_accuracy',\n",
      "            space_id='yqsong/execution_accuracy',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='yulong-me/yl_metric',\n",
      "            space_id='yulong-me/yl_metric',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='yuyijiong/quad_match_score',\n",
      "            space_id='yuyijiong/quad_match_score',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='yzha/ctc_eval',\n",
      "            space_id='yzha/ctc_eval',\n",
      "            description='This repo contains code of an automatic evaluation '\n",
      "                        'metric described in the paper Compression, '\n",
      "                        'Transduction, and Creation: A Unified Framework for '\n",
      "                        'Evaluating Natural Language Generation'),\n",
      " MetricInfo(id='zbeloki/m2',\n",
      "            space_id='zbeloki/m2',\n",
      "            description='TODO: add a description here'),\n",
      " MetricInfo(id='zsqrt/ter',\n",
      "            space_id='zsqrt/ter',\n",
      "            description='TER (Translation Edit Rate, also called Translation '\n",
      "                        'Error Rate) is a metric to quantify the edit '\n",
      "                        'operations that a hypothesis requires to match a '\n",
      "                        'reference translation. We use the implementation that '\n",
      "                        'is already present in sacrebleu '\n",
      "                        '(https://github.com/mjpost/sacreBLEU#ter), which in '\n",
      "                        'turn is inspired by the TERCOM implementation, which '\n",
      "                        'can be found here: '\n",
      "                        'https://github.com/jhclark/tercom.\\n'\n",
      "                        'The implementation here is slightly different from '\n",
      "                        'sacrebleu in terms of the required input format. The '\n",
      "                        'length of the references and hypotheses lists need to '\n",
      "                        'be the same, so you may need to transpose your '\n",
      "                        \"references compared to sacrebleu's required input \"\n",
      "                        'format. See '\n",
      "                        'https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\\n'\n",
      "                        'See the README.md file at '\n",
      "                        'https://github.com/mjpost/sacreBLEU#ter for more '\n",
      "                        'information.')]\n"
     ]
    }
   ],
   "source": [
    "# 查看账户信息\n",
    "\n",
    "# 账户登录与注销\n",
    "# huggingface_hub.login()\n",
    "# huggingface_hub.logout()\n",
    "pprint.pprint(huggingface_hub.whoami())\n",
    "\n",
    "# 写入仓库\n",
    "# huggingface_hub.create_repo()\n",
    "# huggingface_hub.delete_repo()\n",
    "# huggingface_hub.update_repo_visibility()\n",
    "\n",
    "# 读取所有公开仓库\n",
    "pprint.pprint(huggingface_hub.list_models())\n",
    "pprint.pprint(huggingface_hub.list_datasets())\n",
    "pprint.pprint(huggingface_hub.list_metrics())\n",
    "# huggingface_hub.list_repo_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo本身也可以视为一个对象\n",
    "\n",
    "repo = huggingface_hub.Repository(\n",
    "    local_dir=\"本地目录\",\n",
    "    clone_from=\"<USERNAME>/<REPONAME>\"\n",
    ")\n",
    "\n",
    "repo.git_pull()\n",
    "repo.git_add()\n",
    "repo.git_commit()\n",
    "repo.git_push()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
