{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §2.1 预处理/模型推理/后处理\n",
    "`AutoTokenizer`/`AutoModel`/`torch.nn.Functional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], device='cuda:0'),\n",
      " 'input_ids': tensor([[ 101, 1045, 2293, 2017,  102],\n",
      "        [ 101, 1045, 5223, 2017,  102]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "raw_text = [\n",
    "    \"I love you\",\n",
    "    \"I hate you\"\n",
    "]\n",
    "token = tokenizer(raw_text, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "# return_tensors == 'pt', PyTorch\n",
    "# return_tensors == 'tf', TensorFlow\n",
    "# return_tensors == 'np', NumPy\n",
    "\n",
    "pprint.pprint(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': BaseModelOutput(last_hidden_state=tensor([[[ 0.5110,  0.4176, -0.0830,  ...,  0.4443,  0.8320, -0.3374],\n",
      "         [ 0.6341,  0.4539, -0.0347,  ...,  0.3301,  0.8103, -0.2437],\n",
      "         [ 0.8464,  0.5750,  0.1138,  ...,  0.2452,  0.7705, -0.3324],\n",
      "         [ 0.5107,  0.3850,  0.1263,  ...,  0.2038,  0.8559, -0.3721],\n",
      "         [ 1.2287,  0.2805,  0.3941,  ...,  0.6246,  0.4263, -0.8306]],\n",
      "\n",
      "        [[-0.3473,  0.8337, -0.4560,  ..., -0.2185, -0.7385, -0.0927],\n",
      "         [-0.1072,  1.1280, -0.4087,  ..., -0.4267, -0.4758,  0.1905],\n",
      "         [-0.0195,  1.0290, -0.4657,  ..., -0.4203, -0.5142,  0.1062],\n",
      "         [-0.3486,  0.6474, -0.3795,  ..., -0.1535, -0.6559, -0.2947],\n",
      "         [ 0.1300,  0.2511, -0.3323,  ..., -0.0435, -0.5050, -0.2333]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
      "                            hidden_states=None,\n",
      "                            attentions=None),\n",
      " 'feature_shape': torch.Size([2, 5, 768])}\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModel.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ").to(\"cuda:0\")\n",
    "feature = model(**token)\n",
    "pprint.pprint({\n",
    "    \"feature\": feature,\n",
    "    \"feature_shape\": feature.last_hidden_state.shape, # batch_size × sequence_length × hidden_size(encoded_size)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': SequenceClassifierOutput(loss=None,\n",
      "                                     logits=tensor([[-4.2756,  4.6393],\n",
      "        [ 3.8724, -3.1543]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                     hidden_states=None,\n",
      "                                     attentions=None),\n",
      " 'logits_shape': torch.Size([2, 2]),\n",
      " 'model_label_map': {0: 'NEGATIVE', 1: 'POSITIVE'},\n",
      " 'softmax_probility': tensor([[1.3436e-04, 9.9987e-01],\n",
      "        [9.9911e-01, 8.8707e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    ").to(\"cuda:0\")\n",
    "feature = model(**token)\n",
    "pprint.pprint({\n",
    "    \"feature\": feature,\n",
    "    \"logits_shape\": feature.logits.shape,\n",
    "    \"softmax_probility\": torch.nn.functional.softmax(feature.logits, dim=-1),\n",
    "    \"model_label_map\": model.config.id2label\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §2.2 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      ",\n",
      " 'model': BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# 用随机参数创建一个BertModel\n",
    "config = transformers.BertConfig()\n",
    "model = transformers.BertModel(config).to(\"cuda:0\")\n",
    "pprint.pprint({\n",
    "    \"config\": config,\n",
    "    \"model\": model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# 用预训练参数创建BertModel\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-cased\")\n",
    "pprint.pprint({\n",
    "    \"model\": model\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §2.3 分词器(Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_str': '[CLS] Using a Transformer network is simple [SEP]',\n",
      " 'token_id': [7993, 170, 13809, 23763, 2443, 1110, 3014],\n",
      " 'token_list': ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yaner\\Desktop\\Thoughts\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer输出结构\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 第一种方法：先把str拆成list[str]，再使用map转为list[int]\n",
    "token_list = tokenizer.tokenize(\"Using a Transformer network is simple\")\n",
    "token_id = tokenizer.convert_tokens_to_ids(token_list)\n",
    "\n",
    "# 第二种方法：一步到位，tokenizer.__call__(str)\n",
    "token_info = tokenizer(\"Using a Transformer network is simple\")\n",
    "raw_str = tokenizer.decode(token_info[\"input_ids\"])\n",
    "\n",
    "pprint.pprint({\n",
    "    \"token_list\": token_list,\n",
    "    \"token_id\": token_id,\n",
    "    \"raw_str\": raw_str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': SequenceClassifierOutput(loss=None,\n",
      "                                    logits=tensor([[-2.7276,  2.8789]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                    hidden_states=None,\n",
      "                                    attentions=None)}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer与Model结合\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(\"cuda:0\")\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "token_list: list[str] = tokenizer.tokenize(sequence)\n",
    "token_id = tokenizer.convert_tokens_to_ids(token_list)\n",
    "token_id_tensor: torch.Tensor = torch.tensor([token_id]).to(\"cuda:0\") # model()接受batch_size × sequence_length的Tensor，所以要升维[token_id]\n",
    "\n",
    "output = model(token_id_tensor)\n",
    "pprint.pprint({\n",
    "    \"output\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contextual_masked_padding': SequenceClassifierOutput(loss=None,\n",
      "                                                       logits=tensor([[ 0.5803, -0.4125],\n",
      "        [ 0.5803, -0.4125]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                                       hidden_states=None,\n",
      "                                                       attentions=None),\n",
      " 'contextual_padding': SequenceClassifierOutput(loss=None,\n",
      "                                                logits=tensor([[ 1.3373, -1.2163]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                                hidden_states=None,\n",
      "                                                attentions=None),\n",
      " 'manual_padding': SequenceClassifierOutput(loss=None,\n",
      "                                            logits=tensor([[ 1.3373, -1.2163]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                            hidden_states=None,\n",
      "                                            attentions=None),\n",
      " 'unpadding': SequenceClassifierOutput(loss=None,\n",
      "                                       logits=tensor([[ 0.5803, -0.4125]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
      "                                       hidden_states=None,\n",
      "                                       attentions=None)}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer填充非等长序列 + 注意力掩码\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(\"cuda:0\")\n",
    "\n",
    "pprint.pprint({\n",
    "    \"unpadding\": model(torch.tensor(\n",
    "        [[200, 200]]\n",
    "    ).cuda()),\n",
    "    \"manual_padding\": model(torch.tensor(\n",
    "        [[200, 200, 0]]\n",
    "    ).cuda()),\n",
    "    \"contextual_padding\": model(torch.tensor(\n",
    "        [[200, 200, tokenizer.pad_token_id]]\n",
    "    ).cuda()),\n",
    "    \"contextual_masked_padding\": model(torch.tensor(\n",
    "        [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]\n",
    "    ).cuda(), attention_mask=torch.tensor([[1, 1, 0], [1, 1, 0]]).cuda())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_info_1': torch.Size([3, 42]),\n",
      " 'token_info_2': torch.Size([3, 512]),\n",
      " 'token_info_3': torch.Size([3, 1024])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer的填充策略\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sequences = [\n",
    "    \"this is a word \" * 10,\n",
    "    \"this is a word \" * 5,\n",
    "    \"this is a word \" * 2,\n",
    "]\n",
    "\n",
    "token_info_1 = tokenizer(sequences, return_tensors=\"pt\", padding=\"longest\") # 取各序列长度的最大值\n",
    "token_info_2 = tokenizer(sequences, return_tensors=\"pt\", padding=\"max_length\") # Tokenizer模型的最长长度（此处使用DistilBert，最长为512）\n",
    "token_info_3 = tokenizer(sequences, return_tensors=\"pt\", padding=\"max_length\", max_length=1024) # 人为指定最长长度\n",
    "\n",
    "pprint.pprint({\n",
    "    \"token_info_1\": token_info_1[\"input_ids\"].shape,\n",
    "    \"token_info_2\": token_info_2[\"input_ids\"].shape,\n",
    "    \"token_info_3\": token_info_3[\"input_ids\"].shape\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_info_1': torch.Size([3, 42]), 'token_info_2': torch.Size([3, 32])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer的序列截断\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sequences = [\n",
    "    \"this is a word \" * 10,\n",
    "    \"this is a word \" * 5,\n",
    "    \"this is a word \" * 2,\n",
    "]\n",
    "\n",
    "token_info_1 = tokenizer(sequences, return_tensors=\"pt\", padding=True, truncation=True) # Tokenizer模型的最长长度（此处使用DistilBert，最长为512）\n",
    "token_info_2 = tokenizer(sequences, return_tensors=\"pt\", padding=True, truncation=True, max_length=32) # 人为指定最长长度\n",
    "\n",
    "pprint.pprint({\n",
    "    \"token_info_1\": token_info_1[\"input_ids\"].shape,\n",
    "    \"token_info_2\": token_info_2[\"input_ids\"].shape\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tokenizer.__call__()': \"[CLS] hello, i'm here! [SEP]\",\n",
      " 'Tokenizer.tokenize()': \"hello, i'm here!\"}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer.__call__()会在起始和结尾添加标记，但是Tokenizer.tokenize()不会\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "raw_text = \"Hello, I'm here!\"\n",
    "\n",
    "pprint.pprint({\n",
    "    \"Tokenizer.__call__()\": tokenizer.decode(tokenizer(raw_text)[\"input_ids\"]),\n",
    "    \"Tokenizer.tokenize()\": tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(raw_text)))\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
