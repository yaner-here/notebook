<u>**正文七页，参考文献两页！**</u>

部分-参考字数：

- [ ] Abstract：1570字符
- [ ] Introduction：4912字符
- [x] Related Work：3714字符
- [x] Database：4461字符
- [ ] Method：4622字符
- [ ] Experiment：9613字符
- [x] Conclusion：737字符

# SP-OVD: Self-Prompted Open Vocabulary Object Detection supervised by Similar Visual Prompt

# Abstract



# 1. Introduction



# 2. Related Work（√）

## 2.1 Open Vocabulary Object Detection (OVD)

传统的物体检测将类别表述为One-Hot向量，因此受制于长度固定的分类头，严重依赖数据集给定的类别数量，无法识别其它的类别。OVD允许用自然语言自由地指定任意数量的类别。OVR-CNN首先提出了这项任务，尝试将区域特征与图像对应的标题中的名词对齐作为Baseline。CLIP使用对比学习提高了图像和文本模态可以对齐能力，且其注意力机制可有效关注文本描述的区域。GLIP将目标检测和短语定位两个任务统一在一个框架下。
Traditional Close Set Object Detection methods represent categories as one-hot vectors. This approach is constrained by a fixed-length classification head and is heavily dependent on the number of classes pre-defined in the dataset, rendering it incapable of identifying categories outside of this predefined set. In contrast, Open-Vocabulary Detection (OVD) permits the flexible specification of an arbitrary number of categories using natural language. The task was first proposed by OVR-CNN, which attempted to align regional features with nouns from corresponding image captions as a baseline. Subsequently, CLIP enhanced the alignment capability between image and text modalities through contrastive learning, and its attention mechanism can effectively focus on regions described by the text. GLIP unifies the tasks of object detection and phrase grounding within a single framework.



目前的物体检测领域可以分为两派：以YOLO系列为代表的CNN派、以DETR为代表的Transformer派。其中DETR在大规模训练集的情景下显著优于YOLO。OV-DETR首次将闭集DETR模型改成开放词汇检测器，但受制于DETR基线收敛速度慢的问题。
Object Detection can be broadly categorized into two main paradigms: the CNN-based model (exemplified by the YOLO series) and the Transformer-based model (represented by DETR). DETR demonstrates significantly superior performance over YOLO when trained on large-scale datasets. OV-DETR was the first to adapt a closed-set DETR model into an open-vocabulary detector. However, its performance is restricted by the slow convergence speed of the DETR baseline.

> RegionCLIP [ [40](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib40) ] 提出了一种第二阶段预训练机制，使 CLIP 模型能够编码区域特征，并在 OVD 和零样本迁移设置下展示了其能力。 GLIP [ [21](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib21) ] 联合学习目标定位和视觉语言对齐。Matthias *等人* [ [15](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib15) ] 提出微调视觉语言对齐模型进行检测，同时我们修复了预训练的视觉语言模型，以便更好地泛化到新的类别。



由于CLIP在训练时只用到了全局级别的图像和文本描述，因此不适用于细粒度的区域级（Region）任务。RegionCLIP尝试直接构造区域-文本对（Region-Text Pair）实现两者的对齐，从而赋予模型更好的检测区域的能力，但是数据集的多样性有限，零样本（Zero-Shot）识别能力不够理想。CORA使用了Region Prompt和Anchor Prematching，让模型额外处理区域级的ROI，并证明了该方法的泛用性。GroundingDINO将文本类别拆分为若干带有掩码的短语，提供了更细粒度的文本信息，在颈部、查询初始化和头部（neck, query initialization, and head phases）增强了多模态信息融合。
Due to its training on global-level image and text descriptions, CLIP is not suitable for fine-grained, region-level tasks. To address this, RegionCLIP attempts to directly construct region-text pairs to achieve alignment between the two, thereby endowing the model with an enhanced capacity for region detection. However, the limited diversity of its dataset results in suboptimal zero-shot recognition capabilities. CORA introduces Region Prompting and Anchor Pre-matching, enabling the model to additionally process region-level Regions of Interest (ROIs) and demonstrating the generalizability of this approach. GroundingDINO decomposes textual categories into multiple masked phrase-level tokens to provide more fine-grained textual information, thereby enhancing multimodal information fusion in the neck, query initialization, and head phases of the architecture. 

## 2.2 Modality Fusion

原先的开放词汇相关计算机视觉下游任务仅使用自然语言作为类别提示。近些年来，研究者们尝试令更多的模态来源作为其输入，试图给模型传递更多参考信息，以求获取更好的结果。DINOv仅使用视觉提示实现了开放词汇目标分割。T-Rex2同时整合了文本和图像模态的提示，并对齐了两种模态，以获得OVD更好的效果。YOLO-World首次使用YOLO作为Backbone，证明了非基于Transformer模块提取的视觉特征与文本Encoder输出的融合可行性。VINO通过Memory Bank机制融合多个用户推理时给出的视觉提示。
In earlier studies of Open Vocabulary downstream tasks, natural language is the only modality for object class description. Recently, researchers have been exploring the integration of additional modality sources as model inputs, aiming to provide models with richer reference information for improved performance. DINOv achieved Open Vocabulary Object Segmentation using only visual prompts. T-Rex2 concurrently integrated and aligning both text and image prompts to achieve better performance in OVD. YOLO-World is the first to leverage YOLO as a backbone in OVD, demonstrating the feasibility of fusing visual features extracted by non-Transformer image encoder with Transformer-based text encoder. VINO introduced a Memory Bank mechianism to integrate multiple visual prompts provided by users during inference.



然而，上述工作的模态信息需要用户手动提供，这是一件非常耗费精力的事情。面向大规模的生产环境时，针对每组数据提供数量多、种类丰富的模态是不现实的。
However, the modality information required by the methods mentioned above needs to be manually provided by the user, which is a highly labor-intensive task. When facing large-scale production environments, it becomes unrealistic to provide a large quantity and rich variety of modalities for each data group.

## 2.3 Distillation to Detector

使用外部模型提供额外信息，蒸馏到Detector中，已经是一种常见的训练策略。DELO提出了一种基于生成模型的零样本目标检测方法。RegionCLIP使用预训练视觉编码器，将其学习到的大规模区域-文本（Region-Text）对知识蒸馏至文本嵌入的投影层。ViLD使用预训练的开放词汇图像分类（Open Vocabulary Classification）模型（例如CLIP），分别计算类别文本和模型推理得到的裁剪区域图像的嵌入，并最小化每个提案和Ground Truth的区域嵌入之间的距离。
Utilizing external models to provide supplementary information and distill it into a detector has become a common training strategy. DELO proposes a zero-shot object detection method based on a generative model. RegionCLIP employs a pre-trained visual encoder, distilling the knowledge from large-scale region-text pairs it has learned into the projection layer of the text embeddings. ViLD uses a pre-trained open-vocabulary classification model, such as CLIP, to compute embeddings for both class text and cropped image regions inferred by the model, subsequently minimizing the distance between the region embeddings of each proposal and the ground truth.



进入到大模型时代后，大语言模型、扩散模型称为蒸馏来源的首选。LaMI-DETR使用GPT-3.5为文本标签类别生成多样的文本描述，经过T5模型后得到嵌入评估各概念之间的相似性，并通过聚类结果构建更多额外的正负样本对以扩充训练集。LLMDet引入了LoRA微调的LLaVA-OV 0.5B用于描述模型推理结果框选的图像区域，期望其输出结果贴近于Qwen-2 72B的描述，等价于一个即插即用的Loss。
With the advent of the large model era, large language models and diffusion models have become the primary sources for distillation. LaMI-DETR utilizes GPT-3.5 to generate diverse textual descriptions for text labels. These descriptions are processed through a T5 model to obtain embeddings for evaluating the similarity between concepts, and the clustering results are then used to construct additional positive and negative sample pairs to augment the training set. LLMDet introduces a LoRA-finetuned LLaVA-OV 0.5B to describe the image regions boxed by the model's inference. The objective is for its output to closely match the descriptions from Qwen-2.5 72B, which is equivalent to a plug-and-play loss function.



然而以上的工作均只使用外部模型自己提供的高度抽象的特征（Features）或嵌入（Embeddings），而非人类可读的检索结果。
However, all the aforementioned works only utilize highly abstract features or embeddings provided by the external models themselves, rather than human-readable retrieval results.

> 我们的检索工作与上述方法正交。

> 最近， [ [59](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib59) ] 提出了用于开放词汇目标检测的 OVR，其中，然后迁移到零样本目标检测设置。
>
> RegionCLIP, CORA
>
> APE调整了文本类别提示的粒度。
>
> VLM已经能实现OVD。LLMDet使用VLM做监督。
>
> 此外，APE (Shen et al. [2024](https://arxiv.org/html/2408.14032v1#bib.bib20) ) 将模型提示扩展到数千个类别词汇和区域描述，显著提升了模型对大规模文本提示的查询效率。

# 3. Similar Visual Prompt Retrieval（√）

**数据集格式。**由于我们使用LLMDet作为Codebase，为了节省成本，我们选择了其发布的GroundingCap-1M作为基础构建新数据集，其中包含COCO、Flickr30k、GQA、LLaVA-Cap和V3Det。GroundingCap-1M数据集$D_{\text{gc}}$能以单个Ground Truth box为粒度，拆分为Region级别的元组$(I_{r},T_{p})$。基于GroundingCap-1M数据集，我们将其扩充为$D=\{(I_{r},T_{p},S_r,S_p)\}$。其中$I_r$为物体框的截图，$T_p$为物体框的类别文本描述，$S_r$为依据图像模态$I_r$检索的相似图像集合，$S_p$为依据文本模态$T_p$检索的相似图像集合。
**Dataset Schema.** Given our adoption of LLMDet as the codebase, we selected its published GroundingCap-1M as a cost-effective foundation for constructing our new dataset. This foundational dataset is a composite of several others, including COCO, Flickr30k, GQA, LLaVA-Cap, and V3Det. The GroundingCap-1M dataset, denoted as $D_{\text{gc}}$, can be decomposed into region-level tuples $(I_r, T_p)$, with each tuple corresponding to a single ground truth bounding box. Building upon this, we augment the GroundingCap-1M dataset into $D=\{(I_r, T_p, S_r, S_p)\}$, where $I_r$ represents the cropped image of an object bounding box, $T_p$ is the textual category description for that object box. $S_r$ is the set of similar images retrieved based on the image modality $I_r$, while $S_p$ is the set of similar images retrieved using the text modality $T_p$.



**数据集构造。**为了根据文本或图像检索相似的图像，我们使用经典的CLIP和目前最佳的ImageBind作为跨模态Embedder。为防止数据集泄漏，检索数据库仅包含GroundingCap-1M训练集的Ground Truth标注框，后文🟥会展示针对检索数据库容量的消融实验。其中最大规模的向量数据库规模为952k，这使得我们不可能以$O(n^2)$的空间复杂度计算相似度矩阵。这里我们参考工业界RAG的思想，使用带索引的向量数据库来加速检索过程。具体地说，我们使用Milvus Lite作为向量数据库后端，它使用SQLite3的blob格式持久化存储元信息，且支持IVF-FLAT索引。IVF-FLAT索引是一种加速浮点向量搜索检索的索引算法，它能在大规模数据集上保证高吞吐量查询和高精确度检索，其原理是对数据集进行聚类可以减少搜索空间，代价是具有足够内存来存储聚类数据，而这在如今的服务器上并不像GRAM那样成为瓶颈。Benchmark（QPS与数据集容量）如表🟥所示。
**Dataset Construction.** To retrieve similar images based on text or image queries, we utilize the classic CLIP and the current state-of-the-art ImageBind as cross-modal embedders. To prevent dataset leakage, the retrieval database only contains Ground Truth bounding box annotations from the GroundingCap-1M training set. We'll later present an ablation study (indicated by 🟥) on the retrieval database capacity. The largest vector database in our experiments comprises 952k entries, making similarity matrix computation in $O(n^2)$ space complexity almost impossible. Here, we adopt a strategy inspired by Retrieval Augmented Generation (RAG) from industry, employing an indexed vector database to accelerate the retrieval process. Specifically, we use Milvus Lite as the vector database backend. Milvus Lite leverages SQLite3's blob format for persistent metadata storage and supports the IVF-FLAT index. IVF-FLAT is an indexing algorithm designed to accelerate floating-point vector similarity search. It ensures high-throughput queries and high-precision retrieval on large-scale datasets. Its principle involves clustering the dataset to reduce the search space. While this approach requires sufficient memory to store the clustered data, this is no longer a RAM bottleneck on modern servers, unlike GRAM. Benchmarks (QPS vs. dataset capacity) are presented in Table 🟥.



**质量保证。**原先的GroundingCap-1M数据集包含了大量重复的标注框（annotated bounding boxes），这样做的动机是合理的，因为每次VLM描述区域图像时会生成不同的文本，从而提高了描述的多样性，尽可能多地蒸馏VLM中的知识。然而这严重不利于相似度检索，这样做会使得检索出的大量结果就是Ground Truth单个区域本身，这与我们想要检索相似图像而非相等图像的动机相悖。因此我们在构建相似图像检索库前，先对其标注框使用集合去重。除此以外，由于原始数据集（COCO等）包含一些异常标注（例如绝对坐标和相对坐标混用、形状为1×1像素的区域），GroundingCap-1M在构建时并未忽略这些异常，而是在训练时通过DataLoader中手动设计的、近乎随意的异常判定条件当场过滤，这为数据加载阶段带来了额外不必要的检查开销，且该DataLoader依赖于特定的MMEngine框架，不方便迁移至其它基线（Baseline）。即使如此，DataLoader仍不能过滤所有异常情况，例如从JSON文本中反序列化得到的浮点数计算误差使得框的大小为1.0001，恰好大于1，绕过了上述异常过滤。除此以外，GroundingCap-1M的Schema也并不统一，存在一些标注没有任何对应的边界框，或者边界框的phrase有时是`str`有时是`list[str]`。基于此，我们精心设计了更严格的过滤机制来清洗数据集，删除了约4.4K条非法标注。
The original GroundingCap-1M dataset contains a substantial number of duplicate annotated bounding boxes. This design is motivated by the reasonable objective of enhancing descriptive diversity, as a Vision-Language Model (VLM) generates varied textual descriptions for the same image region, thereby distilling as much knowledge as possible from the VLM. However, this redundancy is highly detrimental to similarity retrieval. It causes the retrieval results to be overwhelmingly dominated by the ground-truth single region itself, which contradicts our goal of retrieving semantically similar, rather than identical, images. Therefore, prior to constructing our similarity retrieval gallery, we performed set-based deduplication on the annotated bounding boxes.Furthermore, the original datasets (e.g., COCO) contain anomalous annotations, such as the mixed use of absolute and relative coordinates and regions with dimensions of a single pixel (1x1). GroundingCap-1M did not filter out these anomalies during its creation. Instead, it relied on ad-hoc, nearly arbitrary exception-handling conditions manually implemented within the DataLoader to filter them at runtime. This approach introduces unnecessary computational overhead during the data loading phase. Moreover, this specific DataLoader is dependent on the MMEngine framework, making it difficult to migrate to other baselines. Even with these measures, the DataLoader fails to filter all anomalies. For instance, floating-point precision errors arising from deserializing JSON text can result in a bounding box dimension of 1.0001, which, being marginally greater than 1, bypasses the aforementioned filter. Additionally, the schema of GroundingCap-1M is inconsistent; some annotations lack corresponding bounding boxes, and the "phrase" associated with a bounding box can be either a string or a list of strings. In light of these issues, we have designed a more stringent filtering mechanism to clean the dataset, which resulted in the removal of approximately 4,400 invalid annotations.

[TSNE降维示意图](https://milvus.io/docs/zh/vector_visualization.md)如图🟥所示。图🟥展示了其中的一条样本，并展示了文本/图像的检索内容。

# 4. Method

$$
\text{torch.einsum(ndhw->d)}
$$



# 5. Experiment

## 5.1 Details



## 5.2 Benchmarks



## 5.3 Ablation Study

**Effect of Modality Fusion Module.** 

**Effect of Similarity Retrieval Database Capacity.** 

# 6. Conclusion

在这项工作中，我们探索并验证了融合的相似提示的有效性。在融合的相似视觉提示监督下，检测器能够有效利用这种额外的模态，并构建概念级的理解能力。实验结果表明，这种方法的检测结果更加准确。我们希望我们的工作可以激发目标检测领域更多的针对相似模态提示的探索。
In this work, we investigate and verify the efficacy of similar prompts fusion. Supervised by fused similar visual prompts, detectors could effectively utilize this additional modality and cultivate concept-aware comprehension. Experiments demonstrate that this approach can yield more precise predictions. We wish our work could inspire more exploration into the effect of similar prompts supervision in Object Detection.

> CORA、RegionCLIP、GroundingDINO、VINO、LaMI-DETR、LLMDet
>
> （待T-Rex2，SIA-OVD(尤其注意其写作思路)）
>
> **Open-Vocabulary Object Detection** OVR-CNN [[36](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib36)] firstly put forth this new formulation of detection, and proposes its baseline solution by aligning region features with nouns in captions that are paired with the image. Mingfeng [[11](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib11)] *et al*. mine pseudo labels by utilizing the localization ability of pre-trained vision-language models. PromptDet [[10](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib10)] addresses the gap between image and region classification by adding learnable prompts when encoding the class names, namely regional prompt learning (RPL), which is expected to generalize from base to novel categories. OV-DETR [[35](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib35)] is the first DETR-style open-vocabulary detector, which proposes conditional matching to solve the missing novel class problem in assignment, but at the cost of inefficient inference. RegionCLIP [[40](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib40)] propose a second-stage pre-training mechanism to adapt the CLIP model to encode region features, and demonstrates its capability on OVD and zero-shot transfer setting. GLIP [[21](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib21)] jointly learns object localization and VL alignment. Matthias *et al*. [[15](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib15)] proposes to finetune a VL aligned model for detection, while we fix the pre-trained VL model for better generalization towards novel categories.
> **开放词汇对象检测** Mingfeng [ [11](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib11) ] *等人*利用预训练视觉语言模型的定位能力挖掘伪标签。PromptDet [ [10](https://ar5iv.labs.arxiv.org/html/2303.13076?_immersive_translate_auto_translate=1#bib.bib10) ] 通过在编码类名时添加可学习的提示来解决图像和区域分类之间的差距，即区域提示学习 (RPL)，预计其将从基础类别推广到新类别。
>
> 
>
> **Zero-shot and open-vocabulary object detection**. Zero-shot object detection aims at detecting novel object classes which are not seen during detector training [[2](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib2), [39](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib39), [59](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib59), [18](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib18), [64](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib64), [38](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib38)]. Bansal *et al*. [[2](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib2)] learned to match the visual features of cropped image regions to word embeddings using max-margin loss. Rahman *et al*. [[38](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib38)] proposed polarity loss to model background category and to cluster categories with similar semantics. Zhu *et al*. [[64](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib64)] explored improving localization performance for novel categories by synthesizing visual features with a generative model. These zero-shot object detectors usually rely on the semantic space of pretrained word embeddings [[35](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib35)]. Recently, Zareian *et al*. [[59](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib59)] proposed OVR for open-vocabulary object detection, where a visual encoder was first pretrained on image-text pairs to learn broad object concepts and then transferred to zero-shot object detection setting. Another close work is ViLD [[18](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib18)] that focused on the training of zero-shot object detectors by distilling visual features from a pretrained CLIP model [[37](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib37)]. Similar to OVR and ViLD, our detector also leverages the visual-semantic space learned from vision-language pretraining. Different from OVR, we propose to learn visual region representation from our “pseudo” region-text pairs given by another pretrained CLIP model. Our method is thus not restricted to particular text that pairs with an image. Unlike ViLD, our method focuses on pretraining and the resulting regional representations support both zero-shot inference and transfer learning.
> **零样本和开放词汇目标检测** 。零样本目标检测旨在检测在检测器训练期间未见过的新[目标](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib2)[类别](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib39)[[](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib59) [2、39、59、18、64、38](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib18) []](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib38) [。Bansal](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib64) *等人* [ [2](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib2) ] 学习使用最大间隔损失将裁剪图像区域的视觉特征与词嵌入进行匹配。Rahman *等人* [ [38](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib38) ] 提出极性损失来模拟背景类别并对具有相似语义的类别进行聚类。Zhu *等人* [ [64](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib64) ] 探索了通过将视觉特征与生成模型相结合来提高新类别的定位性能。这些零样本目标检测器通常依赖于预训练词嵌入的语义空间 [ [35](https://ar5iv.labs.arxiv.org/html/2112.09106?_immersive_translate_auto_translate=1#bib.bib35) ] 。与 OVR 和 ViLD 类似，我们的检测器也利用了从视觉语言预训练中学习到的视觉语义空间。与 OVR 不同的是，我们建议从另一个预训练的 CLIP 模型提供的“伪”区域-文本对中学习视觉区域表示。 因此，我们的方法并不局限于与图像配对的特定文本。与 ViLD 不同，我们的方法专注于预训练，所得的区域表征既支持零样本推理，也支持迁移学习。
>
> **Detection Transformers.** Grounding DINO is built upon the DETR-like model DINO [[57](https://arxiv.org/html/2303.05499v5#bib.bib57)], which is an end-to-end Transformer-based detector. DETR was first proposed in [[2](https://arxiv.org/html/2303.05499v5#bib.bib2)] and then has been improved from many directions [[64](https://arxiv.org/html/2303.05499v5#bib.bib64), [33](https://arxiv.org/html/2303.05499v5#bib.bib33), [13](https://arxiv.org/html/2303.05499v5#bib.bib13), [5](https://arxiv.org/html/2303.05499v5#bib.bib5), [48](https://arxiv.org/html/2303.05499v5#bib.bib48), [17](https://arxiv.org/html/2303.05499v5#bib.bib17), [4](https://arxiv.org/html/2303.05499v5#bib.bib4)] in the past few years. DAB-DETR [[31](https://arxiv.org/html/2303.05499v5#bib.bib31)] introduces anchor boxes as DETR queries for more accurate box prediction. DN-DETR [[24](https://arxiv.org/html/2303.05499v5#bib.bib24)] proposes a query-denoising approach to stabilizing the bipartite matching. DINO [[57](https://arxiv.org/html/2303.05499v5#bib.bib57)] further develops several techniques including contrastive de-noising and sets a new record on the COCO object detection benchmark. However, such detectors mainly focus on closed-set detection and are difficult to generalize to novel classes because of the limited pre-defined categories.
> **检测变压器。** 基础 DINO 建立在类似 DETR 的模型 DINO [ [57](https://arxiv.org/html/2303.05499v5#bib.bib57) ] 之上 ，后者是一个基于 Transformer 的端到端检测器。DETR 最初在 [ [2](https://arxiv.org/html/2303.05499v5#bib.bib2) ] 中提出 ，随后在过去几年中从多个方向得到了改进 [[](https://arxiv.org/html/2303.05499v5#bib.bib64) [64、33、13、5、48、17、4](https://arxiv.org/html/2303.05499v5#bib.bib33) []](https://arxiv.org/html/2303.05499v5#bib.bib13) [。DAB](https://arxiv.org/html/2303.05499v5#bib.bib48) [-](https://arxiv.org/html/2303.05499v5#bib.bib5) [DETR](https://arxiv.org/html/2303.05499v5#bib.bib4) [[](https://arxiv.org/html/2303.05499v5#bib.bib17) 31 ][引入](https://arxiv.org/html/2303.05499v5#bib.bib31)锚框作为 DETR 查询，以实现更准确的框预测。DN-DETR [ [24](https://arxiv.org/html/2303.05499v5#bib.bib24) ] 提出了一种查询去噪方法来稳定二分匹配。DINO [ [57](https://arxiv.org/html/2303.05499v5#bib.bib57) ] 进一步发展了包括对比去噪在内的多种技术，并在 COCO 物体检测基准上创下了新纪录。然而，此类检测器主要侧重于闭集检测，由于预定义类别有限，难以推广到新的类别。
>
> **Open-Set Object Detection.** Open-set object detection is trained using existing bounding box annotations and aims at detecting arbitrary classes with the help of language generalization. OV-DETR [[56](https://arxiv.org/html/2303.05499v5#bib.bib56)] uses image and text embedding encoded by a CLIP model as queries to decode the category-specified boxes in the DETR framework [[2](https://arxiv.org/html/2303.05499v5#bib.bib2)]. ViLD [[14](https://arxiv.org/html/2303.05499v5#bib.bib14)] distills knowledge from a CLIP teacher model into a R-CNN-like detector so that the learned region embeddings contain the semantics of language. GLIP [[12](https://arxiv.org/html/2303.05499v5#bib.bib12)] formulates object detection as a grounding problem and leverages additional grounding data to help learn aligned semantics at phrase and region levels. It shows that such a formulation can even achieve stronger performance on fully-supervised detection benchmarks. DetCLIP [[52](https://arxiv.org/html/2303.05499v5#bib.bib52)] involves large-scale image captioning datasets and uses the generated pseudo labels to expand the knowledge database. The generated pseudo labels effectively help extend the generalization ability.
> **开放集目标检测。** 开放集目标检测使用现有的边界框标注进行训练，旨在借助语言泛化能力检测任意类别。OV-DETR [ [56](https://arxiv.org/html/2303.05499v5#bib.bib56) ] 使用 CLIP 模型编码的图像和文本嵌入作为查询，对 DETR 框架 [ [2](https://arxiv.org/html/2303.05499v5#bib.bib2) ] 中指定类别的边界框进行解码 。ViLD [ [14](https://arxiv.org/html/2303.05499v5#bib.bib14) ] 将 CLIP 教师模型中的知识提炼到类似 R-CNN 的检测器中，从而使学习到的区域嵌入包含语言语义。GLIP [ [12](https://arxiv.org/html/2303.05499v5#bib.bib12) ] 将目标检测表述为一个基础问题，并利用额外的基础数据来帮助学习短语和区域级别的对齐语义。结果表明，这种表述甚至可以在全监督检测基准上取得更强的性能。DetCLIP [ [52](https://arxiv.org/html/2303.05499v5#bib.bib52) ] 涉及大规模图像字幕数据集，并使用生成的伪标签来扩展知识库。生成的伪标签有效地帮助扩展了泛化能力。
>
> With the emergence of large pre-trained vision-language models like CLIP(Radford et al. [2021](https://arxiv.org/html/2408.14032v1#bib.bib18)) and ALIGN(Jia et al. [2021](https://arxiv.org/html/2408.14032v1#bib.bib9)), methods based on vision and language(Kamath et al. [2021](https://arxiv.org/html/2408.14032v1#bib.bib11); Gu et al. [2021](https://arxiv.org/html/2408.14032v1#bib.bib4); Zhang et al. [2022](https://arxiv.org/html/2408.14032v1#bib.bib25), [2023](https://arxiv.org/html/2408.14032v1#bib.bib24); Yan et al. [2023](https://arxiv.org/html/2408.14032v1#bib.bib22)) have gained significant popularity in the field of open-vocabulary object detection. These methods locate objects using language queries while effectively handling open set problems. OV-DETR(Zang et al. [2022](https://arxiv.org/html/2408.14032v1#bib.bib23)) is the first end-to-end Transformer-based open-vocabulary detector, combining CLIP embeddings from both images and text as object queries for the DETR decoder. GLIP(Li et al. [2022](https://arxiv.org/html/2408.14032v1#bib.bib14)) treats object detection as a grounding problem and achieves significant success by semantically aligning phrases with regions. To address the limitations of single-stage fusion in GLIP, Grounding DINO(Liu et al. [2023](https://arxiv.org/html/2408.14032v1#bib.bib16)) enhances feature fusion at three stages: neck, query initialization, and head phases, thus tackling the issue of incomplete multimodal information fusion. Furthermore, APE(Shen et al. [2024](https://arxiv.org/html/2408.14032v1#bib.bib20)) scales the model prompts to thousands of category vocabularies and region descriptions, significantly improving the model’s query efficiency for large-scale textual prompts. The language-based models aim to enhance the semantic description of language queries to adapt to various visual environments, achieving remarkable progress in zero-shot and few-shot settings. However, relying solely on text poses limitations due to language ambiguity and potential mismatches between textual descriptions and complex visual scenes. This underscores the ongoing need for improved integration of visual inputs to achieve more accurate and comprehensive results. Recent advancements suggest that incorporating richer visual prompts and enhancing multimodal fusion techniques are crucial for overcoming these challenges and pushing the boundaries of open-vocabulary object detection further.
> 随着 CLIP （Radford 等人， [2021 年](https://arxiv.org/html/2408.14032v1#bib.bib18)） 和 ALIGN （Jia 等人， [2021 年](https://arxiv.org/html/2408.14032v1#bib.bib9)） 等大型预训练视觉语言模型的出现，基于视觉和语言的方法 （Kamath 等人， [2021 年](https://arxiv.org/html/2408.14032v1#bib.bib11)；Gu 等人， [2021 年](https://arxiv.org/html/2408.14032v1#bib.bib4)；Zhang 等人， [2022 年](https://arxiv.org/html/2408.14032v1#bib.bib25)、 [2023 年](https://arxiv.org/html/2408.14032v1#bib.bib24)；Yan 等人， [2023 年](https://arxiv.org/html/2408.14032v1#bib.bib22)） 在开放词汇目标检测领域获得了显著的普及。这些方法使用语言查询来定位目标，同时有效地处理开放集问题。OV-DETR （Zang 等人， [2022 年](https://arxiv.org/html/2408.14032v1#bib.bib23)） 是第一个基于 Transformer 的端到端开放词汇检测器，它将来自图像和文本的 CLIP 向量组合起来作为 DETR 解码器的目标查询。 GLIP (Li et al. [2022](https://arxiv.org/html/2408.14032v1#bib.bib14) ) 将目标检测视为 grounding 问题，通过将短语与区域进行语义对齐取得了显著成功。针对 GLIP 单阶段融合的局限性，基于语言的模型旨在增强语言查询的语义描述以适应各种视觉环境，在零样本和小样本设置下取得了显著进展。 然而，由于语言的歧义性以及文本描述与复杂视觉场景之间可能存在的不匹配，单纯依赖文本会带来诸多限制。这凸显了我们持续改进视觉输入集成以获得更准确、更全面的结果的需求。最近的进展表明，融入更丰富的视觉提示并增强多模态融合技术对于克服这些挑战并进一步突破开放词汇物体检测的界限至关重要。
>
> In open-vocabulary object detection (OVD), the detector is trained on a limited training dataset but aims to detect arbitrary test-time user-input classes. To detect arbitrary classes, open-vocabulary object detection is formulated as a vision-language task so that the detector can detect classes never seen with class names. Motivated by the impressive zero-shot ability of vision-language models (*e.g*. CLIP [[43](https://arxiv.org/html/2501.18954v1#bib.bib43)]), aligning detectors with CLIP [[14](https://arxiv.org/html/2501.18954v1#bib.bib14), [54](https://arxiv.org/html/2501.18954v1#bib.bib54), [26](https://arxiv.org/html/2501.18954v1#bib.bib26)] or integrating CLIP as a part of model [[22](https://arxiv.org/html/2501.18954v1#bib.bib22), [13](https://arxiv.org/html/2501.18954v1#bib.bib13)] are straight-forward directions for addressing OVD. However, since CLIP is pretrained with image-level objectives, the features in CLIP are not perfectly suitable for OVD.
> 在开放词汇对象检测 (OVD) 中，检测器在有限的训练数据集上进行训练，但目标是检测任意的测试时用户输入类别。为了检测任意类别，开放词汇对象检测被制定为视觉语言任务，以便检测器可以检测从未见过的带有类名的类别。受视觉语言模型 ( *例如* CLIP [ [43](https://arxiv.org/html/2501.18954v1#bib.bib43) ] ) 出色的零样本能力启发，将检测器与 CLIP [ [14](https://arxiv.org/html/2501.18954v1#bib.bib14) , [54](https://arxiv.org/html/2501.18954v1#bib.bib54) , [26](https://arxiv.org/html/2501.18954v1#bib.bib26) ] 对齐或将 CLIP 集成为模型 [ [22](https://arxiv.org/html/2501.18954v1#bib.bib22) , [13](https://arxiv.org/html/2501.18954v1#bib.bib13) ] 的一部分是解决 OVD 的直接方向。
>
> Alternatively, building an object-aware visual-language space with massive data [[66](https://arxiv.org/html/2501.18954v1#bib.bib66), [27](https://arxiv.org/html/2501.18954v1#bib.bib27), [36](https://arxiv.org/html/2501.18954v1#bib.bib36), [56](https://arxiv.org/html/2501.18954v1#bib.bib56)] from various resources, including image classification datasets [[9](https://arxiv.org/html/2501.18954v1#bib.bib9)], object detection datasets [[31](https://arxiv.org/html/2501.18954v1#bib.bib31), [15](https://arxiv.org/html/2501.18954v1#bib.bib15), [44](https://arxiv.org/html/2501.18954v1#bib.bib44), [50](https://arxiv.org/html/2501.18954v1#bib.bib50)], grounding datasets [[17](https://arxiv.org/html/2501.18954v1#bib.bib17), [42](https://arxiv.org/html/2501.18954v1#bib.bib42)] and image-text datasets [[45](https://arxiv.org/html/2501.18954v1#bib.bib45)], has shown impressive results. Further, multi-task learning with other language tasks, such as masked language modeling [[62](https://arxiv.org/html/2501.18954v1#bib.bib62)] and dense captioning [[38](https://arxiv.org/html/2501.18954v1#bib.bib38), [58](https://arxiv.org/html/2501.18954v1#bib.bib58)] can achieve better vision-language alignment, thus improving the detector’s open-vocabulary ability. However, prior works [[38](https://arxiv.org/html/2501.18954v1#bib.bib38), [58](https://arxiv.org/html/2501.18954v1#bib.bib58), [30](https://arxiv.org/html/2501.18954v1#bib.bib30), [53](https://arxiv.org/html/2501.18954v1#bib.bib53)] only focus on generating short phrases for regions of interest. In this work, we explore another co-training task, *i.e*. generating image-level detailed captions using large language models.
> 另外，使用来自各种资源（包括图像分类数据集 [ [9](https://arxiv.org/html/2501.18954v1#bib.bib9) ] 、目标检测数据集 [ 31 , 15 , [44](https://arxiv.org/html/2501.18954v1#bib.bib66) [,](https://arxiv.org/html/2501.18954v1#bib.bib27) [50](https://arxiv.org/html/2501.18954v1#bib.bib36) []](https://arxiv.org/html/2501.18954v1#bib.bib56) 、基础数据集 [ 17 , 42 ] 和图像文本数据集 [ [45](https://arxiv.org/html/2501.18954v1#bib.bib45) ]） 的海量数据 [[](https://arxiv.org/html/2501.18954v1#bib.bib17) [66](https://arxiv.org/html/2501.18954v1#bib.bib42) [,](https://arxiv.org/html/2501.18954v1#bib.bib31) [27](https://arxiv.org/html/2501.18954v1#bib.bib15) [,](https://arxiv.org/html/2501.18954v1#bib.bib50) [36](https://arxiv.org/html/2501.18954v1#bib.bib44) , 56 ] 构建对象感知的视觉语言空间，已经取得了令人印象深刻的效果。此外，与其他语言任务结合使用多任务学习，如掩蔽语言建模 [ [62](https://arxiv.org/html/2501.18954v1#bib.bib62) ] 和密集字幕 [ [38](https://arxiv.org/html/2501.18954v1#bib.bib38) , [58](https://arxiv.org/html/2501.18954v1#bib.bib58) ] ， 可以实现更好的视觉语言对齐，从而提高检测器的开放词汇能力。然而，之前的研究 [ [38](https://arxiv.org/html/2501.18954v1#bib.bib38) , [58](https://arxiv.org/html/2501.18954v1#bib.bib58) , [30](https://arxiv.org/html/2501.18954v1#bib.bib30) , [53](https://arxiv.org/html/2501.18954v1#bib.bib53) ] 仅侧重于为感兴趣区域生成短语。在这项工作中，我们探索了另一项共同训练任务， *即*使用大型语言模型生成图像级详细字幕。